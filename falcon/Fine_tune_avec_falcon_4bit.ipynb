{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXa7DOX84GG"
      },
      "source": [
        "# **Tutoriel** - Fine-tuning de Falcon-7B-instruct-4bit\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/opinionscience/FabriqueLLM/main/illustration/falcon_image.png\" alt=\"Falcon logo\"  width=\"500\"/>\n",
        "Ce carnet de code mis à disposition par [OpSci](https://www.opsci.ai/fr/) permet d'effectuer le fine-tuning d'un grand modèle de langue, Falcon-7B avec la **version gratuite de Google Colab**.\n",
        "\n",
        "Créé par le Technology Innovation Institute d'Abu Dhabi, Falcon est aujourd'hui le LLM open source de référence. La principale alternative, Llama, est réservée aux usages de recherche non commerciale. Falcon est disponible sous deux versions : la principale à 40 milliards de paramètres et une version plus légère que nous allons utiliser ici à 7 milliards de paramètres. Ce modèle inclut un corpus plus multilingue que d'autres LLMs ouverts comme Pythia ou MPT.\n",
        "\n",
        "Ce carnet de code utilise une version déjà ré-entraînée de Falcon-7 : instruct-4-bit. C'est aussi une version plus compacte de Falcon qui devrait pouvoir tourner sur une version gratuite de Google Colab : vous aurez besoin d'environ 10go de Vram. Ce fine-tuning sera aussi plus superficiel mais c'est déjà très pratique pour effectuer de premiers tests.\n",
        "\n",
        "Cette démonstration ne fait tourner qu'une seule *epoch* ce qui est suffisant pour avoir un premier aperçu. Pour obtenir un bon modèle, il est conseillé de faire tourner le fine-tuning pendant trois *epochs*. Sur notre corpus de démonstration de 6000 instructions une *epoch* prendra un peu plus d'une heure avec les GPUs gratuits de Google Colab. Un fine-tuning complet (trois *epochs*) prendra environ 4h : sur la version gratuite de Google Colab ce type de traitement d'une durée un peu longue risque d'être interrompu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTyUYSsywxD"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kamSkzAq9KE5"
      },
      "source": [
        "En tout premier lieu nous vérifions si nous disposons de suffisamment de mémoire vive (au moins 10go) sinon ce n'est pas la peine de lancer le script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwOy3-Dl9iRW",
        "outputId": "6eb8b90e-e143-49f2-b18a-55df4e34b560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun 11 14:57:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    40W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgZdKVNFIaXm"
      },
      "source": [
        "D'abord nous allons nous maintenant connecter à Google Drive. C'est vraiment recommandé et tout l'intérêt d'utiliser Google Colab. Autrement à l'expiration de la session tout le modèle sera perdu. À noter qu'il y a une latence plus ou moins importante entre Google Colab et Google Drive : vous ne verrez pas immédiatement les fichiers intermédiaires (*checkpoint*) et les fichiers finaux sur Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5sPY1BeIbRn",
        "outputId": "467ab951-be6f-4725-eab1-34435cc9b2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/falcon\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd \"/content/drive/My Drive/falcon\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6nmahaFHKEK"
      },
      "source": [
        "Nous installons falcontune. C'est une petite application python disponible de Github qui permet d'effectuer le fine-tuning de Falcon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyFPmAByvhV",
        "outputId": "39f27d62-ff60-451c-a22c-856d0a545e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'falcontune'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 124 (delta 43), reused 53 (delta 30), pack-reused 49\u001b[K\n",
            "Receiving objects: 100% (124/124), 63.60 KiB | 4.89 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rmihaylov/falcontune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKbNWMPpdsZg"
      },
      "source": [
        "Et nous récupérons aussi les poids du modèle :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ftYL3Vduas",
        "outputId": "54e9dc5f-ca27-4420-d2cb-865b55478c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-20 16:01:56--  https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/resolve/main/gptq_model-4bit-64g.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.43, 65.9.86.34, 65.9.86.125, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/9a/ea/9aea392ba3a1a4fa936207c60a9ba3cfa28fa3b935dfffd3227300a5ff38d088/ceb8ec3d0c432d043ec563d42e2571c94a5e884aedfb5acc6b38612a60490c7b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gptq_model-4bit-64g.safetensors%3B+filename%3D%22gptq_model-4bit-64g.safetensors%22%3B&Expires=1687536117&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzlhL2VhLzlhZWEzOTJiYTNhMWE0ZmE5MzYyMDdjNjBhOWJhM2NmYTI4ZmEzYjkzNWRmZmZkMzIyNzMwMGE1ZmYzOGQwODgvY2ViOGVjM2QwYzQzMmQwNDNlYzU2M2Q0MmUyNTcxYzk0YTVlODg0YWVkZmI1YWNjNmIzODYxMmE2MDQ5MGM3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc1MzYxMTd9fX1dfQ__&Signature=Qqi8tB6nsxrdwdyDAIZQIvKWiztgiNcFcqrLReBioTtzLiMgQntgVhSuw9%7ENTLkOLAUxja11KKVrV9v2QVZ8bAX28ptxiZiqoKFSQ8QYkKNOx6h1xRIPaKVlpS5X1W21oqAaq7gizZNbxp8B%7E6eqmBkK0k3%7EhYr9ctyOLLoB52tDrvRnWUGRGH9oqJbEAOg6IM7cjoeswH3rRGBrLoZ4P%7EX-JUKksKaXgpEJ-urD1gpZMPV42aSOTdT0CIAG%7ES5IQbgdf6PmdI2R2OHOx490KvyYDV%7ES4WNUT9xyQD9GcDkzIzq5u9fZp4K-EVOQvNAbXswReIcjbeM%7E77KVEiODsA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-20 16:01:56--  https://cdn-lfs.huggingface.co/repos/9a/ea/9aea392ba3a1a4fa936207c60a9ba3cfa28fa3b935dfffd3227300a5ff38d088/ceb8ec3d0c432d043ec563d42e2571c94a5e884aedfb5acc6b38612a60490c7b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gptq_model-4bit-64g.safetensors%3B+filename%3D%22gptq_model-4bit-64g.safetensors%22%3B&Expires=1687536117&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzlhL2VhLzlhZWEzOTJiYTNhMWE0ZmE5MzYyMDdjNjBhOWJhM2NmYTI4ZmEzYjkzNWRmZmZkMzIyNzMwMGE1ZmYzOGQwODgvY2ViOGVjM2QwYzQzMmQwNDNlYzU2M2Q0MmUyNTcxYzk0YTVlODg0YWVkZmI1YWNjNmIzODYxMmE2MDQ5MGM3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc1MzYxMTd9fX1dfQ__&Signature=Qqi8tB6nsxrdwdyDAIZQIvKWiztgiNcFcqrLReBioTtzLiMgQntgVhSuw9%7ENTLkOLAUxja11KKVrV9v2QVZ8bAX28ptxiZiqoKFSQ8QYkKNOx6h1xRIPaKVlpS5X1W21oqAaq7gizZNbxp8B%7E6eqmBkK0k3%7EhYr9ctyOLLoB52tDrvRnWUGRGH9oqJbEAOg6IM7cjoeswH3rRGBrLoZ4P%7EX-JUKksKaXgpEJ-urD1gpZMPV42aSOTdT0CIAG%7ES5IQbgdf6PmdI2R2OHOx490KvyYDV%7ES4WNUT9xyQD9GcDkzIzq5u9fZp4K-EVOQvNAbXswReIcjbeM%7E77KVEiODsA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 65.9.86.27, 65.9.86.11, 65.9.86.70, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|65.9.86.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5942953456 (5.5G) [binary/octet-stream]\n",
            "Saving to: ‘gptq_model-4bit-64g.safetensors’\n",
            "\n",
            "gptq_model-4bit-64g 100%[===================>]   5.53G  74.0MB/s    in 82s     \n",
            "\n",
            "2023-06-20 16:03:18 (69.4 MB/s) - ‘gptq_model-4bit-64g.safetensors’ saved [5942953456/5942953456]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/resolve/main/gptq_model-4bit-64g.safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFnsudjOHtnZ"
      },
      "source": [
        "## Le corpus d'instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PeUIuyeHOdM"
      },
      "source": [
        "Nous récupérons un set d'instruction. Dans l'optique d'un simple exemple de démonstration nous utilisons ici une sélection aléatoire de différents corpus d'instructions traduits en français par le projet Vicogne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjwy8C3kw4fF",
        "outputId": "86e6413d-f67b-43b2-95eb-1fe3afd7bdc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-23 09:15:58--  https://raw.githubusercontent.com/opinionscience/InstructionFr/main/synthetic_instruction/instruct_fr_vicogne_sample.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4934640 (4.7M) [text/plain]\n",
            "Saving to: ‘instruct_fr_vicogne_sample.json’\n",
            "\n",
            "instruct_fr_vicogne 100%[===================>]   4.71M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-06-23 09:15:59 (83.9 MB/s) - ‘instruct_fr_vicogne_sample.json’ saved [4934640/4934640]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/opinionscience/InstructionFr/main/synthetic_instruction/instruct_fr_vicogne_sample.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSABEKyZ1Jy3"
      },
      "source": [
        "Ces instructions utilisent le format classique du projet Alpaca de Stanford : *instructions*, *input* (optionnellement) et *output*. En résumé, les instructions correspondent à des exemples de prompts que pourraient laisser les utilisateurs du LLM, les *outputs* à la réponse que le LLM devrait générer et les *inputs* apporte des éléments de contextes supplémentaires (par exemple sous la forme de textes cités en exemple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw-XyFFe1tvl",
        "outputId": "3518710a-4414-4777-bf66-2a6df6af89cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"language\": \"fr\",\n",
            "    \"instruction\": \"Cr\\u00e9e un tableur en utilisant MS Excel pour suivre l'inventaire d'un magasin de d\\u00e9tail.\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Un tableur est cr\\u00e9\\u00e9 en utilisant MS Excel pour suivre l'inventaire d'un magasin de d\\u00e9tail. Le contenu du tableur se compose de colonnes pour l'article d'inventaire, la description de l'article, la quantit\\u00e9 d'article et le type d'article. Le tableur comprend \\u00e9galement une ligne pour chaque article d'inventaire et permet \\u00e0 l'utilisateur de remplir facilement les informations pour chaque article, telles que la description de l'article, la quantit\\u00e9 d'article et le type d'article. De plus, le tableur comprend une formule pour calculer la valeur totale des articles en inventaire, ce qui est utile pour suivre la performance financi\\u00e8re du magasin.\",\n",
            "    \"source\": \"Vicogne (european_instruct/translated/translated_fr/converted_alpaca_data_cleaned_fr_52k)\"\n",
            "  },\n",
            "  {\n",
            "    \"language\": \"fr\",\n",
            "    \"instruction\": \"G\\u00e9n\\u00e8re une critique de film bas\\u00e9e sur ce synopsis de Little Women : Le film suit le roman classique de Louisa May Alcott et raconte l'histoire des quatre s\\u0153urs March qui grandissent en Am\\u00e9rique \\u00e0 la suite de la guerre civile.\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Little Women est une excellente adaptation du roman classique bien-aim\\u00e9 de Louisa May Alcott. Le film suit les quatre s\\u0153urs March alors qu'elles grandissent en Am\\u00e9rique \\u00e0 la suite de la guerre civile. Avec un casting merveilleux et une r\\u00e9alisation poignante, Little Women est s\\u00fbr de plaire aux fans du roman original et aux nouveaux venus. Son exploration de th\\u00e8mes tels que la famille, l'ind\\u00e9pendance f\\u00e9minine et le pouvoir de l'amour en font un visionnage \\u00e9motionnel et stimulant.\",\n",
            "    \"source\": \"Vicogne (european_instruct/translated/translated_fr/converted_alpaca_data_cleaned_fr_52k)\"\n",
            "  },\n",
            "  {\n",
            "    \"language\": \"fr\",\n",
            "    \"instruction\": \"Donnez-moi un r\\u00e9sum\\u00e9 de la guerre de 1812 : La guerre de 1812 (18 juin 1812 - 17 f\\u00e9vrier 1815) a \\u00e9t\\u00e9 men\\u00e9e par les \\u00c9tats-Unis d'Am\\u00e9rique et ses alli\\u00e9s indig\\u00e8nes contre le Royaume-Uni et ses alli\\u00e9s en Am\\u00e9rique du Nord britannique, avec une participation limit\\u00e9e de l'Espagne en Floride. Les tensions sont n\\u00e9es de divergences de longue date sur l'expansion territoriale en Am\\u00e9rique du Nord et le soutien britannique aux tribus am\\u00e9rindiennes qui s'opposaient \\u00e0 la colonisation am\\u00e9ricaine dans les Territoires du Nord-Ouest.\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"La guerre a \\u00e9galement \\u00e9t\\u00e9 aliment\\u00e9e par les actions provocatrices des deux c\\u00f4t\\u00e9s, notamment la saisie de navires marchands am\\u00e9ricains par la Royal Navy britannique et la pratique de l'impression forc\\u00e9e de marins am\\u00e9ricains par les Britanniques. La guerre s'est d\\u00e9roul\\u00e9e sur terre et sur mer, mais aucun des camps n'a r\\u00e9ussi \\u00e0 remporter une victoire d\\u00e9cisive avant que les deux parties ne d\\u00e9cident de mettre fin au conflit en signant le trait\\u00e9 de Gand en 1814. Bien que la guerre n'ait pas abouti \\u00e0 un changement significatif dans les relations entre les \\u00c9tats-Unis et le Royaume-Uni, elle a renforc\\u00e9 le sentiment nationaliste aux \\u00c9tats-Unis et a contribu\\u00e9 \\u00e0 mettre fin au soutien britannique aux tribus am\\u00e9rindiennes dans les Territoires du Nord-Ouest.\",\n",
            "    \"source\": \"Vicogne (european_instruct/translated/translated_fr/converted_dolly_bactrian_fr_15k)\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open('instruct_fr_vicogne_sample.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "json_formatted_str = json.dumps(data[0:3], indent=2)\n",
        "\n",
        "print(json_formatted_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ4n2nXpAxtL"
      },
      "source": [
        "Nous procédons à l'installation de falcontune. Cela prendra 1-2 minutes. Vous devrez le refaire à chaque nouvelle session même si vous avez déjà chargé l'application sur Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQr_AC8sxBsp",
        "outputId": "488e89bc-137a-463d-8d75-0e45f06fe932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu118\n",
            "Collecting transformers==4.29.2 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
            "Collecting sentencepiece==0.1.99 (from -r requirements.txt (line 4))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.12.0 (from -r requirements.txt (line 5))\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1+cu118 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.0.1+cu118)\n",
            "Collecting accelerate==0.19.0 (from -r requirements.txt (line 8))\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.3.1 (from -r requirements.txt (line 9))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.6.1 (from -r requirements.txt (line 10))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 11))\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 12))\n",
            "  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.29.2->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r requirements.txt (line 3)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r requirements.txt (line 3)) (16.0.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 5)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 5)) (1.5.3)\n",
            "Collecting xxhash (from datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 5)) (2023.4.0)\n",
            "Collecting aiohttp (from datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0->-r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118->-r requirements.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118->-r requirements.txt (line 7)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118->-r requirements.txt (line 7)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118->-r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 11)) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading sentry_sdk-1.26.0-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 11)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.3->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.12.0->-r requirements.txt (line 5))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cu118->-r requirements.txt (line 7)) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cu118->-r requirements.txt (line 7)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 11))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=267df50cf87795ec16a65d9d3b9758c3d6259603357fcc034e6d0728f1b19bb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, bitsandbytes, xxhash, smmap, setproctitle, sentry-sdk, multidict, frozenlist, einops, docker-pycreds, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, gitdb, aiosignal, transformers, GitPython, aiohttp, wandb, datasets, accelerate, peft\n",
            "Successfully installed GitPython-3.1.31 accelerate-0.19.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.39.0 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 einops-0.6.1 frozenlist-1.3.3 gitdb-4.0.10 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 pathtools-0.1.2 peft-0.3.0 responses-0.18.0 safetensors-0.3.1 sentencepiece-0.1.99 sentry-sdk-1.26.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.29.2 wandb-0.15.3 xxhash-3.2.0 yarl-1.9.2\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing falcontune.egg-info/PKG-INFO\n",
            "writing dependency_links to falcontune.egg-info/dependency_links.txt\n",
            "writing entry points to falcontune.egg-info/entry_points.txt\n",
            "writing top-level names to falcontune.egg-info/top_level.txt\n",
            "reading manifest file 'falcontune.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'falcontune.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/falcontune\n",
            "copying build/lib/falcontune/run.py -> build/bdist.linux-x86_64/egg/falcontune\n",
            "copying build/lib/falcontune/__init__.py -> build/bdist.linux-x86_64/egg/falcontune\n",
            "copying build/lib/falcontune/generate.py -> build/bdist.linux-x86_64/egg/falcontune\n",
            "copying build/lib/falcontune/data.py -> build/bdist.linux-x86_64/egg/falcontune\n",
            "copying build/lib/falcontune/finetune.py -> build/bdist.linux-x86_64/egg/falcontune\n",
            "creating build/bdist.linux-x86_64/egg/falcontune/backend\n",
            "copying build/lib/falcontune/backend/base.py -> build/bdist.linux-x86_64/egg/falcontune/backend\n",
            "copying build/lib/falcontune/backend/__init__.py -> build/bdist.linux-x86_64/egg/falcontune/backend\n",
            "creating build/bdist.linux-x86_64/egg/falcontune/backend/cuda\n",
            "copying build/lib/falcontune/backend/cuda/__init__.py -> build/bdist.linux-x86_64/egg/falcontune/backend/cuda\n",
            "copying build/lib/falcontune/backend/cuda/quantlinear.py -> build/bdist.linux-x86_64/egg/falcontune/backend/cuda\n",
            "copying build/lib/falcontune/backend/cuda/autograd.py -> build/bdist.linux-x86_64/egg/falcontune/backend/cuda\n",
            "creating build/bdist.linux-x86_64/egg/falcontune/backend/torch\n",
            "copying build/lib/falcontune/backend/torch/__init__.py -> build/bdist.linux-x86_64/egg/falcontune/backend/torch\n",
            "copying build/lib/falcontune/backend/torch/quantlinear.py -> build/bdist.linux-x86_64/egg/falcontune/backend/torch\n",
            "creating build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "copying build/lib/falcontune/backend/triton/quantlinear.py -> build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "copying build/lib/falcontune/backend/triton/triton_utils.py -> build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "copying build/lib/falcontune/backend/triton/__init__.py -> build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "copying build/lib/falcontune/backend/triton/autograd.py -> build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "copying build/lib/falcontune/backend/triton/flash_attn_triton.py -> build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "copying build/lib/falcontune/backend/triton/custom_autotune.py -> build/bdist.linux-x86_64/egg/falcontune/backend/triton\n",
            "creating build/bdist.linux-x86_64/egg/falcontune/model\n",
            "copying build/lib/falcontune/model/__init__.py -> build/bdist.linux-x86_64/egg/falcontune/model\n",
            "copying build/lib/falcontune/model/gradient_checkpointing.py -> build/bdist.linux-x86_64/egg/falcontune/model\n",
            "copying build/lib/falcontune/model/utils.py -> build/bdist.linux-x86_64/egg/falcontune/model\n",
            "copying build/lib/falcontune/model/lora.py -> build/bdist.linux-x86_64/egg/falcontune/model\n",
            "creating build/bdist.linux-x86_64/egg/falcontune/model/falcon\n",
            "copying build/lib/falcontune/model/falcon/config.py -> build/bdist.linux-x86_64/egg/falcontune/model/falcon\n",
            "copying build/lib/falcontune/model/falcon/__init__.py -> build/bdist.linux-x86_64/egg/falcontune/model/falcon\n",
            "copying build/lib/falcontune/model/falcon/model.py -> build/bdist.linux-x86_64/egg/falcontune/model/falcon\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/run.py to run.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/generate.py to generate.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/data.py to data.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/finetune.py to finetune.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/base.py to base.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/cuda/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/cuda/quantlinear.py to quantlinear.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/cuda/autograd.py to autograd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/torch/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/torch/quantlinear.py to quantlinear.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/triton/quantlinear.py to quantlinear.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/triton/triton_utils.py to triton_utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/triton/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/triton/autograd.py to autograd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/triton/flash_attn_triton.py to flash_attn_triton.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/backend/triton/custom_autotune.py to custom_autotune.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/gradient_checkpointing.py to gradient_checkpointing.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/lora.py to lora.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/falcon/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/falcon/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/falcontune/model/falcon/model.py to model.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying falcontune.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying falcontune.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying falcontune.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying falcontune.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying falcontune.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "falcontune.__pycache__.run.cpython-310: module references __file__\n",
            "creating 'dist/falcontune-0.1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing falcontune-0.1.0-py3.10.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/falcontune-0.1.0-py3.10.egg\n",
            "Extracting falcontune-0.1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding falcontune 0.1.0 to easy-install.pth file\n",
            "Installing falcontune script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/falcontune-0.1.0-py3.10.egg\n",
            "Processing dependencies for falcontune==0.1.0\n",
            "Finished processing dependencies for falcontune==0.1.0\n"
          ]
        }
      ],
      "source": [
        "# Installation:\n",
        "!cd falcontune && pip install -r requirements.txt\n",
        "!cd falcontune && python setup.py install\n",
        "# !cd falcontune && python setup_cuda.py install  # if cuda, default is triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAV1qfdCSni8"
      },
      "source": [
        "Nous allons maintenant réinitiliser notre environnement de travail pour bien intégrer l'installation de falcontune. Tout va crasher mais c'est normal !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U56qd5G3frg"
      },
      "outputs": [],
      "source": [
        "# Restart:\n",
        "import os; os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78_DUqdzGrW"
      },
      "source": [
        "# Finetuning du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIgs_i0BNb5"
      },
      "source": [
        "Tout est prêt à lancer le fine-tuning du modèle. Nous allons juste désactiver Wandb (une extension utilisée par falcontune qui ne présente pas d'intérêt pour nous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdELbxDzIu7",
        "outputId": "27a09a78-7c4d-4ffa-b343-011026135d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/falcon\n"
          ]
        }
      ],
      "source": [
        "# Disable wandb:\n",
        "import os; os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "%cd \"/content/drive/My Drive/falcon\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhMpUgoS6oD"
      },
      "source": [
        "Et nous sommes prêt à lancer la grande commande. Il y a beaucoup de paramètre mais seulement quelqu'uns sont importants :\n",
        "* Nous allons utiliser le modèle Falcon-7b de base et leurs poids correspondants (tiiuae/falcon-7b)\n",
        "* Le fine-tuning sera effectué sur le set d'instruction *instruct_fr_vicogne_sample.json* (évidemment à changer si vous optez pour un autre jeu de données).\n",
        "* Les fichiers du modèle seront placés dans le dossier *falcon-7b-sample* (de nouveau à changer pour le nom de votre modèle).\n",
        "* Nous ne ferons tourner le fine-tuning que sur une *epoch* ce qui est suffisant pour un premier test.\n",
        "\n",
        "Après avoir lancé le script, Google Colab va tourner pendant un peu moins de 40 minutes.\n",
        "\n",
        "Si tout se passe bien vous verrez défiler le processus d'entraînement avec trois indicateurs régulièrement réactualisés : \"{'loss': 1.8581, 'learning_rate': 0.0002993736951983298, 'epoch': 0.0}\" :\n",
        "* Le \"loss\" c'est en quelque sorte le taux d'erreur du modèle : plus cette mesure est basse et plus le modèle parvient à prédire des textes assez approchants de ceux qui sont présent dans le corpus d'instruction.\n",
        "* Le *learning rate* (taux d'apprentissage) c'est la capacité du modèle à mémoriser de nouveaux éléments mais aussi à en oublier des anciens. Cet indicateur va constamment baisser au fur et à mesure de l'apprentissage.\n",
        "* L'*epoch* c'est le cycle d'apprentissage. Comme nous n'avons défini qu'une *epoch* cela correspondra à des pourcentages (de 0 à 0.99 à la fin de l'entraînement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiUKv07zGSm",
        "outputId": "fb8b1d32-f542-4ab8-d346-46f19086487b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-23 09:18:11.804452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-wykdrroqipuc --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Downloading (…)lve/main/config.json: 100% 728/728 [00:00<00:00, 2.97MB/s]\n",
            "The safetensors archive passed at gptq_model-4bit-64g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "Downloading (…)okenizer_config.json: 100% 220/220 [00:00<00:00, 1.49MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 2.73MB [00:00, 3.20MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 281/281 [00:00<00:00, 1.90MB/s]\n",
            "\n",
            "Parameters:\n",
            "-------config-------\n",
            "dataset='./instruct_fr_vicogne_sample.json'\n",
            "data_type='alpaca'\n",
            "lora_out_dir='./falcon-7b-sample-4bit/'\n",
            "lora_apply_dir=None\n",
            "weights='gptq_model-4bit-64g.safetensors'\n",
            "target_modules=['query_key_value']\n",
            "\n",
            "------training------\n",
            "mbatch_size=1\n",
            "batch_size=2\n",
            "gradient_accumulation_steps=2\n",
            "epochs=1\n",
            "lr=0.0003\n",
            "cutoff_len=256\n",
            "lora_r=8\n",
            "lora_alpha=16\n",
            "lora_dropout=0.05\n",
            "val_set_size=0.2\n",
            "gradient_checkpointing=False\n",
            "gradient_checkpointing_ratio=1\n",
            "warmup_steps=5\n",
            "save_steps=50\n",
            "save_total_limit=3\n",
            "logging_steps=5\n",
            "checkpoint=False\n",
            "skip=False\n",
            "world_size=1\n",
            "ddp=False\n",
            "device_map='auto'\n",
            "\n",
            "\n",
            "Converted as Half.\n",
            "trainable params: 2359296 || all params: 593597312 || trainable%: 0.3974573254132256\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c6e696c0e7779c80/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 3041.55it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 182.73it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c6e696c0e7779c80/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 265.14it/s]\n",
            "Run eval every 600 steps\n",
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using cuda_amp half precision backend\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: token_type_ids, instruction, language, source, input, output. If token_type_ids, instruction, language, source, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 4,800\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2,400\n",
            "  Number of trainable parameters = 2,359,296\n",
            "  0% 0/2400 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 2.2814, 'learning_rate': 0.0003, 'epoch': 0.0}\n",
            "{'loss': 2.2309, 'learning_rate': 0.0002993736951983298, 'epoch': 0.0}\n",
            "{'loss': 1.8707, 'learning_rate': 0.0002987473903966597, 'epoch': 0.01}\n",
            "{'loss': 1.3277, 'learning_rate': 0.0002981210855949895, 'epoch': 0.01}\n",
            "{'loss': 1.2512, 'learning_rate': 0.00029749478079331936, 'epoch': 0.01}\n",
            "{'loss': 1.1725, 'learning_rate': 0.00029686847599164924, 'epoch': 0.01}\n",
            "{'loss': 1.0322, 'learning_rate': 0.0002962421711899791, 'epoch': 0.01}\n",
            "{'loss': 0.9567, 'learning_rate': 0.00029561586638830896, 'epoch': 0.02}\n",
            "{'loss': 1.4127, 'learning_rate': 0.0002949895615866388, 'epoch': 0.02}\n",
            "{'loss': 0.99, 'learning_rate': 0.0002943632567849687, 'epoch': 0.02}\n",
            "  2% 50/2400 [01:21<12:20,  3.17it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-50\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1700] due to args.save_total_limit\n",
            "{'loss': 1.0992, 'learning_rate': 0.0002937369519832985, 'epoch': 0.02}\n",
            "{'loss': 1.1614, 'learning_rate': 0.00029311064718162835, 'epoch': 0.03}\n",
            "{'loss': 1.2067, 'learning_rate': 0.00029248434237995823, 'epoch': 0.03}\n",
            "{'loss': 1.0552, 'learning_rate': 0.00029185803757828807, 'epoch': 0.03}\n",
            "{'loss': 1.0487, 'learning_rate': 0.00029123173277661795, 'epoch': 0.03}\n",
            "{'loss': 1.2426, 'learning_rate': 0.0002906054279749478, 'epoch': 0.03}\n",
            "{'loss': 1.2544, 'learning_rate': 0.00028997912317327767, 'epoch': 0.04}\n",
            "{'loss': 1.2009, 'learning_rate': 0.00028935281837160745, 'epoch': 0.04}\n",
            "{'loss': 1.084, 'learning_rate': 0.00028872651356993734, 'epoch': 0.04}\n",
            "{'loss': 1.0767, 'learning_rate': 0.00028810020876826717, 'epoch': 0.04}\n",
            "  4% 100/2400 [01:38<13:00,  2.95it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1750] due to args.save_total_limit\n",
            "{'loss': 1.1061, 'learning_rate': 0.00028747390396659706, 'epoch': 0.04}\n",
            "{'loss': 1.1003, 'learning_rate': 0.0002868475991649269, 'epoch': 0.05}\n",
            "{'loss': 1.0837, 'learning_rate': 0.0002862212943632568, 'epoch': 0.05}\n",
            "{'loss': 1.3678, 'learning_rate': 0.0002855949895615866, 'epoch': 0.05}\n",
            "{'loss': 0.9471, 'learning_rate': 0.00028496868475991644, 'epoch': 0.05}\n",
            "{'loss': 0.8999, 'learning_rate': 0.0002843423799582463, 'epoch': 0.05}\n",
            "{'loss': 0.8365, 'learning_rate': 0.00028371607515657616, 'epoch': 0.06}\n",
            "{'loss': 0.9868, 'learning_rate': 0.00028308977035490605, 'epoch': 0.06}\n",
            "{'loss': 1.1672, 'learning_rate': 0.0002824634655532359, 'epoch': 0.06}\n",
            "{'loss': 0.811, 'learning_rate': 0.00028183716075156576, 'epoch': 0.06}\n",
            "  6% 150/2400 [01:55<12:35,  2.98it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1800] due to args.save_total_limit\n",
            "{'loss': 1.0681, 'learning_rate': 0.0002812108559498956, 'epoch': 0.06}\n",
            "{'loss': 0.983, 'learning_rate': 0.00028058455114822543, 'epoch': 0.07}\n",
            "{'loss': 0.9787, 'learning_rate': 0.0002799582463465553, 'epoch': 0.07}\n",
            "{'loss': 0.9685, 'learning_rate': 0.00027933194154488515, 'epoch': 0.07}\n",
            "{'loss': 1.1554, 'learning_rate': 0.00027870563674321503, 'epoch': 0.07}\n",
            "{'loss': 1.001, 'learning_rate': 0.00027807933194154487, 'epoch': 0.07}\n",
            "{'loss': 0.9046, 'learning_rate': 0.00027745302713987475, 'epoch': 0.08}\n",
            "{'loss': 1.103, 'learning_rate': 0.00027682672233820453, 'epoch': 0.08}\n",
            "{'loss': 0.8736, 'learning_rate': 0.0002762004175365344, 'epoch': 0.08}\n",
            "{'loss': 1.1875, 'learning_rate': 0.00027557411273486425, 'epoch': 0.08}\n",
            "  8% 200/2400 [02:12<12:04,  3.04it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-50] due to args.save_total_limit\n",
            "{'loss': 0.7765, 'learning_rate': 0.00027494780793319414, 'epoch': 0.09}\n",
            "{'loss': 0.9442, 'learning_rate': 0.00027432150313152397, 'epoch': 0.09}\n",
            "{'loss': 1.1418, 'learning_rate': 0.00027369519832985386, 'epoch': 0.09}\n",
            "{'loss': 1.0368, 'learning_rate': 0.0002730688935281837, 'epoch': 0.09}\n",
            "{'loss': 1.1968, 'learning_rate': 0.0002724425887265135, 'epoch': 0.09}\n",
            "{'loss': 1.0251, 'learning_rate': 0.0002718162839248434, 'epoch': 0.1}\n",
            "{'loss': 1.0071, 'learning_rate': 0.00027118997912317324, 'epoch': 0.1}\n",
            "{'loss': 1.0101, 'learning_rate': 0.00027056367432150313, 'epoch': 0.1}\n",
            "{'loss': 1.0897, 'learning_rate': 0.00026993736951983296, 'epoch': 0.1}\n",
            "{'loss': 1.151, 'learning_rate': 0.00026931106471816285, 'epoch': 0.1}\n",
            " 10% 250/2400 [02:28<11:49,  3.03it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 1.0422, 'learning_rate': 0.0002686847599164927, 'epoch': 0.11}\n",
            "{'loss': 1.186, 'learning_rate': 0.0002680584551148225, 'epoch': 0.11}\n",
            "{'loss': 0.9646, 'learning_rate': 0.0002674321503131524, 'epoch': 0.11}\n",
            "{'loss': 1.2669, 'learning_rate': 0.00026680584551148223, 'epoch': 0.11}\n",
            "{'loss': 0.9432, 'learning_rate': 0.00026617954070981206, 'epoch': 0.11}\n",
            "{'loss': 1.0879, 'learning_rate': 0.00026555323590814195, 'epoch': 0.12}\n",
            "{'loss': 0.9143, 'learning_rate': 0.0002649269311064718, 'epoch': 0.12}\n",
            "{'loss': 0.9497, 'learning_rate': 0.0002643006263048016, 'epoch': 0.12}\n",
            "{'loss': 1.008, 'learning_rate': 0.0002636743215031315, 'epoch': 0.12}\n",
            "{'loss': 0.9961, 'learning_rate': 0.00026304801670146133, 'epoch': 0.12}\n",
            " 12% 300/2400 [02:45<11:51,  2.95it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-150] due to args.save_total_limit\n",
            "{'loss': 0.8899, 'learning_rate': 0.0002624217118997912, 'epoch': 0.13}\n",
            "{'loss': 0.948, 'learning_rate': 0.00026179540709812105, 'epoch': 0.13}\n",
            "{'loss': 0.9315, 'learning_rate': 0.00026116910229645094, 'epoch': 0.13}\n",
            "{'loss': 1.0612, 'learning_rate': 0.00026054279749478077, 'epoch': 0.13}\n",
            "{'loss': 0.9063, 'learning_rate': 0.0002599164926931106, 'epoch': 0.14}\n",
            "{'loss': 0.8265, 'learning_rate': 0.0002592901878914405, 'epoch': 0.14}\n",
            "{'loss': 0.8925, 'learning_rate': 0.0002586638830897703, 'epoch': 0.14}\n",
            "{'loss': 1.0535, 'learning_rate': 0.0002580375782881002, 'epoch': 0.14}\n",
            "{'loss': 1.1065, 'learning_rate': 0.00025741127348643004, 'epoch': 0.14}\n",
            "{'loss': 1.1506, 'learning_rate': 0.00025678496868475993, 'epoch': 0.15}\n",
            " 15% 350/2400 [03:02<11:52,  2.88it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.9314, 'learning_rate': 0.0002561586638830897, 'epoch': 0.15}\n",
            "{'loss': 1.0074, 'learning_rate': 0.0002555323590814196, 'epoch': 0.15}\n",
            "{'loss': 0.8482, 'learning_rate': 0.0002549060542797494, 'epoch': 0.15}\n",
            "{'loss': 1.054, 'learning_rate': 0.0002542797494780793, 'epoch': 0.15}\n",
            "{'loss': 0.9758, 'learning_rate': 0.00025365344467640914, 'epoch': 0.16}\n",
            "{'loss': 0.882, 'learning_rate': 0.00025302713987473903, 'epoch': 0.16}\n",
            "{'loss': 0.9782, 'learning_rate': 0.00025240083507306886, 'epoch': 0.16}\n",
            "{'loss': 1.0971, 'learning_rate': 0.0002517745302713987, 'epoch': 0.16}\n",
            "{'loss': 1.0987, 'learning_rate': 0.0002511482254697286, 'epoch': 0.16}\n",
            "{'loss': 1.0217, 'learning_rate': 0.0002505219206680584, 'epoch': 0.17}\n",
            " 17% 400/2400 [03:19<11:18,  2.95it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.9612, 'learning_rate': 0.0002498956158663883, 'epoch': 0.17}\n",
            "{'loss': 1.1218, 'learning_rate': 0.00024926931106471813, 'epoch': 0.17}\n",
            "{'loss': 0.8757, 'learning_rate': 0.000248643006263048, 'epoch': 0.17}\n",
            "{'loss': 1.0222, 'learning_rate': 0.00024801670146137785, 'epoch': 0.17}\n",
            "{'loss': 1.034, 'learning_rate': 0.0002473903966597077, 'epoch': 0.18}\n",
            "{'loss': 0.9843, 'learning_rate': 0.00024676409185803757, 'epoch': 0.18}\n",
            "{'loss': 1.0713, 'learning_rate': 0.0002461377870563674, 'epoch': 0.18}\n",
            "{'loss': 0.8552, 'learning_rate': 0.0002455114822546973, 'epoch': 0.18}\n",
            "{'loss': 1.1333, 'learning_rate': 0.0002448851774530271, 'epoch': 0.19}\n",
            "{'loss': 0.9276, 'learning_rate': 0.000244258872651357, 'epoch': 0.19}\n",
            " 19% 450/2400 [03:36<11:00,  2.95it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-450\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.9954, 'learning_rate': 0.00024363256784968682, 'epoch': 0.19}\n",
            "{'loss': 0.935, 'learning_rate': 0.00024300626304801667, 'epoch': 0.19}\n",
            "{'loss': 1.0798, 'learning_rate': 0.00024237995824634653, 'epoch': 0.19}\n",
            "{'loss': 1.0748, 'learning_rate': 0.0002417536534446764, 'epoch': 0.2}\n",
            "{'loss': 0.9979, 'learning_rate': 0.00024112734864300625, 'epoch': 0.2}\n",
            "{'loss': 1.1063, 'learning_rate': 0.0002405010438413361, 'epoch': 0.2}\n",
            "{'loss': 0.949, 'learning_rate': 0.00023987473903966597, 'epoch': 0.2}\n",
            "{'loss': 1.0822, 'learning_rate': 0.00023924843423799578, 'epoch': 0.2}\n",
            "{'loss': 1.113, 'learning_rate': 0.00023862212943632564, 'epoch': 0.21}\n",
            "{'loss': 1.032, 'learning_rate': 0.0002379958246346555, 'epoch': 0.21}\n",
            " 21% 500/2400 [03:53<10:29,  3.02it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-350] due to args.save_total_limit\n",
            "{'loss': 0.9099, 'learning_rate': 0.00023736951983298536, 'epoch': 0.21}\n",
            "{'loss': 1.1964, 'learning_rate': 0.00023674321503131522, 'epoch': 0.21}\n",
            "{'loss': 0.8111, 'learning_rate': 0.00023611691022964508, 'epoch': 0.21}\n",
            "{'loss': 0.8291, 'learning_rate': 0.00023549060542797493, 'epoch': 0.22}\n",
            "{'loss': 0.8555, 'learning_rate': 0.00023486430062630477, 'epoch': 0.22}\n",
            "{'loss': 0.9164, 'learning_rate': 0.00023423799582463463, 'epoch': 0.22}\n",
            "{'loss': 0.7617, 'learning_rate': 0.00023361169102296449, 'epoch': 0.22}\n",
            "{'loss': 0.9821, 'learning_rate': 0.00023298538622129435, 'epoch': 0.23}\n",
            "{'loss': 0.9572, 'learning_rate': 0.0002323590814196242, 'epoch': 0.23}\n",
            "{'loss': 1.064, 'learning_rate': 0.00023173277661795406, 'epoch': 0.23}\n",
            " 23% 550/2400 [04:10<10:41,  2.88it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-550\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.8134, 'learning_rate': 0.0002311064718162839, 'epoch': 0.23}\n",
            "{'loss': 1.013, 'learning_rate': 0.00023048016701461376, 'epoch': 0.23}\n",
            "{'loss': 0.9138, 'learning_rate': 0.00022985386221294362, 'epoch': 0.24}\n",
            "{'loss': 0.9901, 'learning_rate': 0.00022922755741127348, 'epoch': 0.24}\n",
            "{'loss': 0.9054, 'learning_rate': 0.00022860125260960334, 'epoch': 0.24}\n",
            "{'loss': 1.0744, 'learning_rate': 0.0002279749478079332, 'epoch': 0.24}\n",
            "{'loss': 1.165, 'learning_rate': 0.00022734864300626305, 'epoch': 0.24}\n",
            "{'loss': 0.7993, 'learning_rate': 0.00022672233820459286, 'epoch': 0.25}\n",
            "{'loss': 0.9647, 'learning_rate': 0.00022609603340292272, 'epoch': 0.25}\n",
            "{'loss': 0.8629, 'learning_rate': 0.00022546972860125258, 'epoch': 0.25}\n",
            " 25% 600/2400 [04:27<09:56,  3.02it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: token_type_ids, instruction, language, source, input, output. If token_type_ids, instruction, language, source, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:34,  4.24it/s]\u001b[A\n",
            "  2% 3/150 [00:00<00:49,  2.98it/s]\u001b[A\n",
            "  3% 4/150 [00:01<00:56,  2.58it/s]\u001b[A\n",
            "  3% 5/150 [00:01<01:00,  2.39it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:02,  2.29it/s]\u001b[A\n",
            "  5% 7/150 [00:02<00:59,  2.40it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:01,  2.29it/s]\u001b[A\n",
            "  6% 9/150 [00:03<01:03,  2.23it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:03,  2.19it/s]\u001b[A\n",
            "  7% 11/150 [00:04<01:04,  2.16it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:04,  2.14it/s]\u001b[A\n",
            "  9% 13/150 [00:06<01:55,  1.19it/s]\u001b[A\n",
            "  9% 14/150 [00:07<01:39,  1.36it/s]\u001b[A\n",
            " 10% 15/150 [00:07<01:28,  1.52it/s]\u001b[A\n",
            " 11% 16/150 [00:08<01:20,  1.66it/s]\u001b[A\n",
            " 11% 17/150 [00:08<01:14,  1.79it/s]\u001b[A\n",
            " 12% 18/150 [00:09<01:10,  1.87it/s]\u001b[A\n",
            " 13% 19/150 [00:09<01:07,  1.93it/s]\u001b[A\n",
            " 13% 20/150 [00:10<01:05,  1.98it/s]\u001b[A\n",
            " 14% 21/150 [00:10<01:04,  2.02it/s]\u001b[A\n",
            " 15% 22/150 [00:11<01:02,  2.04it/s]\u001b[A\n",
            " 15% 23/150 [00:11<01:01,  2.06it/s]\u001b[A\n",
            " 16% 24/150 [00:12<01:00,  2.07it/s]\u001b[A\n",
            " 17% 25/150 [00:12<00:59,  2.08it/s]\u001b[A\n",
            " 17% 26/150 [00:13<00:59,  2.09it/s]\u001b[A\n",
            " 18% 27/150 [00:13<00:58,  2.10it/s]\u001b[A\n",
            " 19% 28/150 [00:13<00:58,  2.10it/s]\u001b[A\n",
            " 19% 29/150 [00:14<00:57,  2.10it/s]\u001b[A\n",
            " 20% 30/150 [00:14<00:57,  2.10it/s]\u001b[A\n",
            " 21% 31/150 [00:15<00:56,  2.11it/s]\u001b[A\n",
            " 21% 32/150 [00:15<00:56,  2.11it/s]\u001b[A\n",
            " 22% 33/150 [00:16<00:55,  2.11it/s]\u001b[A\n",
            " 23% 34/150 [00:16<00:55,  2.11it/s]\u001b[A\n",
            " 23% 35/150 [00:17<00:54,  2.11it/s]\u001b[A\n",
            " 24% 36/150 [00:17<00:54,  2.11it/s]\u001b[A\n",
            " 25% 37/150 [00:18<00:53,  2.11it/s]\u001b[A\n",
            " 25% 38/150 [00:18<00:53,  2.11it/s]\u001b[A\n",
            " 26% 39/150 [00:19<00:52,  2.11it/s]\u001b[A\n",
            " 27% 40/150 [00:19<00:52,  2.11it/s]\u001b[A\n",
            " 27% 41/150 [00:20<00:51,  2.11it/s]\u001b[A\n",
            " 28% 42/150 [00:20<00:51,  2.11it/s]\u001b[A\n",
            " 29% 43/150 [00:21<00:50,  2.11it/s]\u001b[A\n",
            " 29% 44/150 [00:21<00:50,  2.11it/s]\u001b[A\n",
            " 30% 45/150 [00:22<00:49,  2.11it/s]\u001b[A\n",
            " 31% 46/150 [00:22<00:48,  2.17it/s]\u001b[A\n",
            " 31% 47/150 [00:22<00:48,  2.14it/s]\u001b[A\n",
            " 32% 48/150 [00:23<00:47,  2.13it/s]\u001b[A\n",
            " 33% 49/150 [00:23<00:47,  2.12it/s]\u001b[A\n",
            " 33% 50/150 [00:24<00:47,  2.12it/s]\u001b[A\n",
            " 34% 51/150 [00:24<00:46,  2.11it/s]\u001b[A\n",
            " 35% 52/150 [00:25<00:46,  2.10it/s]\u001b[A\n",
            " 35% 53/150 [00:25<00:46,  2.09it/s]\u001b[A\n",
            " 36% 54/150 [00:26<00:45,  2.09it/s]\u001b[A\n",
            " 37% 55/150 [00:26<00:45,  2.10it/s]\u001b[A\n",
            " 37% 56/150 [00:27<00:44,  2.10it/s]\u001b[A\n",
            " 38% 57/150 [00:27<00:44,  2.10it/s]\u001b[A\n",
            " 39% 58/150 [00:28<00:43,  2.13it/s]\u001b[A\n",
            " 39% 59/150 [00:28<00:43,  2.11it/s]\u001b[A\n",
            " 40% 60/150 [00:29<00:42,  2.11it/s]\u001b[A\n",
            " 41% 61/150 [00:29<00:42,  2.11it/s]\u001b[A\n",
            " 41% 62/150 [00:30<00:41,  2.11it/s]\u001b[A\n",
            " 42% 63/150 [00:30<00:41,  2.11it/s]\u001b[A\n",
            " 43% 64/150 [00:31<00:40,  2.11it/s]\u001b[A\n",
            " 43% 65/150 [00:31<00:40,  2.11it/s]\u001b[A\n",
            " 44% 66/150 [00:31<00:39,  2.11it/s]\u001b[A\n",
            " 45% 67/150 [00:32<00:39,  2.11it/s]\u001b[A\n",
            " 45% 68/150 [00:32<00:38,  2.11it/s]\u001b[A\n",
            " 46% 69/150 [00:33<00:38,  2.11it/s]\u001b[A\n",
            " 47% 70/150 [00:33<00:37,  2.13it/s]\u001b[A\n",
            " 47% 71/150 [00:34<00:36,  2.15it/s]\u001b[A\n",
            " 48% 72/150 [00:34<00:36,  2.13it/s]\u001b[A\n",
            " 49% 73/150 [00:35<00:36,  2.12it/s]\u001b[A\n",
            " 49% 74/150 [00:35<00:35,  2.12it/s]\u001b[A\n",
            " 50% 75/150 [00:46<04:34,  3.66s/it]\u001b[A\n",
            " 51% 76/150 [00:47<03:20,  2.71s/it]\u001b[A\n",
            " 51% 77/150 [00:47<02:26,  2.00s/it]\u001b[A\n",
            " 52% 78/150 [00:48<01:51,  1.55s/it]\u001b[A\n",
            " 53% 79/150 [00:48<01:26,  1.22s/it]\u001b[A\n",
            " 53% 80/150 [00:49<01:09,  1.00it/s]\u001b[A\n",
            " 54% 81/150 [00:49<00:58,  1.19it/s]\u001b[A\n",
            " 55% 82/150 [00:50<00:49,  1.37it/s]\u001b[A\n",
            " 55% 83/150 [00:50<00:43,  1.53it/s]\u001b[A\n",
            " 56% 84/150 [00:51<00:39,  1.66it/s]\u001b[A\n",
            " 57% 85/150 [00:51<00:36,  1.78it/s]\u001b[A\n",
            " 57% 86/150 [00:51<00:34,  1.86it/s]\u001b[A\n",
            " 58% 87/150 [00:52<00:31,  1.98it/s]\u001b[A\n",
            " 59% 88/150 [00:52<00:30,  2.01it/s]\u001b[A\n",
            " 59% 89/150 [00:53<00:29,  2.04it/s]\u001b[A\n",
            " 60% 90/150 [00:53<00:29,  2.06it/s]\u001b[A\n",
            " 61% 91/150 [00:54<00:28,  2.07it/s]\u001b[A\n",
            " 61% 92/150 [00:54<00:27,  2.08it/s]\u001b[A\n",
            " 62% 93/150 [00:55<00:25,  2.25it/s]\u001b[A\n",
            " 63% 94/150 [00:55<00:25,  2.20it/s]\u001b[A\n",
            " 63% 95/150 [00:56<00:25,  2.17it/s]\u001b[A\n",
            " 64% 96/150 [00:56<00:25,  2.15it/s]\u001b[A\n",
            " 65% 97/150 [00:57<00:24,  2.14it/s]\u001b[A\n",
            " 65% 98/150 [00:57<00:24,  2.13it/s]\u001b[A\n",
            " 66% 99/150 [00:57<00:24,  2.12it/s]\u001b[A\n",
            " 67% 100/150 [00:58<00:23,  2.12it/s]\u001b[A\n",
            " 67% 101/150 [00:58<00:23,  2.11it/s]\u001b[A\n",
            " 68% 102/150 [00:59<00:22,  2.11it/s]\u001b[A\n",
            " 69% 103/150 [00:59<00:22,  2.11it/s]\u001b[A\n",
            " 69% 104/150 [01:00<00:21,  2.16it/s]\u001b[A\n",
            " 70% 105/150 [01:00<00:21,  2.13it/s]\u001b[A\n",
            " 71% 106/150 [01:01<00:20,  2.14it/s]\u001b[A\n",
            " 71% 107/150 [01:01<00:20,  2.12it/s]\u001b[A\n",
            " 72% 108/150 [01:02<00:19,  2.11it/s]\u001b[A\n",
            " 73% 109/150 [01:02<00:19,  2.11it/s]\u001b[A\n",
            " 73% 110/150 [01:03<00:17,  2.27it/s]\u001b[A\n",
            " 74% 111/150 [01:03<00:17,  2.21it/s]\u001b[A\n",
            " 75% 112/150 [01:04<00:17,  2.18it/s]\u001b[A\n",
            " 75% 113/150 [01:04<00:16,  2.19it/s]\u001b[A\n",
            " 76% 114/150 [01:04<00:16,  2.15it/s]\u001b[A\n",
            " 77% 115/150 [01:05<00:16,  2.17it/s]\u001b[A\n",
            " 77% 116/150 [01:05<00:15,  2.16it/s]\u001b[A\n",
            " 78% 117/150 [01:06<00:15,  2.13it/s]\u001b[A\n",
            " 79% 118/150 [01:06<00:15,  2.12it/s]\u001b[A\n",
            " 79% 119/150 [01:07<00:14,  2.12it/s]\u001b[A\n",
            " 80% 120/150 [01:07<00:14,  2.11it/s]\u001b[A\n",
            " 81% 121/150 [01:08<00:13,  2.11it/s]\u001b[A\n",
            " 81% 122/150 [01:08<00:13,  2.11it/s]\u001b[A\n",
            " 82% 123/150 [01:09<00:12,  2.11it/s]\u001b[A\n",
            " 83% 124/150 [01:09<00:12,  2.11it/s]\u001b[A\n",
            " 83% 125/150 [01:10<00:11,  2.11it/s]\u001b[A\n",
            " 84% 126/150 [01:10<00:11,  2.09it/s]\u001b[A\n",
            " 85% 127/150 [01:11<00:11,  2.08it/s]\u001b[A\n",
            " 85% 128/150 [01:11<00:10,  2.09it/s]\u001b[A\n",
            " 86% 129/150 [01:12<00:10,  2.09it/s]\u001b[A\n",
            " 87% 130/150 [01:12<00:09,  2.13it/s]\u001b[A\n",
            " 87% 131/150 [01:13<00:08,  2.11it/s]\u001b[A\n",
            " 88% 132/150 [01:13<00:08,  2.11it/s]\u001b[A\n",
            " 89% 133/150 [01:13<00:07,  2.14it/s]\u001b[A\n",
            " 89% 134/150 [01:14<00:07,  2.12it/s]\u001b[A\n",
            " 90% 135/150 [01:14<00:07,  2.11it/s]\u001b[A\n",
            " 91% 136/150 [01:15<00:06,  2.18it/s]\u001b[A\n",
            " 91% 137/150 [01:15<00:06,  2.15it/s]\u001b[A\n",
            " 92% 138/150 [01:16<00:05,  2.13it/s]\u001b[A\n",
            " 93% 139/150 [01:16<00:05,  2.12it/s]\u001b[A\n",
            " 93% 140/150 [01:17<00:04,  2.14it/s]\u001b[A\n",
            " 94% 141/150 [01:17<00:04,  2.12it/s]\u001b[A\n",
            " 95% 142/150 [01:18<00:03,  2.12it/s]\u001b[A\n",
            " 95% 143/150 [01:18<00:03,  2.11it/s]\u001b[A\n",
            " 96% 144/150 [01:19<00:02,  2.11it/s]\u001b[A\n",
            " 97% 145/150 [01:19<00:02,  2.11it/s]\u001b[A\n",
            " 97% 146/150 [01:20<00:01,  2.11it/s]\u001b[A\n",
            " 98% 147/150 [01:20<00:01,  2.11it/s]\u001b[A\n",
            " 99% 148/150 [01:21<00:00,  2.11it/s]\u001b[A\n",
            " 99% 149/150 [01:21<00:00,  2.11it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.0926212072372437, 'eval_runtime': 94.7345, 'eval_samples_per_second': 12.667, 'eval_steps_per_second': 1.583, 'epoch': 0.25}\n",
            " 25% 600/2400 [06:01<09:56,  3.02it/s]\n",
            "100% 150/150 [01:21<00:00,  2.11it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-450] due to args.save_total_limit\n",
            "{'loss': 1.0501, 'learning_rate': 0.00022484342379958244, 'epoch': 0.25}\n",
            "{'loss': 1.0442, 'learning_rate': 0.0002242171189979123, 'epoch': 0.25}\n",
            "{'loss': 0.9624, 'learning_rate': 0.00022359081419624216, 'epoch': 0.26}\n",
            "{'loss': 0.9268, 'learning_rate': 0.00022296450939457202, 'epoch': 0.26}\n",
            "{'loss': 0.9835, 'learning_rate': 0.00022233820459290185, 'epoch': 0.26}\n",
            "{'loss': 0.9653, 'learning_rate': 0.0002217118997912317, 'epoch': 0.26}\n",
            "{'loss': 0.9796, 'learning_rate': 0.00022108559498956157, 'epoch': 0.26}\n",
            "{'loss': 0.858, 'learning_rate': 0.00022045929018789143, 'epoch': 0.27}\n",
            "{'loss': 1.0296, 'learning_rate': 0.0002198329853862213, 'epoch': 0.27}\n",
            "{'loss': 1.2268, 'learning_rate': 0.00021920668058455115, 'epoch': 0.27}\n",
            " 27% 650/2400 [06:18<09:36,  3.04it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-650\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 1.0831, 'learning_rate': 0.00021858037578288098, 'epoch': 0.27}\n",
            "{'loss': 0.989, 'learning_rate': 0.00021795407098121084, 'epoch': 0.28}\n",
            "{'loss': 0.9504, 'learning_rate': 0.0002173277661795407, 'epoch': 0.28}\n",
            "{'loss': 1.0495, 'learning_rate': 0.00021670146137787056, 'epoch': 0.28}\n",
            "{'loss': 1.1505, 'learning_rate': 0.0002160751565762004, 'epoch': 0.28}\n",
            "{'loss': 0.9637, 'learning_rate': 0.00021544885177453025, 'epoch': 0.28}\n",
            "{'loss': 0.9686, 'learning_rate': 0.0002148225469728601, 'epoch': 0.29}\n",
            "{'loss': 1.0282, 'learning_rate': 0.00021419624217118994, 'epoch': 0.29}\n",
            "{'loss': 1.0153, 'learning_rate': 0.0002135699373695198, 'epoch': 0.29}\n",
            "{'loss': 1.0551, 'learning_rate': 0.00021294363256784966, 'epoch': 0.29}\n",
            " 29% 700/2400 [06:35<09:25,  3.00it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-550] due to args.save_total_limit\n",
            "{'loss': 0.8528, 'learning_rate': 0.00021231732776617952, 'epoch': 0.29}\n",
            "{'loss': 0.9261, 'learning_rate': 0.00021169102296450938, 'epoch': 0.3}\n",
            "{'loss': 1.023, 'learning_rate': 0.00021106471816283924, 'epoch': 0.3}\n",
            "{'loss': 1.0474, 'learning_rate': 0.0002104384133611691, 'epoch': 0.3}\n",
            "{'loss': 0.9888, 'learning_rate': 0.00020981210855949893, 'epoch': 0.3}\n",
            "{'loss': 1.0904, 'learning_rate': 0.0002091858037578288, 'epoch': 0.3}\n",
            "{'loss': 1.1375, 'learning_rate': 0.00020855949895615865, 'epoch': 0.31}\n",
            "{'loss': 0.9351, 'learning_rate': 0.0002079331941544885, 'epoch': 0.31}\n",
            "{'loss': 0.9929, 'learning_rate': 0.00020730688935281837, 'epoch': 0.31}\n",
            "{'loss': 0.945, 'learning_rate': 0.00020668058455114823, 'epoch': 0.31}\n",
            " 31% 750/2400 [06:52<09:17,  2.96it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.9818, 'learning_rate': 0.00020605427974947803, 'epoch': 0.31}\n",
            "{'loss': 0.9558, 'learning_rate': 0.0002054279749478079, 'epoch': 0.32}\n",
            "{'loss': 0.9046, 'learning_rate': 0.00020480167014613775, 'epoch': 0.32}\n",
            "{'loss': 1.0169, 'learning_rate': 0.0002041753653444676, 'epoch': 0.32}\n",
            "{'loss': 1.1618, 'learning_rate': 0.00020354906054279747, 'epoch': 0.32}\n",
            "{'loss': 1.1051, 'learning_rate': 0.00020292275574112733, 'epoch': 0.33}\n",
            "{'loss': 1.1741, 'learning_rate': 0.0002022964509394572, 'epoch': 0.33}\n",
            "{'loss': 0.9954, 'learning_rate': 0.00020167014613778702, 'epoch': 0.33}\n",
            "{'loss': 0.9034, 'learning_rate': 0.00020104384133611688, 'epoch': 0.33}\n",
            "{'loss': 1.0346, 'learning_rate': 0.00020041753653444674, 'epoch': 0.33}\n",
            " 33% 800/2400 [07:09<08:59,  2.97it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-650] due to args.save_total_limit\n",
            "{'loss': 1.0677, 'learning_rate': 0.0001997912317327766, 'epoch': 0.34}\n",
            "{'loss': 1.0241, 'learning_rate': 0.00019916492693110646, 'epoch': 0.34}\n",
            "{'loss': 0.9392, 'learning_rate': 0.00019853862212943632, 'epoch': 0.34}\n",
            "{'loss': 1.1927, 'learning_rate': 0.00019791231732776615, 'epoch': 0.34}\n",
            "{'loss': 0.9882, 'learning_rate': 0.000197286012526096, 'epoch': 0.34}\n",
            "{'loss': 1.022, 'learning_rate': 0.00019665970772442587, 'epoch': 0.35}\n",
            "{'loss': 1.163, 'learning_rate': 0.00019603340292275573, 'epoch': 0.35}\n",
            "{'loss': 1.0227, 'learning_rate': 0.0001954070981210856, 'epoch': 0.35}\n",
            "{'loss': 0.9157, 'learning_rate': 0.00019478079331941545, 'epoch': 0.35}\n",
            "{'loss': 0.8677, 'learning_rate': 0.0001941544885177453, 'epoch': 0.35}\n",
            " 35% 850/2400 [07:26<08:22,  3.08it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-850\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 1.014, 'learning_rate': 0.00019352818371607512, 'epoch': 0.36}\n",
            "{'loss': 0.8803, 'learning_rate': 0.00019290187891440498, 'epoch': 0.36}\n",
            "{'loss': 0.8978, 'learning_rate': 0.00019227557411273484, 'epoch': 0.36}\n",
            "{'loss': 0.9384, 'learning_rate': 0.0001916492693110647, 'epoch': 0.36}\n",
            "{'loss': 1.0634, 'learning_rate': 0.00019102296450939455, 'epoch': 0.36}\n",
            "{'loss': 0.9878, 'learning_rate': 0.00019039665970772441, 'epoch': 0.37}\n",
            "{'loss': 1.1658, 'learning_rate': 0.00018977035490605427, 'epoch': 0.37}\n",
            "{'loss': 1.0384, 'learning_rate': 0.0001891440501043841, 'epoch': 0.37}\n",
            "{'loss': 1.1302, 'learning_rate': 0.00018851774530271397, 'epoch': 0.37}\n",
            "{'loss': 0.9074, 'learning_rate': 0.00018789144050104382, 'epoch': 0.38}\n",
            " 38% 900/2400 [07:42<08:08,  3.07it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 1.0251, 'learning_rate': 0.00018726513569937368, 'epoch': 0.38}\n",
            "{'loss': 0.9846, 'learning_rate': 0.00018663883089770354, 'epoch': 0.38}\n",
            "{'loss': 1.0307, 'learning_rate': 0.0001860125260960334, 'epoch': 0.38}\n",
            "{'loss': 0.8371, 'learning_rate': 0.00018538622129436324, 'epoch': 0.38}\n",
            "{'loss': 0.9356, 'learning_rate': 0.0001847599164926931, 'epoch': 0.39}\n",
            "{'loss': 1.0763, 'learning_rate': 0.00018413361169102295, 'epoch': 0.39}\n",
            "{'loss': 0.9429, 'learning_rate': 0.00018350730688935281, 'epoch': 0.39}\n",
            "{'loss': 0.9833, 'learning_rate': 0.00018288100208768265, 'epoch': 0.39}\n",
            "{'loss': 1.1086, 'learning_rate': 0.0001822546972860125, 'epoch': 0.39}\n",
            "{'loss': 0.9301, 'learning_rate': 0.00018162839248434237, 'epoch': 0.4}\n",
            " 40% 950/2400 [07:59<08:16,  2.92it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-950\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.9957, 'learning_rate': 0.0001810020876826722, 'epoch': 0.4}\n",
            "{'loss': 1.1727, 'learning_rate': 0.00018037578288100206, 'epoch': 0.4}\n",
            "{'loss': 0.9393, 'learning_rate': 0.00017974947807933192, 'epoch': 0.4}\n",
            "{'loss': 0.9155, 'learning_rate': 0.00017912317327766178, 'epoch': 0.4}\n",
            "{'loss': 0.9847, 'learning_rate': 0.00017849686847599164, 'epoch': 0.41}\n",
            "{'loss': 1.1044, 'learning_rate': 0.0001778705636743215, 'epoch': 0.41}\n",
            "{'loss': 0.8991, 'learning_rate': 0.00017724425887265136, 'epoch': 0.41}\n",
            "{'loss': 0.841, 'learning_rate': 0.0001766179540709812, 'epoch': 0.41}\n",
            "{'loss': 0.8813, 'learning_rate': 0.00017599164926931105, 'epoch': 0.41}\n",
            "{'loss': 1.0444, 'learning_rate': 0.0001753653444676409, 'epoch': 0.42}\n",
            " 42% 1000/2400 [08:16<08:01,  2.91it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-850] due to args.save_total_limit\n",
            "{'loss': 0.8875, 'learning_rate': 0.00017473903966597077, 'epoch': 0.42}\n",
            "{'loss': 1.0584, 'learning_rate': 0.00017411273486430063, 'epoch': 0.42}\n",
            "{'loss': 1.0662, 'learning_rate': 0.00017348643006263049, 'epoch': 0.42}\n",
            "{'loss': 0.8656, 'learning_rate': 0.0001728601252609603, 'epoch': 0.42}\n",
            "{'loss': 1.0368, 'learning_rate': 0.00017223382045929015, 'epoch': 0.43}\n",
            "{'loss': 1.0193, 'learning_rate': 0.00017160751565762, 'epoch': 0.43}\n",
            "{'loss': 1.1094, 'learning_rate': 0.00017098121085594987, 'epoch': 0.43}\n",
            "{'loss': 1.0028, 'learning_rate': 0.00017035490605427973, 'epoch': 0.43}\n",
            "{'loss': 1.3104, 'learning_rate': 0.0001697286012526096, 'epoch': 0.44}\n",
            "{'loss': 0.9467, 'learning_rate': 0.00016910229645093945, 'epoch': 0.44}\n",
            " 44% 1050/2400 [08:33<07:30,  3.00it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1050\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 1.0476, 'learning_rate': 0.00016847599164926928, 'epoch': 0.44}\n",
            "{'loss': 1.0026, 'learning_rate': 0.00016784968684759914, 'epoch': 0.44}\n",
            "{'loss': 1.0179, 'learning_rate': 0.000167223382045929, 'epoch': 0.44}\n",
            "{'loss': 0.9919, 'learning_rate': 0.00016659707724425886, 'epoch': 0.45}\n",
            "{'loss': 0.979, 'learning_rate': 0.00016597077244258872, 'epoch': 0.45}\n",
            "{'loss': 0.9164, 'learning_rate': 0.00016534446764091858, 'epoch': 0.45}\n",
            "{'loss': 0.9639, 'learning_rate': 0.00016471816283924844, 'epoch': 0.45}\n",
            "{'loss': 1.0316, 'learning_rate': 0.00016409185803757827, 'epoch': 0.45}\n",
            "{'loss': 1.0503, 'learning_rate': 0.00016346555323590813, 'epoch': 0.46}\n",
            "{'loss': 1.0358, 'learning_rate': 0.000162839248434238, 'epoch': 0.46}\n",
            " 46% 1100/2400 [08:50<07:14,  2.99it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-950] due to args.save_total_limit\n",
            "{'loss': 0.8614, 'learning_rate': 0.00016221294363256785, 'epoch': 0.46}\n",
            "{'loss': 0.9386, 'learning_rate': 0.0001615866388308977, 'epoch': 0.46}\n",
            "{'loss': 0.8254, 'learning_rate': 0.00016096033402922757, 'epoch': 0.46}\n",
            "{'loss': 0.913, 'learning_rate': 0.00016033402922755737, 'epoch': 0.47}\n",
            "{'loss': 0.9252, 'learning_rate': 0.00015970772442588723, 'epoch': 0.47}\n",
            "{'loss': 0.9833, 'learning_rate': 0.0001590814196242171, 'epoch': 0.47}\n",
            "{'loss': 0.9633, 'learning_rate': 0.00015845511482254695, 'epoch': 0.47}\n",
            "{'loss': 1.272, 'learning_rate': 0.0001578288100208768, 'epoch': 0.47}\n",
            "{'loss': 1.1127, 'learning_rate': 0.00015720250521920667, 'epoch': 0.48}\n",
            "{'loss': 0.854, 'learning_rate': 0.00015657620041753653, 'epoch': 0.48}\n",
            " 48% 1150/2400 [09:07<07:00,  2.98it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 1.1614, 'learning_rate': 0.00015594989561586636, 'epoch': 0.48}\n",
            "{'loss': 0.9541, 'learning_rate': 0.00015532359081419622, 'epoch': 0.48}\n",
            "{'loss': 0.9425, 'learning_rate': 0.00015469728601252608, 'epoch': 0.49}\n",
            "{'loss': 1.1006, 'learning_rate': 0.00015407098121085594, 'epoch': 0.49}\n",
            "{'loss': 0.9072, 'learning_rate': 0.0001534446764091858, 'epoch': 0.49}\n",
            "{'loss': 1.3246, 'learning_rate': 0.00015281837160751566, 'epoch': 0.49}\n",
            "{'loss': 0.806, 'learning_rate': 0.00015219206680584552, 'epoch': 0.49}\n",
            "{'loss': 1.0706, 'learning_rate': 0.00015156576200417535, 'epoch': 0.5}\n",
            "{'loss': 1.065, 'learning_rate': 0.0001509394572025052, 'epoch': 0.5}\n",
            "{'loss': 0.9218, 'learning_rate': 0.00015031315240083507, 'epoch': 0.5}\n",
            " 50% 1200/2400 [09:24<06:52,  2.91it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: token_type_ids, instruction, language, source, input, output. If token_type_ids, instruction, language, source, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:35,  4.22it/s]\u001b[A\n",
            "  2% 3/150 [00:00<00:49,  2.97it/s]\u001b[A\n",
            "  3% 4/150 [00:01<00:56,  2.58it/s]\u001b[A\n",
            "  3% 5/150 [00:01<01:00,  2.39it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:02,  2.29it/s]\u001b[A\n",
            "  5% 7/150 [00:02<00:59,  2.41it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:01,  2.30it/s]\u001b[A\n",
            "  6% 9/150 [00:03<01:03,  2.24it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:03,  2.19it/s]\u001b[A\n",
            "  7% 11/150 [00:04<01:04,  2.17it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:04,  2.15it/s]\u001b[A\n",
            "  9% 13/150 [00:05<00:59,  2.29it/s]\u001b[A\n",
            "  9% 14/150 [00:05<01:01,  2.23it/s]\u001b[A\n",
            " 10% 15/150 [00:06<01:01,  2.19it/s]\u001b[A\n",
            " 11% 16/150 [00:06<01:01,  2.16it/s]\u001b[A\n",
            " 11% 17/150 [00:07<01:01,  2.17it/s]\u001b[A\n",
            " 12% 18/150 [00:07<01:02,  2.13it/s]\u001b[A\n",
            " 13% 19/150 [00:08<01:01,  2.12it/s]\u001b[A\n",
            " 13% 20/150 [00:08<01:01,  2.12it/s]\u001b[A\n",
            " 14% 21/150 [00:09<01:01,  2.11it/s]\u001b[A\n",
            " 15% 22/150 [00:09<01:00,  2.11it/s]\u001b[A\n",
            " 15% 23/150 [00:10<01:00,  2.11it/s]\u001b[A\n",
            " 16% 24/150 [00:10<00:59,  2.10it/s]\u001b[A\n",
            " 17% 25/150 [00:11<00:59,  2.10it/s]\u001b[A\n",
            " 17% 26/150 [00:11<00:59,  2.10it/s]\u001b[A\n",
            " 18% 27/150 [00:12<00:58,  2.10it/s]\u001b[A\n",
            " 19% 28/150 [00:12<00:58,  2.10it/s]\u001b[A\n",
            " 19% 29/150 [00:13<00:57,  2.10it/s]\u001b[A\n",
            " 20% 30/150 [00:13<00:57,  2.10it/s]\u001b[A\n",
            " 21% 31/150 [00:14<00:56,  2.10it/s]\u001b[A\n",
            " 21% 32/150 [00:14<00:56,  2.11it/s]\u001b[A\n",
            " 22% 33/150 [00:15<00:55,  2.11it/s]\u001b[A\n",
            " 23% 34/150 [00:15<00:55,  2.11it/s]\u001b[A\n",
            " 23% 35/150 [00:15<00:54,  2.11it/s]\u001b[A\n",
            " 24% 36/150 [00:16<00:54,  2.11it/s]\u001b[A\n",
            " 25% 37/150 [00:16<00:53,  2.11it/s]\u001b[A\n",
            " 25% 38/150 [00:17<00:53,  2.11it/s]\u001b[A\n",
            " 26% 39/150 [00:17<00:52,  2.11it/s]\u001b[A\n",
            " 27% 40/150 [00:18<00:52,  2.11it/s]\u001b[A\n",
            " 27% 41/150 [00:18<00:51,  2.11it/s]\u001b[A\n",
            " 28% 42/150 [00:19<00:51,  2.11it/s]\u001b[A\n",
            " 29% 43/150 [00:19<00:50,  2.11it/s]\u001b[A\n",
            " 29% 44/150 [00:20<00:50,  2.11it/s]\u001b[A\n",
            " 30% 45/150 [00:20<00:49,  2.11it/s]\u001b[A\n",
            " 31% 46/150 [00:21<00:48,  2.16it/s]\u001b[A\n",
            " 31% 47/150 [00:21<00:48,  2.14it/s]\u001b[A\n",
            " 32% 48/150 [00:22<00:47,  2.13it/s]\u001b[A\n",
            " 33% 49/150 [00:22<00:47,  2.12it/s]\u001b[A\n",
            " 33% 50/150 [00:23<00:47,  2.12it/s]\u001b[A\n",
            " 34% 51/150 [00:23<00:46,  2.11it/s]\u001b[A\n",
            " 35% 52/150 [00:23<00:46,  2.10it/s]\u001b[A\n",
            " 35% 53/150 [00:24<00:46,  2.09it/s]\u001b[A\n",
            " 36% 54/150 [00:24<00:45,  2.09it/s]\u001b[A\n",
            " 37% 55/150 [00:25<00:45,  2.09it/s]\u001b[A\n",
            " 37% 56/150 [00:25<00:44,  2.10it/s]\u001b[A\n",
            " 38% 57/150 [00:26<00:44,  2.10it/s]\u001b[A\n",
            " 39% 58/150 [00:26<00:43,  2.12it/s]\u001b[A\n",
            " 39% 59/150 [00:27<00:43,  2.11it/s]\u001b[A\n",
            " 40% 60/150 [00:27<00:42,  2.11it/s]\u001b[A\n",
            " 41% 61/150 [00:28<00:42,  2.11it/s]\u001b[A\n",
            " 41% 62/150 [00:28<00:41,  2.11it/s]\u001b[A\n",
            " 42% 63/150 [00:29<00:41,  2.11it/s]\u001b[A\n",
            " 43% 64/150 [00:29<00:40,  2.11it/s]\u001b[A\n",
            " 43% 65/150 [00:30<00:40,  2.11it/s]\u001b[A\n",
            " 44% 66/150 [00:30<00:39,  2.11it/s]\u001b[A\n",
            " 45% 67/150 [00:31<00:39,  2.11it/s]\u001b[A\n",
            " 45% 68/150 [00:31<00:38,  2.11it/s]\u001b[A\n",
            " 46% 69/150 [00:32<00:38,  2.11it/s]\u001b[A\n",
            " 47% 70/150 [00:32<00:37,  2.13it/s]\u001b[A\n",
            " 47% 71/150 [00:32<00:36,  2.16it/s]\u001b[A\n",
            " 48% 72/150 [00:33<00:36,  2.13it/s]\u001b[A\n",
            " 49% 73/150 [00:33<00:36,  2.13it/s]\u001b[A\n",
            " 49% 74/150 [00:34<00:35,  2.12it/s]\u001b[A\n",
            " 50% 75/150 [00:34<00:32,  2.28it/s]\u001b[A\n",
            " 51% 76/150 [00:35<00:33,  2.22it/s]\u001b[A\n",
            " 51% 77/150 [00:35<00:30,  2.36it/s]\u001b[A\n",
            " 52% 78/150 [00:36<00:31,  2.27it/s]\u001b[A\n",
            " 53% 79/150 [00:36<00:32,  2.21it/s]\u001b[A\n",
            " 53% 80/150 [00:37<00:32,  2.18it/s]\u001b[A\n",
            " 54% 81/150 [00:37<00:31,  2.16it/s]\u001b[A\n",
            " 55% 82/150 [00:37<00:31,  2.14it/s]\u001b[A\n",
            " 55% 83/150 [00:38<00:31,  2.13it/s]\u001b[A\n",
            " 56% 84/150 [00:38<00:31,  2.12it/s]\u001b[A\n",
            " 57% 85/150 [00:39<00:30,  2.12it/s]\u001b[A\n",
            " 57% 86/150 [00:39<00:30,  2.11it/s]\u001b[A\n",
            " 58% 87/150 [00:40<00:29,  2.17it/s]\u001b[A\n",
            " 59% 88/150 [00:40<00:29,  2.14it/s]\u001b[A\n",
            " 59% 89/150 [00:41<00:28,  2.13it/s]\u001b[A\n",
            " 60% 90/150 [00:41<00:28,  2.12it/s]\u001b[A\n",
            " 61% 91/150 [00:42<00:27,  2.12it/s]\u001b[A\n",
            " 61% 92/150 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 62% 93/150 [00:43<00:25,  2.27it/s]\u001b[A\n",
            " 63% 94/150 [00:43<00:25,  2.22it/s]\u001b[A\n",
            " 63% 95/150 [00:44<00:25,  2.18it/s]\u001b[A\n",
            " 64% 96/150 [00:44<00:25,  2.16it/s]\u001b[A\n",
            " 65% 97/150 [00:44<00:24,  2.14it/s]\u001b[A\n",
            " 65% 98/150 [00:45<00:24,  2.13it/s]\u001b[A\n",
            " 66% 99/150 [00:45<00:24,  2.12it/s]\u001b[A\n",
            " 67% 100/150 [00:46<00:23,  2.12it/s]\u001b[A\n",
            " 67% 101/150 [00:46<00:23,  2.12it/s]\u001b[A\n",
            " 68% 102/150 [00:47<00:22,  2.11it/s]\u001b[A\n",
            " 69% 103/150 [00:47<00:22,  2.11it/s]\u001b[A\n",
            " 69% 104/150 [00:48<00:21,  2.17it/s]\u001b[A\n",
            " 70% 105/150 [00:48<00:21,  2.14it/s]\u001b[A\n",
            " 71% 106/150 [00:49<00:20,  2.15it/s]\u001b[A\n",
            " 71% 107/150 [00:49<00:20,  2.13it/s]\u001b[A\n",
            " 72% 108/150 [00:50<00:19,  2.12it/s]\u001b[A\n",
            " 73% 109/150 [00:50<00:19,  2.12it/s]\u001b[A\n",
            " 73% 110/150 [00:50<00:17,  2.27it/s]\u001b[A\n",
            " 74% 111/150 [00:51<00:17,  2.22it/s]\u001b[A\n",
            " 75% 112/150 [00:51<00:17,  2.18it/s]\u001b[A\n",
            " 75% 113/150 [00:52<00:16,  2.19it/s]\u001b[A\n",
            " 76% 114/150 [00:52<00:16,  2.15it/s]\u001b[A\n",
            " 77% 115/150 [00:53<00:16,  2.17it/s]\u001b[A\n",
            " 77% 116/150 [00:53<00:15,  2.16it/s]\u001b[A\n",
            " 78% 117/150 [00:54<00:15,  2.13it/s]\u001b[A\n",
            " 79% 118/150 [00:54<00:15,  2.12it/s]\u001b[A\n",
            " 79% 119/150 [00:55<00:14,  2.11it/s]\u001b[A\n",
            " 80% 120/150 [00:55<00:14,  2.11it/s]\u001b[A\n",
            " 81% 121/150 [00:56<00:13,  2.11it/s]\u001b[A\n",
            " 81% 122/150 [00:56<00:13,  2.11it/s]\u001b[A\n",
            " 82% 123/150 [00:57<00:12,  2.11it/s]\u001b[A\n",
            " 83% 124/150 [00:57<00:12,  2.11it/s]\u001b[A\n",
            " 83% 125/150 [00:58<00:11,  2.11it/s]\u001b[A\n",
            " 84% 126/150 [00:58<00:11,  2.10it/s]\u001b[A\n",
            " 85% 127/150 [00:59<00:11,  2.09it/s]\u001b[A\n",
            " 85% 128/150 [00:59<00:10,  2.09it/s]\u001b[A\n",
            " 86% 129/150 [01:00<00:10,  2.09it/s]\u001b[A\n",
            " 87% 130/150 [01:00<00:09,  2.13it/s]\u001b[A\n",
            " 87% 131/150 [01:00<00:08,  2.11it/s]\u001b[A\n",
            " 88% 132/150 [01:01<00:08,  2.11it/s]\u001b[A\n",
            " 89% 133/150 [01:01<00:07,  2.14it/s]\u001b[A\n",
            " 89% 134/150 [01:02<00:07,  2.12it/s]\u001b[A\n",
            " 90% 135/150 [01:02<00:07,  2.12it/s]\u001b[A\n",
            " 91% 136/150 [01:03<00:06,  2.18it/s]\u001b[A\n",
            " 91% 137/150 [01:03<00:06,  2.15it/s]\u001b[A\n",
            " 92% 138/150 [01:04<00:05,  2.13it/s]\u001b[A\n",
            " 93% 139/150 [01:04<00:05,  2.12it/s]\u001b[A\n",
            " 93% 140/150 [01:05<00:04,  2.14it/s]\u001b[A\n",
            " 94% 141/150 [01:05<00:04,  2.12it/s]\u001b[A\n",
            " 95% 142/150 [01:06<00:03,  2.12it/s]\u001b[A\n",
            " 95% 143/150 [01:06<00:03,  2.11it/s]\u001b[A\n",
            " 96% 144/150 [01:07<00:02,  2.11it/s]\u001b[A\n",
            " 97% 145/150 [01:07<00:02,  2.11it/s]\u001b[A\n",
            " 97% 146/150 [01:07<00:01,  2.11it/s]\u001b[A\n",
            " 98% 147/150 [01:08<00:01,  2.11it/s]\u001b[A\n",
            " 99% 148/150 [01:08<00:00,  2.11it/s]\u001b[A\n",
            " 99% 149/150 [01:09<00:00,  2.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0736838579177856, 'eval_runtime': 70.3852, 'eval_samples_per_second': 17.049, 'eval_steps_per_second': 2.131, 'epoch': 0.5}\n",
            " 50% 1200/2400 [10:34<06:52,  2.91it/s]\n",
            "100% 150/150 [01:09<00:00,  2.11it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1050] due to args.save_total_limit\n",
            "{'loss': 0.9921, 'learning_rate': 0.0001496868475991649, 'epoch': 0.5}\n",
            "{'loss': 1.1189, 'learning_rate': 0.00014906054279749476, 'epoch': 0.5}\n",
            "{'loss': 0.7947, 'learning_rate': 0.00014843423799582462, 'epoch': 0.51}\n",
            "{'loss': 0.9559, 'learning_rate': 0.00014780793319415448, 'epoch': 0.51}\n",
            "{'loss': 0.9745, 'learning_rate': 0.00014718162839248434, 'epoch': 0.51}\n",
            "{'loss': 0.9123, 'learning_rate': 0.00014655532359081417, 'epoch': 0.51}\n",
            "{'loss': 1.0902, 'learning_rate': 0.00014592901878914403, 'epoch': 0.51}\n",
            "{'loss': 1.0319, 'learning_rate': 0.0001453027139874739, 'epoch': 0.52}\n",
            "{'loss': 0.8793, 'learning_rate': 0.00014467640918580373, 'epoch': 0.52}\n",
            "{'loss': 0.9623, 'learning_rate': 0.00014405010438413358, 'epoch': 0.52}\n",
            " 52% 1250/2400 [10:51<06:20,  3.03it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 0.9549, 'learning_rate': 0.00014342379958246344, 'epoch': 0.52}\n",
            "{'loss': 1.0335, 'learning_rate': 0.0001427974947807933, 'epoch': 0.53}\n",
            "{'loss': 0.9758, 'learning_rate': 0.00014217118997912316, 'epoch': 0.53}\n",
            "{'loss': 0.9112, 'learning_rate': 0.00014154488517745302, 'epoch': 0.53}\n",
            "{'loss': 0.9359, 'learning_rate': 0.00014091858037578288, 'epoch': 0.53}\n",
            "{'loss': 1.0779, 'learning_rate': 0.00014029227557411271, 'epoch': 0.53}\n",
            "{'loss': 0.9113, 'learning_rate': 0.00013966597077244257, 'epoch': 0.54}\n",
            "{'loss': 0.9509, 'learning_rate': 0.00013903966597077243, 'epoch': 0.54}\n",
            "{'loss': 0.8195, 'learning_rate': 0.00013841336116910227, 'epoch': 0.54}\n",
            "{'loss': 0.8289, 'learning_rate': 0.00013778705636743213, 'epoch': 0.54}\n",
            " 54% 1300/2400 [11:08<05:50,  3.14it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1150] due to args.save_total_limit\n",
            "{'loss': 1.0739, 'learning_rate': 0.00013716075156576199, 'epoch': 0.54}\n",
            "{'loss': 1.067, 'learning_rate': 0.00013653444676409184, 'epoch': 0.55}\n",
            "{'loss': 1.1414, 'learning_rate': 0.0001359081419624217, 'epoch': 0.55}\n",
            "{'loss': 1.0514, 'learning_rate': 0.00013528183716075156, 'epoch': 0.55}\n",
            "{'loss': 1.1703, 'learning_rate': 0.00013465553235908142, 'epoch': 0.55}\n",
            "{'loss': 1.0904, 'learning_rate': 0.00013402922755741126, 'epoch': 0.55}\n",
            "{'loss': 1.0511, 'learning_rate': 0.00013340292275574112, 'epoch': 0.56}\n",
            "{'loss': 0.9438, 'learning_rate': 0.00013277661795407097, 'epoch': 0.56}\n",
            "{'loss': 0.8502, 'learning_rate': 0.0001321503131524008, 'epoch': 0.56}\n",
            "{'loss': 1.1254, 'learning_rate': 0.00013152400835073067, 'epoch': 0.56}\n",
            " 56% 1350/2400 [11:24<05:42,  3.07it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.9635, 'learning_rate': 0.00013089770354906053, 'epoch': 0.56}\n",
            "{'loss': 1.0018, 'learning_rate': 0.00013027139874739039, 'epoch': 0.57}\n",
            "{'loss': 0.908, 'learning_rate': 0.00012964509394572025, 'epoch': 0.57}\n",
            "{'loss': 0.871, 'learning_rate': 0.0001290187891440501, 'epoch': 0.57}\n",
            "{'loss': 1.2935, 'learning_rate': 0.00012839248434237996, 'epoch': 0.57}\n",
            "{'loss': 1.0196, 'learning_rate': 0.0001277661795407098, 'epoch': 0.57}\n",
            "{'loss': 1.0884, 'learning_rate': 0.00012713987473903966, 'epoch': 0.58}\n",
            "{'loss': 1.0453, 'learning_rate': 0.00012651356993736952, 'epoch': 0.58}\n",
            "{'loss': 0.9129, 'learning_rate': 0.00012588726513569935, 'epoch': 0.58}\n",
            "{'loss': 0.7959, 'learning_rate': 0.0001252609603340292, 'epoch': 0.58}\n",
            " 58% 1400/2400 [11:41<05:23,  3.09it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1250] due to args.save_total_limit\n",
            "{'loss': 0.9098, 'learning_rate': 0.00012463465553235907, 'epoch': 0.59}\n",
            "{'loss': 0.8049, 'learning_rate': 0.00012400835073068893, 'epoch': 0.59}\n",
            "{'loss': 0.8509, 'learning_rate': 0.00012338204592901879, 'epoch': 0.59}\n",
            "{'loss': 0.8478, 'learning_rate': 0.00012275574112734865, 'epoch': 0.59}\n",
            "{'loss': 0.9624, 'learning_rate': 0.0001221294363256785, 'epoch': 0.59}\n",
            "{'loss': 0.9433, 'learning_rate': 0.00012150313152400834, 'epoch': 0.6}\n",
            "{'loss': 1.0186, 'learning_rate': 0.0001208768267223382, 'epoch': 0.6}\n",
            "{'loss': 0.9136, 'learning_rate': 0.00012025052192066806, 'epoch': 0.6}\n",
            "{'loss': 1.1454, 'learning_rate': 0.00011962421711899789, 'epoch': 0.6}\n",
            "{'loss': 0.8049, 'learning_rate': 0.00011899791231732775, 'epoch': 0.6}\n",
            " 60% 1450/2400 [11:58<05:26,  2.91it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1450\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 0.9898, 'learning_rate': 0.00011837160751565761, 'epoch': 0.61}\n",
            "{'loss': 0.9474, 'learning_rate': 0.00011774530271398747, 'epoch': 0.61}\n",
            "{'loss': 1.03, 'learning_rate': 0.00011711899791231731, 'epoch': 0.61}\n",
            "{'loss': 0.9122, 'learning_rate': 0.00011649269311064717, 'epoch': 0.61}\n",
            "{'loss': 1.048, 'learning_rate': 0.00011586638830897703, 'epoch': 0.61}\n",
            "{'loss': 0.8363, 'learning_rate': 0.00011524008350730688, 'epoch': 0.62}\n",
            "{'loss': 0.9646, 'learning_rate': 0.00011461377870563674, 'epoch': 0.62}\n",
            "{'loss': 1.0441, 'learning_rate': 0.0001139874739039666, 'epoch': 0.62}\n",
            "{'loss': 1.0138, 'learning_rate': 0.00011336116910229643, 'epoch': 0.62}\n",
            "{'loss': 0.8119, 'learning_rate': 0.00011273486430062629, 'epoch': 0.62}\n",
            " 62% 1500/2400 [12:15<04:51,  3.09it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1350] due to args.save_total_limit\n",
            "{'loss': 1.1076, 'learning_rate': 0.00011210855949895615, 'epoch': 0.63}\n",
            "{'loss': 0.8636, 'learning_rate': 0.00011148225469728601, 'epoch': 0.63}\n",
            "{'loss': 1.0068, 'learning_rate': 0.00011085594989561585, 'epoch': 0.63}\n",
            "{'loss': 1.1314, 'learning_rate': 0.00011022964509394571, 'epoch': 0.63}\n",
            "{'loss': 0.848, 'learning_rate': 0.00010960334029227557, 'epoch': 0.64}\n",
            "{'loss': 1.0326, 'learning_rate': 0.00010897703549060542, 'epoch': 0.64}\n",
            "{'loss': 0.8711, 'learning_rate': 0.00010835073068893528, 'epoch': 0.64}\n",
            "{'loss': 0.8006, 'learning_rate': 0.00010772442588726512, 'epoch': 0.64}\n",
            "{'loss': 0.9279, 'learning_rate': 0.00010709812108559497, 'epoch': 0.64}\n",
            "{'loss': 0.9658, 'learning_rate': 0.00010647181628392483, 'epoch': 0.65}\n",
            " 65% 1550/2400 [12:32<04:33,  3.11it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1550\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 0.846, 'learning_rate': 0.00010584551148225469, 'epoch': 0.65}\n",
            "{'loss': 1.0121, 'learning_rate': 0.00010521920668058455, 'epoch': 0.65}\n",
            "{'loss': 1.1496, 'learning_rate': 0.0001045929018789144, 'epoch': 0.65}\n",
            "{'loss': 0.845, 'learning_rate': 0.00010396659707724425, 'epoch': 0.65}\n",
            "{'loss': 0.9276, 'learning_rate': 0.00010334029227557411, 'epoch': 0.66}\n",
            "{'loss': 0.989, 'learning_rate': 0.00010271398747390395, 'epoch': 0.66}\n",
            "{'loss': 0.7561, 'learning_rate': 0.0001020876826722338, 'epoch': 0.66}\n",
            "{'loss': 0.8279, 'learning_rate': 0.00010146137787056367, 'epoch': 0.66}\n",
            "{'loss': 0.9334, 'learning_rate': 0.00010083507306889351, 'epoch': 0.66}\n",
            "{'loss': 1.143, 'learning_rate': 0.00010020876826722337, 'epoch': 0.67}\n",
            " 67% 1600/2400 [12:48<04:29,  2.97it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1450] due to args.save_total_limit\n",
            "{'loss': 0.9526, 'learning_rate': 9.958246346555323e-05, 'epoch': 0.67}\n",
            "{'loss': 0.9826, 'learning_rate': 9.895615866388308e-05, 'epoch': 0.67}\n",
            "{'loss': 0.9916, 'learning_rate': 9.832985386221294e-05, 'epoch': 0.67}\n",
            "{'loss': 0.8942, 'learning_rate': 9.77035490605428e-05, 'epoch': 0.68}\n",
            "{'loss': 0.7953, 'learning_rate': 9.707724425887266e-05, 'epoch': 0.68}\n",
            "{'loss': 1.0369, 'learning_rate': 9.645093945720249e-05, 'epoch': 0.68}\n",
            "{'loss': 0.9201, 'learning_rate': 9.582463465553235e-05, 'epoch': 0.68}\n",
            "{'loss': 1.1297, 'learning_rate': 9.519832985386221e-05, 'epoch': 0.68}\n",
            "{'loss': 1.025, 'learning_rate': 9.457202505219205e-05, 'epoch': 0.69}\n",
            "{'loss': 1.0183, 'learning_rate': 9.394572025052191e-05, 'epoch': 0.69}\n",
            " 69% 1650/2400 [13:05<04:16,  2.92it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1650\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 0.8952, 'learning_rate': 9.331941544885177e-05, 'epoch': 0.69}\n",
            "{'loss': 1.112, 'learning_rate': 9.269311064718162e-05, 'epoch': 0.69}\n",
            "{'loss': 1.1158, 'learning_rate': 9.206680584551148e-05, 'epoch': 0.69}\n",
            "{'loss': 0.8974, 'learning_rate': 9.144050104384132e-05, 'epoch': 0.7}\n",
            "{'loss': 1.0259, 'learning_rate': 9.081419624217118e-05, 'epoch': 0.7}\n",
            "{'loss': 0.9537, 'learning_rate': 9.018789144050103e-05, 'epoch': 0.7}\n",
            "{'loss': 0.8112, 'learning_rate': 8.956158663883089e-05, 'epoch': 0.7}\n",
            "{'loss': 0.9341, 'learning_rate': 8.893528183716075e-05, 'epoch': 0.7}\n",
            "{'loss': 1.0244, 'learning_rate': 8.83089770354906e-05, 'epoch': 0.71}\n",
            "{'loss': 1.0417, 'learning_rate': 8.768267223382045e-05, 'epoch': 0.71}\n",
            " 71% 1700/2400 [13:22<04:03,  2.88it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1550] due to args.save_total_limit\n",
            "{'loss': 0.825, 'learning_rate': 8.705636743215031e-05, 'epoch': 0.71}\n",
            "{'loss': 0.9527, 'learning_rate': 8.643006263048015e-05, 'epoch': 0.71}\n",
            "{'loss': 0.9394, 'learning_rate': 8.580375782881e-05, 'epoch': 0.71}\n",
            "{'loss': 1.041, 'learning_rate': 8.517745302713986e-05, 'epoch': 0.72}\n",
            "{'loss': 0.9311, 'learning_rate': 8.455114822546972e-05, 'epoch': 0.72}\n",
            "{'loss': 0.9465, 'learning_rate': 8.392484342379957e-05, 'epoch': 0.72}\n",
            "{'loss': 0.9486, 'learning_rate': 8.329853862212943e-05, 'epoch': 0.72}\n",
            "{'loss': 0.9745, 'learning_rate': 8.267223382045929e-05, 'epoch': 0.72}\n",
            "{'loss': 0.8823, 'learning_rate': 8.204592901878913e-05, 'epoch': 0.73}\n",
            "{'loss': 0.9307, 'learning_rate': 8.1419624217119e-05, 'epoch': 0.73}\n",
            " 73% 1750/2400 [13:39<03:42,  2.93it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1600] due to args.save_total_limit\n",
            "{'loss': 1.0211, 'learning_rate': 8.079331941544885e-05, 'epoch': 0.73}\n",
            "{'loss': 1.1494, 'learning_rate': 8.016701461377869e-05, 'epoch': 0.73}\n",
            "{'loss': 0.9123, 'learning_rate': 7.954070981210855e-05, 'epoch': 0.74}\n",
            "{'loss': 0.8751, 'learning_rate': 7.89144050104384e-05, 'epoch': 0.74}\n",
            "{'loss': 0.913, 'learning_rate': 7.828810020876826e-05, 'epoch': 0.74}\n",
            "{'loss': 1.0383, 'learning_rate': 7.766179540709811e-05, 'epoch': 0.74}\n",
            "{'loss': 1.03, 'learning_rate': 7.703549060542797e-05, 'epoch': 0.74}\n",
            "{'loss': 0.9522, 'learning_rate': 7.640918580375783e-05, 'epoch': 0.75}\n",
            "{'loss': 0.9352, 'learning_rate': 7.578288100208768e-05, 'epoch': 0.75}\n",
            "{'loss': 0.8015, 'learning_rate': 7.515657620041754e-05, 'epoch': 0.75}\n",
            " 75% 1800/2400 [13:56<03:25,  2.92it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: token_type_ids, instruction, language, source, input, output. If token_type_ids, instruction, language, source, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:35,  4.21it/s]\u001b[A\n",
            "  2% 3/150 [00:00<00:49,  2.97it/s]\u001b[A\n",
            "  3% 4/150 [00:01<00:56,  2.58it/s]\u001b[A\n",
            "  3% 5/150 [00:01<01:00,  2.39it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:02,  2.29it/s]\u001b[A\n",
            "  5% 7/150 [00:02<00:59,  2.41it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:01,  2.30it/s]\u001b[A\n",
            "  6% 9/150 [00:03<01:03,  2.24it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:03,  2.19it/s]\u001b[A\n",
            "  7% 11/150 [00:04<01:04,  2.17it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:04,  2.15it/s]\u001b[A\n",
            "  9% 13/150 [00:05<00:59,  2.29it/s]\u001b[A\n",
            "  9% 14/150 [00:05<01:01,  2.23it/s]\u001b[A\n",
            " 10% 15/150 [00:06<01:01,  2.19it/s]\u001b[A\n",
            " 11% 16/150 [00:06<01:01,  2.16it/s]\u001b[A\n",
            " 11% 17/150 [00:07<01:01,  2.17it/s]\u001b[A\n",
            " 12% 18/150 [00:07<01:01,  2.14it/s]\u001b[A\n",
            " 13% 19/150 [00:08<01:01,  2.13it/s]\u001b[A\n",
            " 13% 20/150 [00:08<01:01,  2.12it/s]\u001b[A\n",
            " 14% 21/150 [00:09<01:00,  2.12it/s]\u001b[A\n",
            " 15% 22/150 [00:09<01:00,  2.11it/s]\u001b[A\n",
            " 15% 23/150 [00:10<01:00,  2.11it/s]\u001b[A\n",
            " 16% 24/150 [00:10<00:59,  2.11it/s]\u001b[A\n",
            " 17% 25/150 [00:11<00:59,  2.11it/s]\u001b[A\n",
            " 17% 26/150 [00:11<00:58,  2.11it/s]\u001b[A\n",
            " 18% 27/150 [00:12<00:58,  2.11it/s]\u001b[A\n",
            " 19% 28/150 [00:12<00:57,  2.11it/s]\u001b[A\n",
            " 19% 29/150 [00:13<00:57,  2.11it/s]\u001b[A\n",
            " 20% 30/150 [00:13<00:56,  2.11it/s]\u001b[A\n",
            " 21% 31/150 [00:14<00:56,  2.11it/s]\u001b[A\n",
            " 21% 32/150 [00:14<00:56,  2.11it/s]\u001b[A\n",
            " 22% 33/150 [00:14<00:55,  2.11it/s]\u001b[A\n",
            " 23% 34/150 [00:15<00:55,  2.11it/s]\u001b[A\n",
            " 23% 35/150 [00:15<00:54,  2.11it/s]\u001b[A\n",
            " 24% 36/150 [00:16<00:54,  2.11it/s]\u001b[A\n",
            " 25% 37/150 [00:16<00:53,  2.11it/s]\u001b[A\n",
            " 25% 38/150 [00:17<00:53,  2.11it/s]\u001b[A\n",
            " 26% 39/150 [00:17<00:52,  2.11it/s]\u001b[A\n",
            " 27% 40/150 [00:18<00:52,  2.11it/s]\u001b[A\n",
            " 27% 41/150 [00:18<00:51,  2.11it/s]\u001b[A\n",
            " 28% 42/150 [00:19<00:51,  2.11it/s]\u001b[A\n",
            " 29% 43/150 [00:19<00:50,  2.11it/s]\u001b[A\n",
            " 29% 44/150 [00:20<00:50,  2.11it/s]\u001b[A\n",
            " 30% 45/150 [00:20<00:49,  2.11it/s]\u001b[A\n",
            " 31% 46/150 [00:21<00:48,  2.17it/s]\u001b[A\n",
            " 31% 47/150 [00:21<00:48,  2.14it/s]\u001b[A\n",
            " 32% 48/150 [00:22<00:47,  2.13it/s]\u001b[A\n",
            " 33% 49/150 [00:22<00:47,  2.12it/s]\u001b[A\n",
            " 33% 50/150 [00:23<00:47,  2.12it/s]\u001b[A\n",
            " 34% 51/150 [00:23<00:46,  2.11it/s]\u001b[A\n",
            " 35% 52/150 [00:23<00:46,  2.10it/s]\u001b[A\n",
            " 35% 53/150 [00:24<00:46,  2.09it/s]\u001b[A\n",
            " 36% 54/150 [00:24<00:45,  2.10it/s]\u001b[A\n",
            " 37% 55/150 [00:25<00:45,  2.10it/s]\u001b[A\n",
            " 37% 56/150 [00:25<00:44,  2.10it/s]\u001b[A\n",
            " 38% 57/150 [00:26<00:44,  2.10it/s]\u001b[A\n",
            " 39% 58/150 [00:26<00:43,  2.13it/s]\u001b[A\n",
            " 39% 59/150 [00:27<00:43,  2.11it/s]\u001b[A\n",
            " 40% 60/150 [00:27<00:42,  2.11it/s]\u001b[A\n",
            " 41% 61/150 [00:28<00:42,  2.11it/s]\u001b[A\n",
            " 41% 62/150 [00:28<00:41,  2.11it/s]\u001b[A\n",
            " 42% 63/150 [00:29<00:41,  2.11it/s]\u001b[A\n",
            " 43% 64/150 [00:29<00:40,  2.11it/s]\u001b[A\n",
            " 43% 65/150 [00:30<00:40,  2.11it/s]\u001b[A\n",
            " 44% 66/150 [00:30<00:39,  2.11it/s]\u001b[A\n",
            " 45% 67/150 [00:31<00:39,  2.11it/s]\u001b[A\n",
            " 45% 68/150 [00:31<00:38,  2.11it/s]\u001b[A\n",
            " 46% 69/150 [00:32<00:38,  2.11it/s]\u001b[A\n",
            " 47% 70/150 [00:32<00:37,  2.13it/s]\u001b[A\n",
            " 47% 71/150 [00:32<00:36,  2.16it/s]\u001b[A\n",
            " 48% 72/150 [00:33<00:36,  2.13it/s]\u001b[A\n",
            " 49% 73/150 [00:33<00:36,  2.12it/s]\u001b[A\n",
            " 49% 74/150 [00:34<00:35,  2.12it/s]\u001b[A\n",
            " 50% 75/150 [00:34<00:32,  2.28it/s]\u001b[A\n",
            " 51% 76/150 [00:35<00:33,  2.22it/s]\u001b[A\n",
            " 51% 77/150 [00:35<00:30,  2.36it/s]\u001b[A\n",
            " 52% 78/150 [00:36<00:31,  2.27it/s]\u001b[A\n",
            " 53% 79/150 [00:36<00:32,  2.22it/s]\u001b[A\n",
            " 53% 80/150 [00:37<00:32,  2.18it/s]\u001b[A\n",
            " 54% 81/150 [00:37<00:31,  2.16it/s]\u001b[A\n",
            " 55% 82/150 [00:37<00:31,  2.14it/s]\u001b[A\n",
            " 55% 83/150 [00:38<00:31,  2.13it/s]\u001b[A\n",
            " 56% 84/150 [00:38<00:31,  2.12it/s]\u001b[A\n",
            " 57% 85/150 [00:39<00:30,  2.12it/s]\u001b[A\n",
            " 57% 86/150 [00:39<00:30,  2.11it/s]\u001b[A\n",
            " 58% 87/150 [00:40<00:29,  2.16it/s]\u001b[A\n",
            " 59% 88/150 [00:40<00:29,  2.14it/s]\u001b[A\n",
            " 59% 89/150 [00:41<00:28,  2.13it/s]\u001b[A\n",
            " 60% 90/150 [00:41<00:28,  2.12it/s]\u001b[A\n",
            " 61% 91/150 [00:42<00:27,  2.12it/s]\u001b[A\n",
            " 61% 92/150 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 62% 93/150 [00:43<00:25,  2.27it/s]\u001b[A\n",
            " 63% 94/150 [00:43<00:25,  2.21it/s]\u001b[A\n",
            " 63% 95/150 [00:43<00:25,  2.18it/s]\u001b[A\n",
            " 64% 96/150 [00:44<00:25,  2.16it/s]\u001b[A\n",
            " 65% 97/150 [00:44<00:24,  2.14it/s]\u001b[A\n",
            " 65% 98/150 [00:45<00:24,  2.13it/s]\u001b[A\n",
            " 66% 99/150 [00:45<00:24,  2.12it/s]\u001b[A\n",
            " 67% 100/150 [00:46<00:23,  2.12it/s]\u001b[A\n",
            " 67% 101/150 [00:46<00:23,  2.12it/s]\u001b[A\n",
            " 68% 102/150 [00:47<00:22,  2.11it/s]\u001b[A\n",
            " 69% 103/150 [00:47<00:22,  2.11it/s]\u001b[A\n",
            " 69% 104/150 [00:48<00:21,  2.17it/s]\u001b[A\n",
            " 70% 105/150 [00:48<00:21,  2.14it/s]\u001b[A\n",
            " 71% 106/150 [00:49<00:20,  2.15it/s]\u001b[A\n",
            " 71% 107/150 [00:49<00:20,  2.13it/s]\u001b[A\n",
            " 72% 108/150 [00:50<00:19,  2.12it/s]\u001b[A\n",
            " 73% 109/150 [00:50<00:19,  2.12it/s]\u001b[A\n",
            " 73% 110/150 [00:50<00:17,  2.28it/s]\u001b[A\n",
            " 74% 111/150 [00:51<00:17,  2.22it/s]\u001b[A\n",
            " 75% 112/150 [00:51<00:17,  2.18it/s]\u001b[A\n",
            " 75% 113/150 [00:52<00:16,  2.19it/s]\u001b[A\n",
            " 76% 114/150 [00:52<00:16,  2.15it/s]\u001b[A\n",
            " 77% 115/150 [00:53<00:16,  2.17it/s]\u001b[A\n",
            " 77% 116/150 [00:53<00:15,  2.15it/s]\u001b[A\n",
            " 78% 117/150 [00:54<00:15,  2.12it/s]\u001b[A\n",
            " 79% 118/150 [00:54<00:15,  2.12it/s]\u001b[A\n",
            " 79% 119/150 [00:55<00:14,  2.11it/s]\u001b[A\n",
            " 80% 120/150 [00:55<00:14,  2.11it/s]\u001b[A\n",
            " 81% 121/150 [00:56<00:13,  2.11it/s]\u001b[A\n",
            " 81% 122/150 [00:56<00:13,  2.11it/s]\u001b[A\n",
            " 82% 123/150 [00:57<00:12,  2.11it/s]\u001b[A\n",
            " 83% 124/150 [00:57<00:12,  2.11it/s]\u001b[A\n",
            " 83% 125/150 [00:58<00:11,  2.11it/s]\u001b[A\n",
            " 84% 126/150 [00:58<00:11,  2.09it/s]\u001b[A\n",
            " 85% 127/150 [00:59<00:11,  2.09it/s]\u001b[A\n",
            " 85% 128/150 [00:59<00:10,  2.09it/s]\u001b[A\n",
            " 86% 129/150 [00:59<00:10,  2.08it/s]\u001b[A\n",
            " 87% 130/150 [01:00<00:09,  2.13it/s]\u001b[A\n",
            " 87% 131/150 [01:00<00:09,  2.11it/s]\u001b[A\n",
            " 88% 132/150 [01:01<00:08,  2.11it/s]\u001b[A\n",
            " 89% 133/150 [01:01<00:07,  2.14it/s]\u001b[A\n",
            " 89% 134/150 [01:02<00:07,  2.11it/s]\u001b[A\n",
            " 90% 135/150 [01:02<00:07,  2.11it/s]\u001b[A\n",
            " 91% 136/150 [01:03<00:06,  2.17it/s]\u001b[A\n",
            " 91% 137/150 [01:03<00:06,  2.14it/s]\u001b[A\n",
            " 92% 138/150 [01:04<00:05,  2.13it/s]\u001b[A\n",
            " 93% 139/150 [01:04<00:05,  2.12it/s]\u001b[A\n",
            " 93% 140/150 [01:05<00:04,  2.14it/s]\u001b[A\n",
            " 94% 141/150 [01:05<00:04,  2.12it/s]\u001b[A\n",
            " 95% 142/150 [01:06<00:03,  2.11it/s]\u001b[A\n",
            " 95% 143/150 [01:06<00:03,  2.11it/s]\u001b[A\n",
            " 96% 144/150 [01:07<00:02,  2.11it/s]\u001b[A\n",
            " 97% 145/150 [01:07<00:02,  2.11it/s]\u001b[A\n",
            " 97% 146/150 [01:07<00:01,  2.11it/s]\u001b[A\n",
            " 98% 147/150 [01:08<00:01,  2.11it/s]\u001b[A\n",
            " 99% 148/150 [01:08<00:00,  2.10it/s]\u001b[A\n",
            " 99% 149/150 [01:09<00:00,  2.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0631992816925049, 'eval_runtime': 70.3725, 'eval_samples_per_second': 17.052, 'eval_steps_per_second': 2.132, 'epoch': 0.75}\n",
            " 75% 1800/2400 [15:06<03:25,  2.92it/s]\n",
            "100% 150/150 [01:09<00:00,  2.10it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1650] due to args.save_total_limit\n",
            "{'loss': 1.0525, 'learning_rate': 7.453027139874738e-05, 'epoch': 0.75}\n",
            "{'loss': 0.9862, 'learning_rate': 7.390396659707724e-05, 'epoch': 0.75}\n",
            "{'loss': 0.8937, 'learning_rate': 7.327766179540709e-05, 'epoch': 0.76}\n",
            "{'loss': 1.176, 'learning_rate': 7.265135699373695e-05, 'epoch': 0.76}\n",
            "{'loss': 1.0847, 'learning_rate': 7.202505219206679e-05, 'epoch': 0.76}\n",
            "{'loss': 0.8695, 'learning_rate': 7.139874739039665e-05, 'epoch': 0.76}\n",
            "{'loss': 0.9547, 'learning_rate': 7.077244258872651e-05, 'epoch': 0.76}\n",
            "{'loss': 0.9558, 'learning_rate': 7.014613778705636e-05, 'epoch': 0.77}\n",
            "{'loss': 0.8969, 'learning_rate': 6.951983298538622e-05, 'epoch': 0.77}\n",
            "{'loss': 1.0159, 'learning_rate': 6.889352818371606e-05, 'epoch': 0.77}\n",
            " 77% 1850/2400 [15:23<03:04,  2.98it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1850\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1700] due to args.save_total_limit\n",
            "{'loss': 1.0078, 'learning_rate': 6.826722338204592e-05, 'epoch': 0.77}\n",
            "{'loss': 0.9361, 'learning_rate': 6.764091858037578e-05, 'epoch': 0.78}\n",
            "{'loss': 0.9013, 'learning_rate': 6.701461377870563e-05, 'epoch': 0.78}\n",
            "{'loss': 1.1885, 'learning_rate': 6.638830897703549e-05, 'epoch': 0.78}\n",
            "{'loss': 1.0305, 'learning_rate': 6.576200417536533e-05, 'epoch': 0.78}\n",
            "{'loss': 1.0274, 'learning_rate': 6.513569937369519e-05, 'epoch': 0.78}\n",
            "{'loss': 0.8254, 'learning_rate': 6.450939457202505e-05, 'epoch': 0.79}\n",
            "{'loss': 0.8255, 'learning_rate': 6.38830897703549e-05, 'epoch': 0.79}\n",
            "{'loss': 0.9761, 'learning_rate': 6.325678496868476e-05, 'epoch': 0.79}\n",
            "{'loss': 0.9499, 'learning_rate': 6.26304801670146e-05, 'epoch': 0.79}\n",
            " 79% 1900/2400 [15:40<02:46,  2.99it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1750] due to args.save_total_limit\n",
            "{'loss': 0.9366, 'learning_rate': 6.200417536534446e-05, 'epoch': 0.79}\n",
            "{'loss': 0.8201, 'learning_rate': 6.137787056367432e-05, 'epoch': 0.8}\n",
            "{'loss': 0.8691, 'learning_rate': 6.075156576200417e-05, 'epoch': 0.8}\n",
            "{'loss': 0.8625, 'learning_rate': 6.012526096033403e-05, 'epoch': 0.8}\n",
            "{'loss': 1.0653, 'learning_rate': 5.9498956158663874e-05, 'epoch': 0.8}\n",
            "{'loss': 1.0183, 'learning_rate': 5.8872651356993734e-05, 'epoch': 0.8}\n",
            "{'loss': 1.2207, 'learning_rate': 5.8246346555323586e-05, 'epoch': 0.81}\n",
            "{'loss': 0.9886, 'learning_rate': 5.762004175365344e-05, 'epoch': 0.81}\n",
            "{'loss': 0.8762, 'learning_rate': 5.69937369519833e-05, 'epoch': 0.81}\n",
            "{'loss': 0.8021, 'learning_rate': 5.6367432150313145e-05, 'epoch': 0.81}\n",
            " 81% 1950/2400 [15:56<02:27,  3.04it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-1950\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1800] due to args.save_total_limit\n",
            "{'loss': 1.0612, 'learning_rate': 5.5741127348643004e-05, 'epoch': 0.81}\n",
            "{'loss': 0.6885, 'learning_rate': 5.511482254697286e-05, 'epoch': 0.82}\n",
            "{'loss': 1.2053, 'learning_rate': 5.448851774530271e-05, 'epoch': 0.82}\n",
            "{'loss': 1.0851, 'learning_rate': 5.386221294363256e-05, 'epoch': 0.82}\n",
            "{'loss': 1.0587, 'learning_rate': 5.3235908141962415e-05, 'epoch': 0.82}\n",
            "{'loss': 0.7788, 'learning_rate': 5.2609603340292275e-05, 'epoch': 0.82}\n",
            "{'loss': 1.201, 'learning_rate': 5.198329853862213e-05, 'epoch': 0.83}\n",
            "{'loss': 0.9365, 'learning_rate': 5.1356993736951973e-05, 'epoch': 0.83}\n",
            "{'loss': 0.8505, 'learning_rate': 5.073068893528183e-05, 'epoch': 0.83}\n",
            "{'loss': 0.8967, 'learning_rate': 5.0104384133611686e-05, 'epoch': 0.83}\n",
            " 83% 2000/2400 [16:13<02:12,  3.03it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1850] due to args.save_total_limit\n",
            "{'loss': 1.0385, 'learning_rate': 4.947807933194154e-05, 'epoch': 0.84}\n",
            "{'loss': 0.8167, 'learning_rate': 4.88517745302714e-05, 'epoch': 0.84}\n",
            "{'loss': 0.994, 'learning_rate': 4.8225469728601244e-05, 'epoch': 0.84}\n",
            "{'loss': 0.9223, 'learning_rate': 4.7599164926931103e-05, 'epoch': 0.84}\n",
            "{'loss': 0.9638, 'learning_rate': 4.6972860125260956e-05, 'epoch': 0.84}\n",
            "{'loss': 1.0119, 'learning_rate': 4.634655532359081e-05, 'epoch': 0.85}\n",
            "{'loss': 1.0952, 'learning_rate': 4.572025052192066e-05, 'epoch': 0.85}\n",
            "{'loss': 0.9882, 'learning_rate': 4.5093945720250514e-05, 'epoch': 0.85}\n",
            "{'loss': 1.0251, 'learning_rate': 4.4467640918580374e-05, 'epoch': 0.85}\n",
            "{'loss': 1.0793, 'learning_rate': 4.384133611691023e-05, 'epoch': 0.85}\n",
            " 85% 2050/2400 [16:30<02:00,  2.90it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2050\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1900] due to args.save_total_limit\n",
            "{'loss': 0.9608, 'learning_rate': 4.321503131524007e-05, 'epoch': 0.86}\n",
            "{'loss': 1.0482, 'learning_rate': 4.258872651356993e-05, 'epoch': 0.86}\n",
            "{'loss': 1.1285, 'learning_rate': 4.1962421711899785e-05, 'epoch': 0.86}\n",
            "{'loss': 1.031, 'learning_rate': 4.1336116910229644e-05, 'epoch': 0.86}\n",
            "{'loss': 0.8837, 'learning_rate': 4.07098121085595e-05, 'epoch': 0.86}\n",
            "{'loss': 0.9188, 'learning_rate': 4.008350730688934e-05, 'epoch': 0.87}\n",
            "{'loss': 0.8089, 'learning_rate': 3.94572025052192e-05, 'epoch': 0.87}\n",
            "{'loss': 1.0376, 'learning_rate': 3.8830897703549055e-05, 'epoch': 0.87}\n",
            "{'loss': 0.9537, 'learning_rate': 3.8204592901878915e-05, 'epoch': 0.87}\n",
            "{'loss': 1.1345, 'learning_rate': 3.757828810020877e-05, 'epoch': 0.88}\n",
            " 88% 2100/2400 [16:47<01:39,  3.00it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-1950] due to args.save_total_limit\n",
            "{'loss': 0.9372, 'learning_rate': 3.695198329853862e-05, 'epoch': 0.88}\n",
            "{'loss': 1.0774, 'learning_rate': 3.632567849686847e-05, 'epoch': 0.88}\n",
            "{'loss': 0.8954, 'learning_rate': 3.5699373695198326e-05, 'epoch': 0.88}\n",
            "{'loss': 0.9358, 'learning_rate': 3.507306889352818e-05, 'epoch': 0.88}\n",
            "{'loss': 0.9744, 'learning_rate': 3.444676409185803e-05, 'epoch': 0.89}\n",
            "{'loss': 1.1243, 'learning_rate': 3.382045929018789e-05, 'epoch': 0.89}\n",
            "{'loss': 0.8764, 'learning_rate': 3.3194154488517744e-05, 'epoch': 0.89}\n",
            "{'loss': 0.7564, 'learning_rate': 3.2567849686847596e-05, 'epoch': 0.89}\n",
            "{'loss': 0.849, 'learning_rate': 3.194154488517745e-05, 'epoch': 0.89}\n",
            "{'loss': 0.9351, 'learning_rate': 3.13152400835073e-05, 'epoch': 0.9}\n",
            " 90% 2150/2400 [17:04<01:19,  3.14it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 0.8817, 'learning_rate': 3.068893528183716e-05, 'epoch': 0.9}\n",
            "{'loss': 0.9661, 'learning_rate': 3.0062630480167014e-05, 'epoch': 0.9}\n",
            "{'loss': 0.8842, 'learning_rate': 2.9436325678496867e-05, 'epoch': 0.9}\n",
            "{'loss': 0.8144, 'learning_rate': 2.881002087682672e-05, 'epoch': 0.9}\n",
            "{'loss': 0.9866, 'learning_rate': 2.8183716075156572e-05, 'epoch': 0.91}\n",
            "{'loss': 0.9656, 'learning_rate': 2.755741127348643e-05, 'epoch': 0.91}\n",
            "{'loss': 0.9499, 'learning_rate': 2.693110647181628e-05, 'epoch': 0.91}\n",
            "{'loss': 0.9873, 'learning_rate': 2.6304801670146137e-05, 'epoch': 0.91}\n",
            "{'loss': 0.9097, 'learning_rate': 2.5678496868475987e-05, 'epoch': 0.91}\n",
            "{'loss': 0.9747, 'learning_rate': 2.5052192066805843e-05, 'epoch': 0.92}\n",
            " 92% 2200/2400 [17:21<01:06,  3.00it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-2050] due to args.save_total_limit\n",
            "{'loss': 0.9217, 'learning_rate': 2.44258872651357e-05, 'epoch': 0.92}\n",
            "{'loss': 0.9194, 'learning_rate': 2.3799582463465552e-05, 'epoch': 0.92}\n",
            "{'loss': 0.806, 'learning_rate': 2.3173277661795404e-05, 'epoch': 0.92}\n",
            "{'loss': 1.1397, 'learning_rate': 2.2546972860125257e-05, 'epoch': 0.93}\n",
            "{'loss': 0.7908, 'learning_rate': 2.1920668058455113e-05, 'epoch': 0.93}\n",
            "{'loss': 1.0847, 'learning_rate': 2.1294363256784966e-05, 'epoch': 0.93}\n",
            "{'loss': 1.1425, 'learning_rate': 2.0668058455114822e-05, 'epoch': 0.93}\n",
            "{'loss': 1.1829, 'learning_rate': 2.004175365344467e-05, 'epoch': 0.93}\n",
            "{'loss': 1.0052, 'learning_rate': 1.9415448851774528e-05, 'epoch': 0.94}\n",
            "{'loss': 1.0862, 'learning_rate': 1.8789144050104384e-05, 'epoch': 0.94}\n",
            " 94% 2250/2400 [17:38<00:49,  3.00it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-2100] due to args.save_total_limit\n",
            "{'loss': 0.8686, 'learning_rate': 1.8162839248434237e-05, 'epoch': 0.94}\n",
            "{'loss': 0.7884, 'learning_rate': 1.753653444676409e-05, 'epoch': 0.94}\n",
            "{'loss': 0.8786, 'learning_rate': 1.6910229645093945e-05, 'epoch': 0.94}\n",
            "{'loss': 1.0637, 'learning_rate': 1.6283924843423798e-05, 'epoch': 0.95}\n",
            "{'loss': 1.0663, 'learning_rate': 1.565762004175365e-05, 'epoch': 0.95}\n",
            "{'loss': 0.8606, 'learning_rate': 1.5031315240083507e-05, 'epoch': 0.95}\n",
            "{'loss': 1.0524, 'learning_rate': 1.440501043841336e-05, 'epoch': 0.95}\n",
            "{'loss': 0.9434, 'learning_rate': 1.3778705636743214e-05, 'epoch': 0.95}\n",
            "{'loss': 0.9522, 'learning_rate': 1.3152400835073069e-05, 'epoch': 0.96}\n",
            "{'loss': 0.9416, 'learning_rate': 1.2526096033402921e-05, 'epoch': 0.96}\n",
            " 96% 2300/2400 [17:55<00:33,  2.95it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-2150] due to args.save_total_limit\n",
            "{'loss': 1.1585, 'learning_rate': 1.1899791231732776e-05, 'epoch': 0.96}\n",
            "{'loss': 0.8875, 'learning_rate': 1.1273486430062629e-05, 'epoch': 0.96}\n",
            "{'loss': 0.9511, 'learning_rate': 1.0647181628392483e-05, 'epoch': 0.96}\n",
            "{'loss': 0.9962, 'learning_rate': 1.0020876826722336e-05, 'epoch': 0.97}\n",
            "{'loss': 1.0372, 'learning_rate': 9.394572025052192e-06, 'epoch': 0.97}\n",
            "{'loss': 0.9215, 'learning_rate': 8.768267223382045e-06, 'epoch': 0.97}\n",
            "{'loss': 0.8669, 'learning_rate': 8.141962421711899e-06, 'epoch': 0.97}\n",
            "{'loss': 0.9741, 'learning_rate': 7.5156576200417535e-06, 'epoch': 0.97}\n",
            "{'loss': 1.1281, 'learning_rate': 6.889352818371607e-06, 'epoch': 0.98}\n",
            "{'loss': 0.87, 'learning_rate': 6.263048016701461e-06, 'epoch': 0.98}\n",
            " 98% 2350/2400 [18:12<00:16,  3.01it/s]Saving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-2200] due to args.save_total_limit\n",
            "{'loss': 0.9293, 'learning_rate': 5.636743215031314e-06, 'epoch': 0.98}\n",
            "{'loss': 1.1296, 'learning_rate': 5.010438413361168e-06, 'epoch': 0.98}\n",
            "{'loss': 0.9588, 'learning_rate': 4.384133611691022e-06, 'epoch': 0.99}\n",
            "{'loss': 1.0956, 'learning_rate': 3.7578288100208768e-06, 'epoch': 0.99}\n",
            "{'loss': 0.8957, 'learning_rate': 3.1315240083507304e-06, 'epoch': 0.99}\n",
            "{'loss': 0.97, 'learning_rate': 2.505219206680584e-06, 'epoch': 0.99}\n",
            "{'loss': 1.0637, 'learning_rate': 1.8789144050104384e-06, 'epoch': 0.99}\n",
            "{'loss': 0.9998, 'learning_rate': 1.252609603340292e-06, 'epoch': 1.0}\n",
            "{'loss': 0.7925, 'learning_rate': 6.26304801670146e-07, 'epoch': 1.0}\n",
            "{'loss': 0.9954, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 2400/2400 [18:28<00:00,  3.05it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: token_type_ids, instruction, language, source, input, output. If token_type_ids, instruction, language, source, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:35,  4.22it/s]\u001b[A\n",
            "  2% 3/150 [00:00<00:49,  2.98it/s]\u001b[A\n",
            "  3% 4/150 [00:01<00:56,  2.58it/s]\u001b[A\n",
            "  3% 5/150 [00:01<01:00,  2.39it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:02,  2.29it/s]\u001b[A\n",
            "  5% 7/150 [00:02<00:59,  2.40it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:01,  2.29it/s]\u001b[A\n",
            "  6% 9/150 [00:03<01:03,  2.23it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:03,  2.19it/s]\u001b[A\n",
            "  7% 11/150 [00:04<01:04,  2.16it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:04,  2.15it/s]\u001b[A\n",
            "  9% 13/150 [00:05<00:59,  2.29it/s]\u001b[A\n",
            "  9% 14/150 [00:05<01:01,  2.23it/s]\u001b[A\n",
            " 10% 15/150 [00:06<01:01,  2.19it/s]\u001b[A\n",
            " 11% 16/150 [00:06<01:01,  2.16it/s]\u001b[A\n",
            " 11% 17/150 [00:07<01:01,  2.17it/s]\u001b[A\n",
            " 12% 18/150 [00:07<01:01,  2.14it/s]\u001b[A\n",
            " 13% 19/150 [00:08<01:01,  2.13it/s]\u001b[A\n",
            " 13% 20/150 [00:08<01:01,  2.12it/s]\u001b[A\n",
            " 14% 21/150 [00:09<01:00,  2.12it/s]\u001b[A\n",
            " 15% 22/150 [00:09<01:00,  2.11it/s]\u001b[A\n",
            " 15% 23/150 [00:10<01:00,  2.11it/s]\u001b[A\n",
            " 16% 24/150 [00:10<00:59,  2.11it/s]\u001b[A\n",
            " 17% 25/150 [00:11<00:59,  2.11it/s]\u001b[A\n",
            " 17% 26/150 [00:11<00:58,  2.11it/s]\u001b[A\n",
            " 18% 27/150 [00:12<00:58,  2.11it/s]\u001b[A\n",
            " 19% 28/150 [00:12<00:57,  2.11it/s]\u001b[A\n",
            " 19% 29/150 [00:13<00:57,  2.11it/s]\u001b[A\n",
            " 20% 30/150 [00:13<00:56,  2.11it/s]\u001b[A\n",
            " 21% 31/150 [00:14<00:56,  2.11it/s]\u001b[A\n",
            " 21% 32/150 [00:14<00:56,  2.11it/s]\u001b[A\n",
            " 22% 33/150 [00:14<00:55,  2.11it/s]\u001b[A\n",
            " 23% 34/150 [00:15<00:55,  2.11it/s]\u001b[A\n",
            " 23% 35/150 [00:15<00:54,  2.11it/s]\u001b[A\n",
            " 24% 36/150 [00:16<00:54,  2.11it/s]\u001b[A\n",
            " 25% 37/150 [00:16<00:53,  2.11it/s]\u001b[A\n",
            " 25% 38/150 [00:17<00:53,  2.11it/s]\u001b[A\n",
            " 26% 39/150 [00:17<00:52,  2.11it/s]\u001b[A\n",
            " 27% 40/150 [00:18<00:52,  2.11it/s]\u001b[A\n",
            " 27% 41/150 [00:18<00:51,  2.11it/s]\u001b[A\n",
            " 28% 42/150 [00:19<00:51,  2.11it/s]\u001b[A\n",
            " 29% 43/150 [00:19<00:50,  2.11it/s]\u001b[A\n",
            " 29% 44/150 [00:20<00:50,  2.11it/s]\u001b[A\n",
            " 30% 45/150 [00:20<00:49,  2.11it/s]\u001b[A\n",
            " 31% 46/150 [00:21<00:47,  2.17it/s]\u001b[A\n",
            " 31% 47/150 [00:21<00:48,  2.14it/s]\u001b[A\n",
            " 32% 48/150 [00:22<00:47,  2.13it/s]\u001b[A\n",
            " 33% 49/150 [00:22<00:47,  2.12it/s]\u001b[A\n",
            " 33% 50/150 [00:23<00:47,  2.12it/s]\u001b[A\n",
            " 34% 51/150 [00:23<00:46,  2.11it/s]\u001b[A\n",
            " 35% 52/150 [00:23<00:46,  2.10it/s]\u001b[A\n",
            " 35% 53/150 [00:24<00:46,  2.09it/s]\u001b[A\n",
            " 36% 54/150 [00:24<00:45,  2.09it/s]\u001b[A\n",
            " 37% 55/150 [00:25<00:45,  2.10it/s]\u001b[A\n",
            " 37% 56/150 [00:25<00:44,  2.10it/s]\u001b[A\n",
            " 38% 57/150 [00:26<00:44,  2.10it/s]\u001b[A\n",
            " 39% 58/150 [00:26<00:43,  2.12it/s]\u001b[A\n",
            " 39% 59/150 [00:27<00:43,  2.11it/s]\u001b[A\n",
            " 40% 60/150 [00:27<00:42,  2.11it/s]\u001b[A\n",
            " 41% 61/150 [00:28<00:42,  2.11it/s]\u001b[A\n",
            " 41% 62/150 [00:28<00:41,  2.11it/s]\u001b[A\n",
            " 42% 63/150 [00:29<00:41,  2.11it/s]\u001b[A\n",
            " 43% 64/150 [00:29<00:40,  2.11it/s]\u001b[A\n",
            " 43% 65/150 [00:30<00:40,  2.11it/s]\u001b[A\n",
            " 44% 66/150 [00:30<00:39,  2.10it/s]\u001b[A\n",
            " 45% 67/150 [00:31<00:39,  2.10it/s]\u001b[A\n",
            " 45% 68/150 [00:31<00:38,  2.11it/s]\u001b[A\n",
            " 46% 69/150 [00:32<00:38,  2.11it/s]\u001b[A\n",
            " 47% 70/150 [00:32<00:37,  2.13it/s]\u001b[A\n",
            " 47% 71/150 [00:32<00:36,  2.15it/s]\u001b[A\n",
            " 48% 72/150 [00:33<00:36,  2.13it/s]\u001b[A\n",
            " 49% 73/150 [00:33<00:36,  2.12it/s]\u001b[A\n",
            " 49% 74/150 [00:34<00:35,  2.12it/s]\u001b[A\n",
            " 50% 75/150 [00:34<00:32,  2.28it/s]\u001b[A\n",
            " 51% 76/150 [00:35<00:33,  2.22it/s]\u001b[A\n",
            " 51% 77/150 [00:35<00:31,  2.35it/s]\u001b[A\n",
            " 52% 78/150 [00:36<00:31,  2.27it/s]\u001b[A\n",
            " 53% 79/150 [00:36<00:32,  2.22it/s]\u001b[A\n",
            " 53% 80/150 [00:37<00:32,  2.18it/s]\u001b[A\n",
            " 54% 81/150 [00:37<00:31,  2.16it/s]\u001b[A\n",
            " 55% 82/150 [00:37<00:31,  2.14it/s]\u001b[A\n",
            " 55% 83/150 [00:38<00:31,  2.13it/s]\u001b[A\n",
            " 56% 84/150 [00:38<00:31,  2.12it/s]\u001b[A\n",
            " 57% 85/150 [00:39<00:30,  2.12it/s]\u001b[A\n",
            " 57% 86/150 [00:39<00:30,  2.11it/s]\u001b[A\n",
            " 58% 87/150 [00:40<00:29,  2.17it/s]\u001b[A\n",
            " 59% 88/150 [00:40<00:28,  2.14it/s]\u001b[A\n",
            " 59% 89/150 [00:41<00:28,  2.13it/s]\u001b[A\n",
            " 60% 90/150 [00:41<00:28,  2.12it/s]\u001b[A\n",
            " 61% 91/150 [00:42<00:27,  2.12it/s]\u001b[A\n",
            " 61% 92/150 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 62% 93/150 [00:43<00:25,  2.27it/s]\u001b[A\n",
            " 63% 94/150 [00:43<00:25,  2.21it/s]\u001b[A\n",
            " 63% 95/150 [00:44<00:25,  2.18it/s]\u001b[A\n",
            " 64% 96/150 [00:44<00:25,  2.16it/s]\u001b[A\n",
            " 65% 97/150 [00:44<00:24,  2.14it/s]\u001b[A\n",
            " 65% 98/150 [00:45<00:24,  2.13it/s]\u001b[A\n",
            " 66% 99/150 [00:45<00:24,  2.12it/s]\u001b[A\n",
            " 67% 100/150 [00:46<00:23,  2.12it/s]\u001b[A\n",
            " 67% 101/150 [00:46<00:23,  2.11it/s]\u001b[A\n",
            " 68% 102/150 [00:47<00:22,  2.11it/s]\u001b[A\n",
            " 69% 103/150 [00:47<00:22,  2.11it/s]\u001b[A\n",
            " 69% 104/150 [00:48<00:21,  2.17it/s]\u001b[A\n",
            " 70% 105/150 [00:48<00:21,  2.14it/s]\u001b[A\n",
            " 71% 106/150 [00:49<00:20,  2.15it/s]\u001b[A\n",
            " 71% 107/150 [00:49<00:20,  2.13it/s]\u001b[A\n",
            " 72% 108/150 [00:50<00:19,  2.12it/s]\u001b[A\n",
            " 73% 109/150 [00:50<00:19,  2.12it/s]\u001b[A\n",
            " 73% 110/150 [00:50<00:17,  2.27it/s]\u001b[A\n",
            " 74% 111/150 [00:51<00:17,  2.21it/s]\u001b[A\n",
            " 75% 112/150 [00:51<00:17,  2.18it/s]\u001b[A\n",
            " 75% 113/150 [00:52<00:16,  2.19it/s]\u001b[A\n",
            " 76% 114/150 [00:52<00:16,  2.15it/s]\u001b[A\n",
            " 77% 115/150 [00:53<00:16,  2.17it/s]\u001b[A\n",
            " 77% 116/150 [00:53<00:15,  2.15it/s]\u001b[A\n",
            " 78% 117/150 [00:54<00:15,  2.12it/s]\u001b[A\n",
            " 79% 118/150 [00:54<00:15,  2.12it/s]\u001b[A\n",
            " 79% 119/150 [00:55<00:14,  2.11it/s]\u001b[A\n",
            " 80% 120/150 [00:55<00:14,  2.11it/s]\u001b[A\n",
            " 81% 121/150 [00:56<00:13,  2.11it/s]\u001b[A\n",
            " 81% 122/150 [00:56<00:13,  2.11it/s]\u001b[A\n",
            " 82% 123/150 [00:57<00:12,  2.11it/s]\u001b[A\n",
            " 83% 124/150 [00:57<00:12,  2.11it/s]\u001b[A\n",
            " 83% 125/150 [00:58<00:11,  2.11it/s]\u001b[A\n",
            " 84% 126/150 [00:58<00:11,  2.09it/s]\u001b[A\n",
            " 85% 127/150 [00:59<00:11,  2.08it/s]\u001b[A\n",
            " 85% 128/150 [00:59<00:10,  2.09it/s]\u001b[A\n",
            " 86% 129/150 [01:00<00:10,  2.08it/s]\u001b[A\n",
            " 87% 130/150 [01:00<00:09,  2.13it/s]\u001b[A\n",
            " 87% 131/150 [01:00<00:09,  2.11it/s]\u001b[A\n",
            " 88% 132/150 [01:01<00:08,  2.11it/s]\u001b[A\n",
            " 89% 133/150 [01:01<00:07,  2.14it/s]\u001b[A\n",
            " 89% 134/150 [01:02<00:07,  2.12it/s]\u001b[A\n",
            " 90% 135/150 [01:02<00:07,  2.11it/s]\u001b[A\n",
            " 91% 136/150 [01:03<00:06,  2.17it/s]\u001b[A\n",
            " 91% 137/150 [01:03<00:06,  2.14it/s]\u001b[A\n",
            " 92% 138/150 [01:04<00:05,  2.13it/s]\u001b[A\n",
            " 93% 139/150 [01:04<00:05,  2.12it/s]\u001b[A\n",
            " 93% 140/150 [01:05<00:04,  2.14it/s]\u001b[A\n",
            " 94% 141/150 [01:05<00:04,  2.12it/s]\u001b[A\n",
            " 95% 142/150 [01:06<00:03,  2.12it/s]\u001b[A\n",
            " 95% 143/150 [01:06<00:03,  2.11it/s]\u001b[A\n",
            " 96% 144/150 [01:07<00:02,  2.11it/s]\u001b[A\n",
            " 97% 145/150 [01:07<00:02,  2.11it/s]\u001b[A\n",
            " 97% 146/150 [01:08<00:01,  2.11it/s]\u001b[A\n",
            " 98% 147/150 [01:08<00:01,  2.11it/s]\u001b[A\n",
            " 99% 148/150 [01:08<00:00,  2.11it/s]\u001b[A\n",
            " 99% 149/150 [01:09<00:00,  2.10it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0580947399139404, 'eval_runtime': 70.3942, 'eval_samples_per_second': 17.047, 'eval_steps_per_second': 2.131, 'epoch': 1.0}\n",
            "100% 2400/2400 [19:39<00:00,  3.05it/s]\n",
            "100% 150/150 [01:09<00:00,  2.11it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-sample-4bit/checkpoint-2400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-sample-4bit/checkpoint-2250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1179.7754, 'train_samples_per_second': 4.069, 'train_steps_per_second': 2.034, 'train_loss': 0.9977304060260455, 'epoch': 1.0}\n",
            "100% 2400/2400 [19:39<00:00,  2.04it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/My Drive/falcon/wandb/offline-run-20230623_091956-k57dxlpl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230623_091956-k57dxlpl/logs\u001b[0m\n",
            "Train completed.\n",
            "Model Saved.\n"
          ]
        }
      ],
      "source": [
        "!falcontune finetune \\\n",
        "    --model=falcon-7b-instruct-4bit \\\n",
        "    --weights=gptq_model-4bit-64g.safetensors \\\n",
        "    --dataset=./instruct_fr_vicogne_sample.json \\\n",
        "    --data_type=alpaca \\\n",
        "    --lora_out_dir=./falcon-7b-sample-4bit/ \\\n",
        "    --mbatch_size=1 \\\n",
        "    --batch_size=2 \\\n",
        "    --epochs=1 \\\n",
        "    --lr=3e-4 \\\n",
        "    --cutoff_len=256 \\\n",
        "    --lora_r=8 \\\n",
        "    --lora_alpha=16 \\\n",
        "    --lora_dropout=0.05 \\\n",
        "    --warmup_steps=5 \\\n",
        "    --save_steps=50 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --logging_steps=5 \\\n",
        "    --target_modules='[\"query_key_value\"]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_84CU2sE46s"
      },
      "source": [
        "Si tout se passe bien, vous devrez avoir comlètement fini l'entraînement. Un petit message final apparaît vous invitant à partager le modèle sur HuggingFace: \"Training completed. Do not forget to share your model on huggingface.co/models =)\"\n",
        "\n",
        "Après un petit temps de synchronisation entre Google Colab, vous allez voir apparaître deux fichiers dans le dossier du modèle de fine-tuning : adapter_model.bin (le modèle proprepement dit) et adapter_model.config (un fichier de configuration). À noter que le modèle de fine-tuning est considérablement plus petit que le modèle d'origine : c'est en quelque sorte un modèle complémentaire qui vient ajuster le LLM (et il en aura toujours besoin pour fonctionner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aKvXziTziXY"
      },
      "source": [
        "# Générer du texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPV1WuvEvhD"
      },
      "source": [
        "Et maintenant il est possible de générer du texte. La fonction par défaut de falcontune n'est pas pour l'instant pas très pratique mais cela devrait s'améliorer prochainement. À noter aussi que les instructions trop brèves peut susciter un bug un peu agaçant (probability distribution error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L_qtyiFzgv2",
        "outputId": "6ad09d8e-5d57-44a3-e78a-de98e9890411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-23 09:46:25.828176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-wykdrroqipuc --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "The safetensors archive passed at gptq_model-4bit-64g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "Device map for lora: {'': 0}\n",
            "./falcon-7b-sample-4bit/ loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pour se rendre sur la Lune, une personne devrait embarquer sur un vaisseau spatial en orbite autour de la Terre, puis parcourir environ 384 400 miles (643 200 kilomètres) pour se rendre à la surface de la Lune. Le vaisseau spatial doit être conçu pour être capable de voyager dans l'espace et devrait être propulsé par un moteur à combustion pour le faire fonctionner. Le vaisseau spatial doit également être conçu pour protéger les passagers contre les effets de l'environnement extérieur, tels que la gravité, les radiations et les températures extrêmes. Une fois que le vaisseau spatial atteint la Lune, il se pose sur la surface lunaire et le passager sort du vaisseau pour explorer le terrain. Une fois que le vaisseau spatial est parti, il orbite la Lune jusqu'à la prochaine nuit terrestre,\n",
            "\n",
            "Took 29.129 s\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!falcontune generate \\\n",
        "    --model falcon-7b-instruct-4bit \\\n",
        "    --weights gptq_model-4bit-64g.safetensors \\\n",
        "    --lora_apply_dir ./falcon-7b-sample-4bit/ \\\n",
        "    --max_new_tokens 200 \\\n",
        "    --use_cache \\\n",
        "    --do_sample \\\n",
        "    --instruction \"Peux-tu m'expliquer en détail comment je pourrais me rendre sur la Lune ? ###OUTPUT\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
