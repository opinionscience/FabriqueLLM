{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXa7DOX84GG"
      },
      "source": [
        "# **Tutoriel** - Fine-tuning de romans du 17e siècle Falcon-7B-instruct-4bit\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/opinionscience/FabriqueLLM/main/illustration/falcon_image.png\" alt=\"Falcon logo\"  width=\"500\"/>\n",
        "\n",
        "Ce carnet de code mis à disposition par [OpSci](https://www.opsci.ai/fr/) permet d'effectuer le fine-tuning d'un grand modèle de langue, Falcon-7B avec la **version gratuite de Google Colab** sur un corpus complexe : 6000 extraits de romans en français 17e siècle. C'est un bon cas d'usage où le fine-tuning permet d'obtenir rapidement des résultats plus intéressants que GPT-4.\n",
        "\n",
        "Créé par le Technology Innovation Institute d'Abu Dhabi, Falcon est aujourd'hui le LLM open source de référence. La principale alternative, Llama, est réservée aux usages de recherche non commerciale. Falcon est disponible sous deux versions : la principale à 40 milliards de paramètres et une version plus légère que nous allons utiliser ici à 7 milliards de paramètres. Ce modèle inclut un corpus plus multilingue que d'autres LLMs ouverts comme Pythia ou MPT.\n",
        "\n",
        "Ce carnet de code utilise une version déjà ré-entraînée de Falcon-7 : instruct-4-bit. C'est aussi une version plus compacte de Falcon qui devrait pouvoir tourner sur une version gratuite de Google Colab : vous aurez besoin d'environ 10go de Vram. Ce fine-tuning sera aussi plus superficiel mais c'est déjà très pratique pour effectuer de premiers tests.\n",
        "\n",
        "Cette démonstration ne fait tourner qu'une seule *epoch* ce qui est suffisant pour avoir un premier aperçu. Pour obtenir un bon modèle, il est conseillé de faire tourner le fine-tuning pendant trois *epochs*. Sur notre corpus de démonstration de 6000 instructions une *epoch* prendra un peu plus d'une heure avec les GPUs gratuits de Google Colab. Un fine-tuning complet (trois *epochs*) prendra environ 3h : sur la version gratuite de Google Colab ce type de traitement d'une durée un peu longue risque d'être interrompu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTyUYSsywxD"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toute cette section ne doit être exécutée qu'une seule fois afin d'initialiser votre environnement d'installation sur Google Drive. Pour toutes les exécutions suivantes vous pouvez normalement la sauter."
      ],
      "metadata": {
        "id": "044T6qnIql7s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kamSkzAq9KE5"
      },
      "source": [
        "En tout premier lieu nous vérifions si nous disposons de suffisamment de mémoire vive (au moins 24go) sinon ce n'est pas la peine de lancer le script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwOy3-Dl9iRW",
        "outputId": "6eb8b90e-e143-49f2-b18a-55df4e34b560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun 11 14:57:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    40W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgZdKVNFIaXm"
      },
      "source": [
        "D'abord nous allons nous maintenant connecter à Google Drive. C'est vraiment recommandé et tout l'intérêt d'utiliser Google Colab. Autrement à l'expiration de la session tout le modèle sera perdu. À noter qu'il y a une latence plus ou moins importante entre Google Colab et Google Drive : vous ne verrez pas immédiatement les fichiers intermédiaires (*checkpoint*) et les fichiers finaux sur Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5sPY1BeIbRn",
        "outputId": "55fc7d54-91fb-456e-ddcc-eee92b4ce4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/falcon\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd \"/content/drive/My Drive/falcon\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6nmahaFHKEK"
      },
      "source": [
        "Nous installons falcontune. C'est une petite application python disponible de Github qui permet d'effectuer le fine-tuning de Falcon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyFPmAByvhV",
        "outputId": "39f27d62-ff60-451c-a22c-856d0a545e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'falcontune'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 124 (delta 43), reused 53 (delta 30), pack-reused 49\u001b[K\n",
            "Receiving objects: 100% (124/124), 63.60 KiB | 4.89 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rmihaylov/falcontune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKbNWMPpdsZg"
      },
      "source": [
        "Et nous récupérons aussi les poids du modèle :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ftYL3Vduas",
        "outputId": "54e9dc5f-ca27-4420-d2cb-865b55478c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-20 16:01:56--  https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/resolve/main/gptq_model-4bit-64g.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.43, 65.9.86.34, 65.9.86.125, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/9a/ea/9aea392ba3a1a4fa936207c60a9ba3cfa28fa3b935dfffd3227300a5ff38d088/ceb8ec3d0c432d043ec563d42e2571c94a5e884aedfb5acc6b38612a60490c7b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gptq_model-4bit-64g.safetensors%3B+filename%3D%22gptq_model-4bit-64g.safetensors%22%3B&Expires=1687536117&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzlhL2VhLzlhZWEzOTJiYTNhMWE0ZmE5MzYyMDdjNjBhOWJhM2NmYTI4ZmEzYjkzNWRmZmZkMzIyNzMwMGE1ZmYzOGQwODgvY2ViOGVjM2QwYzQzMmQwNDNlYzU2M2Q0MmUyNTcxYzk0YTVlODg0YWVkZmI1YWNjNmIzODYxMmE2MDQ5MGM3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc1MzYxMTd9fX1dfQ__&Signature=Qqi8tB6nsxrdwdyDAIZQIvKWiztgiNcFcqrLReBioTtzLiMgQntgVhSuw9%7ENTLkOLAUxja11KKVrV9v2QVZ8bAX28ptxiZiqoKFSQ8QYkKNOx6h1xRIPaKVlpS5X1W21oqAaq7gizZNbxp8B%7E6eqmBkK0k3%7EhYr9ctyOLLoB52tDrvRnWUGRGH9oqJbEAOg6IM7cjoeswH3rRGBrLoZ4P%7EX-JUKksKaXgpEJ-urD1gpZMPV42aSOTdT0CIAG%7ES5IQbgdf6PmdI2R2OHOx490KvyYDV%7ES4WNUT9xyQD9GcDkzIzq5u9fZp4K-EVOQvNAbXswReIcjbeM%7E77KVEiODsA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-20 16:01:56--  https://cdn-lfs.huggingface.co/repos/9a/ea/9aea392ba3a1a4fa936207c60a9ba3cfa28fa3b935dfffd3227300a5ff38d088/ceb8ec3d0c432d043ec563d42e2571c94a5e884aedfb5acc6b38612a60490c7b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gptq_model-4bit-64g.safetensors%3B+filename%3D%22gptq_model-4bit-64g.safetensors%22%3B&Expires=1687536117&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzlhL2VhLzlhZWEzOTJiYTNhMWE0ZmE5MzYyMDdjNjBhOWJhM2NmYTI4ZmEzYjkzNWRmZmZkMzIyNzMwMGE1ZmYzOGQwODgvY2ViOGVjM2QwYzQzMmQwNDNlYzU2M2Q0MmUyNTcxYzk0YTVlODg0YWVkZmI1YWNjNmIzODYxMmE2MDQ5MGM3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc1MzYxMTd9fX1dfQ__&Signature=Qqi8tB6nsxrdwdyDAIZQIvKWiztgiNcFcqrLReBioTtzLiMgQntgVhSuw9%7ENTLkOLAUxja11KKVrV9v2QVZ8bAX28ptxiZiqoKFSQ8QYkKNOx6h1xRIPaKVlpS5X1W21oqAaq7gizZNbxp8B%7E6eqmBkK0k3%7EhYr9ctyOLLoB52tDrvRnWUGRGH9oqJbEAOg6IM7cjoeswH3rRGBrLoZ4P%7EX-JUKksKaXgpEJ-urD1gpZMPV42aSOTdT0CIAG%7ES5IQbgdf6PmdI2R2OHOx490KvyYDV%7ES4WNUT9xyQD9GcDkzIzq5u9fZp4K-EVOQvNAbXswReIcjbeM%7E77KVEiODsA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 65.9.86.27, 65.9.86.11, 65.9.86.70, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|65.9.86.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5942953456 (5.5G) [binary/octet-stream]\n",
            "Saving to: ‘gptq_model-4bit-64g.safetensors’\n",
            "\n",
            "gptq_model-4bit-64g 100%[===================>]   5.53G  74.0MB/s    in 82s     \n",
            "\n",
            "2023-06-20 16:03:18 (69.4 MB/s) - ‘gptq_model-4bit-64g.safetensors’ saved [5942953456/5942953456]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/resolve/main/gptq_model-4bit-64g.safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFnsudjOHtnZ"
      },
      "source": [
        "## Le corpus d'instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PeUIuyeHOdM"
      },
      "source": [
        "Nous récupérons notre set d'instruction qui ne sont en réalité pas vraiment des \"instructions\" mais juste des extraits de romans du 17e siècle (avec la première partie du texte et la suite que le modèle est censé prédire)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjwy8C3kw4fF",
        "outputId": "466e8597-b32b-416e-f5cf-2538d3d8ce0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-23 09:50:58--  https://raw.githubusercontent.com/opinionscience/InstructionFr/main/public_domain/instruct_fr_novel17.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8361701 (8.0M) [text/plain]\n",
            "Saving to: ‘instruct_fr_novel17.json’\n",
            "\n",
            "instruct_fr_novel17 100%[===================>]   7.97M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-06-23 09:50:59 (66.7 MB/s) - ‘instruct_fr_novel17.json’ saved [8361701/8361701]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/opinionscience/InstructionFr/main/public_domain/instruct_fr_novel17.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSABEKyZ1Jy3"
      },
      "source": [
        "Ces instructions utilisent le format classique du projet Alpaca de Stanford : *instructions*, *input* (optionnellement) et *output*. En résumé, les instructions correspondent à des exemples de prompts que pourraient laisser les utilisateurs du LLM, les *outputs* à la réponse que le LLM devrait générer et les *inputs* apporte des éléments de contextes supplémentaires (par exemple sous la forme de textes cités en exemple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw-XyFFe1tvl",
        "outputId": "437a9071-d668-4b0d-a86b-db83cdaed6a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"instruction_id\": \"gbooks_Yvg5AAAAcAAJ.pdf_141\",\n",
            "    \"instruction\": \"El le pafle generalementpour allez grande mais il n'e\\u017ft pas aficur\\u00e9 que per\\u017fonne ne les puifle \\u017furpafler & comme tous mes compagnons ont allez bonne opinion de vous pour croire que vous pourriez bien au moins les egalerils m'ont charg\\u00e9 de vous \\u017folliciter \\u00e0 venir di fputer avec nous queiques -uns des prix que la bont\\u00e9 du Roi & les aplauditlements du peuple nous ont accordez. Ulfle qui pour plus d'une rai\\u017fon n'avoit point envie de \\u017fe m\\u00ealer dans ces jeux remercia Laodamas le plus honn\\u00ea . tement qu'il putlouant beaucoup au re\\u017fte& la force & l'adrefle de ceux qui avoient di\\u017fput\\u00e9 & lur tout de ceux qui s'\\u00e9toient dittingu\\u00e9s en remportant les prix .\",\n",
            "    \"output\": \"Si Lao damas fut content de cette mode\\u017fte repon\\u017feil n'en fut pas de m\\u00eame de quelques -uns des autres qui \\u017fe per\\u017fuadant que s'il \\u017fe defen doit d'entrer en licecela ne provenoit que de \\u017fa foiblefle & de fon peu d'experience dans ces \\u017fortes d'exercices \\u017fe mirent \\u00e0 le prefler d'u ne maniere fort incivile . Eurialus particuliere ment tout boufi de l'honneur qu'il venoit d'ac querir piqua Uli\\u017fle pardes paroles non leule. ment dures mais injurieu\\u017feslc taxant d'e\\u017ftre plus propre \\u00e0 courir la mer \\u017fur quelque vailleau CON\",\n",
            "    \"input\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"instruction_id\": \"gbooks_jSCVXaiKDT0C.pdf_267\",\n",
            "    \"instruction\": \"Comme ces deux Ou vrages \\u00e9toient trop au de\\u017f\\u017fus de \\u017fesles forces il \\u00e9toit en peine de trouver quelqu'un qui p\\u00fbt l'aider. Ceux qui l'avoient \\u017fervi n'en vouloient plus entendre parler parce qu'il n'avoit rien tenu de tout ce qu'il leur avoit promis. Dans cet embarras il e\\u00fbt recours \\u00e0 Terentius & lui jura que : s'il vouloit travailler avec lui non \\u017feulement il en auroit une \\u00e9ternelle : reconnoiffance mais qu'il lui don neroit dis lou\\u00efs d'or.\",\n",
            "    \"output\": \"Quoi que Te. rentius n'e\\u00fbt pas l'ameint\\u00e9re\\u017f\\u017f\\u00e9eil accepta la propofition & le \\u017fervit fi 3 bien que \\u017fon Pan\\u00e9gyrique& \\u017fes Hymnes eurent tout le \\u017fucc\\u00e8s qu'il en pouvoit e\\u017fp\\u00e9rer. Il re\\u00e7\\u00fat \\u017foixane. te pi\\u017ftoles de la Cour& dix des C .... Alors Latinus ne \\u017fe \\u017fouvint plus de fa parole ou pour mieux dire il ne voulut plus la tenir. Ce proc\\u00e9d\\u00e9 \\u017furprit Terentius ; & comme il lui en faifoit quelque reproche il Ora fon bonnetqu'il avoit \\u017fur la t\\u00eate & le jetra de d\\u00e9pit dans le feu. Te rentius le rama\\u017f\\u017fa auffit\\u00f4t & le lui remit \\u017fur la t\\u00eate avec autantderer pect\",\n",
            "    \"input\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"instruction_id\": \"gbooks_H3sfUXpI9TEC.pdf_196\",\n",
            "    \"instruction\": \"Aflig\\u00e9e. 179 se croire. Pour ne point encourir l\\u00e9 blata me de s'eftre lai\\u017f\\u017f\\u00e9 tromper \\u00e0 vne fille il manda \\u017fes parens s'informa de l'humeur de cette Bergere & apprit d'eux que ia mais elle n'auoit rien veu que \\u017fon villageo\\u00f9 ce peu n'en e\\u017ftoit pas fort e\\u017floign\\u00e9. Qu'il e\\u017ftoit vray que depuis peu elle les entretenoit \\u00e0 tous fous propos des guerres du Roy & des Anglois& mefme que les di\\u017f \\u00e7ours les auoit obligez de la retenir \\u00e0 la mai\\u017fon craignansqu'elle ne s'en allaftle Capicaine e\\u017ftoit \\u017fur le point de la renuo. yeriugeant que \\u017fes propo\\u017fitions n'auoi\\u1ebft pas alles d'appuy: ceux qui e\\u017ftoient au pr\\u00e9s de loy fe mocquoient de tout ce qu'elle auoit auanc\\u00e9.\",\n",
            "    \"output\": \"Ily en eut me\\u017fme qui ne \\u017fe peurent tenir de dire quelques bons mots de cette pauvre fille : d'autres effrontez conceurent des id\\u00e9es qui offen 1 \\u00e7oient \\u017fa cha\\u017ftet\\u00e9. Mais il arriua par gr\\u00e1d miracle,quetous ceux qui la regardoient auec impuret\\u00e9 qui prenoient de manuai fes pens\\u00e9es en la voyant furent rendus impui\\u017f\\u017fans & tous glacez pour iamais.Ce -\\u017fecret.n'e\\u017ftoitpas connu du \\u017fieur deBau ce dricourt'c'est pourquoy ce prodige ne le perluadoit pas lur la verit\\u00e9 de Leanne luy auoir dit. Au contraire toutes \\u043d\\u0456 cho\\u017fes\",\n",
            "    \"input\": \"\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open('instruct_fr_novel17.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "json_formatted_str = json.dumps(data[0:3], indent=2)\n",
        "\n",
        "print(json_formatted_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ4n2nXpAxtL"
      },
      "source": [
        "Nous procédons à l'installation de falcontune. Cela prendra 1-2 minutes. Vous devrez le refaire à chaque nouvelle session même si vous avez déjà chargé l'application sur Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQr_AC8sxBsp"
      },
      "outputs": [],
      "source": [
        "# Installation:\n",
        "!cd falcontune && pip install -r requirements.txt\n",
        "!cd falcontune && python setup.py install\n",
        "# !cd falcontune && python setup_cuda.py install  # if cuda, default is triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAV1qfdCSni8"
      },
      "source": [
        "Nous allons maintenant réinitiliser notre environnement de travail pour bien intégrer l'installation de falcontune. Tout va crasher mais c'est normal !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U56qd5G3frg"
      },
      "outputs": [],
      "source": [
        "# Restart:\n",
        "import os; os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78_DUqdzGrW"
      },
      "source": [
        "# Finetuning du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIgs_i0BNb5"
      },
      "source": [
        "Tout est prêt à lancer le fine-tuning du modèle. Nous allons juste désactiver Wandb (une extension utilisée par falcontune qui ne présente pas d'intérêt pour nous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdELbxDzIu7",
        "outputId": "cf4909aa-55a2-42e0-96e3-823777fe2aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/falcon\n"
          ]
        }
      ],
      "source": [
        "# Disable wandb:\n",
        "import os; os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "%cd \"/content/drive/My Drive/falcon\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhMpUgoS6oD"
      },
      "source": [
        "Et nous sommes prêt à lancer la grande commande. Il y a beaucoup de paramètre mais seulement quelqu'uns sont importants :\n",
        "* Nous allons utiliser le modèle Falcon-7b de base et leurs poids correspondants (tiiuae/falcon-7b)\n",
        "* Le fine-tuning sera effectué sur le set d'instruction *instruct_fr_novel17.json* (évidemment à changer si vous optez pour un autre jeu de données).\n",
        "* Les fichiers du modèle seront placés dans le dossier *falcon-7b-novel17c-4bit* (de nouveau à changer pour le nom de votre modèle).\n",
        "* Nous ne ferons tourner le fine-tuning que sur une *epoch* ce qui est suffisant pour un premier test.\n",
        "* Nous utilisons un taux d'apprentissage plus élevé que la recommandation par défaut comme le corpus est assez inhabituel.\n",
        "\n",
        "Après avoir lancé le script, Google Colab va tourner pendant un peu moins de 40 minutes.\n",
        "\n",
        "Si tout se passe bien vous verrez défiler le processus d'entraînement avec trois indicateurs régulièrement réactualisés : \"{'loss': 1.8581, 'learning_rate': 0.0002993736951983298, 'epoch': 0.0}\" :\n",
        "* Le \"loss\" c'est en quelque sorte le taux d'erreur du modèle : plus cette mesure est basse et plus le modèle parvient à prédire des textes assez approchants de ceux qui sont présent dans le corpus d'instruction.\n",
        "* Le *learning rate* (taux d'apprentissage) c'est la capacité du modèle à mémoriser de nouveaux éléments mais aussi à en oublier des anciens. Cet indicateur va constamment baisser au fur et à mesure de l'apprentissage.\n",
        "* L'*epoch* c'est le cycle d'apprentissage. Comme nous n'avons défini qu'une *epoch* cela correspondra à des pourcentages (de 0 à 0.99 à la fin de l'entraînement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiUKv07zGSm",
        "outputId": "429ce89f-0cea-4517-f660-bef6c97db838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-23 09:53:13.415168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('8013'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-1w6r6ll4exows --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Downloading (…)lve/main/config.json: 100% 728/728 [00:00<00:00, 2.76MB/s]\n",
            "The safetensors archive passed at gptq_model-4bit-64g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "Downloading (…)okenizer_config.json: 100% 220/220 [00:00<00:00, 1.56MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 2.73MB [00:00, 10.3MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 281/281 [00:00<00:00, 1.49MB/s]\n",
            "\n",
            "Parameters:\n",
            "-------config-------\n",
            "dataset='./instruct_fr_novel17.json'\n",
            "data_type='alpaca'\n",
            "lora_out_dir='./falcon-7b-novel17c-4bit/'\n",
            "lora_apply_dir=None\n",
            "weights='gptq_model-4bit-64g.safetensors'\n",
            "target_modules=['query_key_value']\n",
            "\n",
            "------training------\n",
            "mbatch_size=1\n",
            "batch_size=2\n",
            "gradient_accumulation_steps=2\n",
            "epochs=1\n",
            "lr=0.0005\n",
            "cutoff_len=256\n",
            "lora_r=8\n",
            "lora_alpha=16\n",
            "lora_dropout=0.05\n",
            "val_set_size=0.2\n",
            "gradient_checkpointing=False\n",
            "gradient_checkpointing_ratio=1\n",
            "warmup_steps=5\n",
            "save_steps=50\n",
            "save_total_limit=3\n",
            "logging_steps=5\n",
            "checkpoint=False\n",
            "skip=False\n",
            "world_size=1\n",
            "ddp=False\n",
            "device_map='auto'\n",
            "\n",
            "\n",
            "Converted as Half.\n",
            "trainable params: 2359296 || all params: 593597312 || trainable%: 0.3974573254132256\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-323c5c076819c1ce/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 2355.03it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 154.02it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-323c5c076819c1ce/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 215.31it/s]\n",
            "Run eval every 600 steps\n",
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using cuda_amp half precision backend\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction_id, input, token_type_ids, output, instruction. If instruction_id, input, token_type_ids, output, instruction are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 4,800\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2,400\n",
            "  Number of trainable parameters = 2,359,296\n",
            "  0% 0/2400 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 4.303, 'learning_rate': 0.0005, 'epoch': 0.0}\n",
            "{'loss': 4.3796, 'learning_rate': 0.0004989561586638831, 'epoch': 0.0}\n",
            "{'loss': 3.9768, 'learning_rate': 0.0004979123173277662, 'epoch': 0.01}\n",
            "{'loss': 3.9142, 'learning_rate': 0.0004968684759916493, 'epoch': 0.01}\n",
            "{'loss': 4.0907, 'learning_rate': 0.0004958246346555323, 'epoch': 0.01}\n",
            "{'loss': 3.9963, 'learning_rate': 0.0004949895615866388, 'epoch': 0.01}\n",
            "{'loss': 3.8066, 'learning_rate': 0.000493945720250522, 'epoch': 0.01}\n",
            "{'loss': 3.878, 'learning_rate': 0.000492901878914405, 'epoch': 0.02}\n",
            "{'loss': 3.7775, 'learning_rate': 0.0004918580375782881, 'epoch': 0.02}\n",
            "{'loss': 3.6911, 'learning_rate': 0.0004908141962421712, 'epoch': 0.02}\n",
            "  2% 50/2400 [00:50<13:26,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-50\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 3.8978, 'learning_rate': 0.0004897703549060544, 'epoch': 0.02}\n",
            "{'loss': 3.8734, 'learning_rate': 0.0004887265135699373, 'epoch': 0.03}\n",
            "{'loss': 3.5916, 'learning_rate': 0.00048768267223382045, 'epoch': 0.03}\n",
            "{'loss': 3.7556, 'learning_rate': 0.00048663883089770357, 'epoch': 0.03}\n",
            "{'loss': 3.6227, 'learning_rate': 0.00048559498956158664, 'epoch': 0.03}\n",
            "{'loss': 3.456, 'learning_rate': 0.00048455114822546977, 'epoch': 0.03}\n",
            "{'loss': 3.7089, 'learning_rate': 0.00048350730688935284, 'epoch': 0.04}\n",
            "{'loss': 3.756, 'learning_rate': 0.0004824634655532359, 'epoch': 0.04}\n",
            "{'loss': 3.6057, 'learning_rate': 0.000481419624217119, 'epoch': 0.04}\n",
            "{'loss': 3.7047, 'learning_rate': 0.0004803757828810021, 'epoch': 0.04}\n",
            "  4% 100/2400 [01:07<13:09,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 3.6371, 'learning_rate': 0.0004793319415448852, 'epoch': 0.04}\n",
            "{'loss': 3.6758, 'learning_rate': 0.0004782881002087683, 'epoch': 0.05}\n",
            "{'loss': 3.6399, 'learning_rate': 0.0004772442588726514, 'epoch': 0.05}\n",
            "{'loss': 3.3746, 'learning_rate': 0.00047620041753653446, 'epoch': 0.05}\n",
            "{'loss': 3.4822, 'learning_rate': 0.00047515657620041753, 'epoch': 0.05}\n",
            "{'loss': 3.6981, 'learning_rate': 0.0004741127348643006, 'epoch': 0.05}\n",
            "{'loss': 3.672, 'learning_rate': 0.0004730688935281837, 'epoch': 0.06}\n",
            "{'loss': 3.6825, 'learning_rate': 0.0004720250521920668, 'epoch': 0.06}\n",
            "{'loss': 3.3308, 'learning_rate': 0.0004709812108559499, 'epoch': 0.06}\n",
            "{'loss': 3.6159, 'learning_rate': 0.000469937369519833, 'epoch': 0.06}\n",
            "  6% 150/2400 [01:25<12:55,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 3.6854, 'learning_rate': 0.0004688935281837161, 'epoch': 0.06}\n",
            "{'loss': 3.5441, 'learning_rate': 0.00046784968684759914, 'epoch': 0.07}\n",
            "{'loss': 3.5312, 'learning_rate': 0.00046680584551148227, 'epoch': 0.07}\n",
            "{'loss': 3.5743, 'learning_rate': 0.00046576200417536534, 'epoch': 0.07}\n",
            "{'loss': 3.7554, 'learning_rate': 0.00046471816283924847, 'epoch': 0.07}\n",
            "{'loss': 3.4687, 'learning_rate': 0.00046367432150313154, 'epoch': 0.07}\n",
            "{'loss': 3.4755, 'learning_rate': 0.00046263048016701466, 'epoch': 0.08}\n",
            "{'loss': 3.428, 'learning_rate': 0.0004615866388308977, 'epoch': 0.08}\n",
            "{'loss': 3.4448, 'learning_rate': 0.0004605427974947808, 'epoch': 0.08}\n",
            "{'loss': 3.6274, 'learning_rate': 0.0004594989561586639, 'epoch': 0.08}\n",
            "  8% 200/2400 [01:42<12:34,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-50] due to args.save_total_limit\n",
            "{'loss': 3.9505, 'learning_rate': 0.000458455114822547, 'epoch': 0.09}\n",
            "{'loss': 3.5333, 'learning_rate': 0.0004574112734864301, 'epoch': 0.09}\n",
            "{'loss': 3.3165, 'learning_rate': 0.0004563674321503132, 'epoch': 0.09}\n",
            "{'loss': 3.7135, 'learning_rate': 0.0004553235908141963, 'epoch': 0.09}\n",
            "{'loss': 3.4385, 'learning_rate': 0.0004542797494780793, 'epoch': 0.09}\n",
            "{'loss': 3.5735, 'learning_rate': 0.0004532359081419624, 'epoch': 0.1}\n",
            "{'loss': 3.7872, 'learning_rate': 0.0004521920668058455, 'epoch': 0.1}\n",
            "{'loss': 3.2275, 'learning_rate': 0.0004511482254697286, 'epoch': 0.1}\n",
            "{'loss': 3.6396, 'learning_rate': 0.0004501043841336117, 'epoch': 0.1}\n",
            "{'loss': 3.6454, 'learning_rate': 0.0004490605427974948, 'epoch': 0.1}\n",
            " 10% 250/2400 [01:59<12:17,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 3.5486, 'learning_rate': 0.0004480167014613779, 'epoch': 0.11}\n",
            "{'loss': 3.462, 'learning_rate': 0.00044697286012526096, 'epoch': 0.11}\n",
            "{'loss': 3.3435, 'learning_rate': 0.00044592901878914403, 'epoch': 0.11}\n",
            "{'loss': 3.3957, 'learning_rate': 0.00044488517745302716, 'epoch': 0.11}\n",
            "{'loss': 3.2912, 'learning_rate': 0.00044384133611691023, 'epoch': 0.11}\n",
            "{'loss': 3.1976, 'learning_rate': 0.00044279749478079336, 'epoch': 0.12}\n",
            "{'loss': 3.3912, 'learning_rate': 0.00044175365344467643, 'epoch': 0.12}\n",
            "{'loss': 3.5385, 'learning_rate': 0.0004407098121085595, 'epoch': 0.12}\n",
            "{'loss': 3.2241, 'learning_rate': 0.0004396659707724426, 'epoch': 0.12}\n",
            "{'loss': 3.8015, 'learning_rate': 0.0004386221294363257, 'epoch': 0.12}\n",
            " 12% 300/2400 [02:17<12:05,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-150] due to args.save_total_limit\n",
            "{'loss': 3.5649, 'learning_rate': 0.0004375782881002088, 'epoch': 0.13}\n",
            "{'loss': 3.3703, 'learning_rate': 0.0004365344467640919, 'epoch': 0.13}\n",
            "{'loss': 3.2751, 'learning_rate': 0.00043549060542797497, 'epoch': 0.13}\n",
            "{'loss': 3.471, 'learning_rate': 0.0004344467640918581, 'epoch': 0.13}\n",
            "{'loss': 3.6928, 'learning_rate': 0.0004334029227557411, 'epoch': 0.14}\n",
            "{'loss': 3.7421, 'learning_rate': 0.0004323590814196242, 'epoch': 0.14}\n",
            "{'loss': 3.6753, 'learning_rate': 0.0004313152400835073, 'epoch': 0.14}\n",
            "{'loss': 3.3137, 'learning_rate': 0.0004302713987473904, 'epoch': 0.14}\n",
            "{'loss': 3.5818, 'learning_rate': 0.0004292275574112735, 'epoch': 0.14}\n",
            "{'loss': 3.6591, 'learning_rate': 0.0004281837160751566, 'epoch': 0.15}\n",
            " 15% 350/2400 [02:34<11:45,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 3.4008, 'learning_rate': 0.0004271398747390397, 'epoch': 0.15}\n",
            "{'loss': 3.5653, 'learning_rate': 0.00042609603340292273, 'epoch': 0.15}\n",
            "{'loss': 3.7191, 'learning_rate': 0.00042505219206680586, 'epoch': 0.15}\n",
            "{'loss': 3.6513, 'learning_rate': 0.00042400835073068893, 'epoch': 0.15}\n",
            "{'loss': 3.4637, 'learning_rate': 0.00042296450939457205, 'epoch': 0.16}\n",
            "{'loss': 3.5914, 'learning_rate': 0.0004219206680584551, 'epoch': 0.16}\n",
            "{'loss': 3.2158, 'learning_rate': 0.00042087682672233825, 'epoch': 0.16}\n",
            "{'loss': 3.4632, 'learning_rate': 0.00041983298538622127, 'epoch': 0.16}\n",
            "{'loss': 3.3001, 'learning_rate': 0.0004187891440501044, 'epoch': 0.16}\n",
            "{'loss': 3.4845, 'learning_rate': 0.00041774530271398747, 'epoch': 0.17}\n",
            " 17% 400/2400 [02:52<11:25,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 3.4904, 'learning_rate': 0.0004167014613778706, 'epoch': 0.17}\n",
            "{'loss': 3.4928, 'learning_rate': 0.00041565762004175367, 'epoch': 0.17}\n",
            "{'loss': 3.2117, 'learning_rate': 0.0004146137787056368, 'epoch': 0.17}\n",
            "{'loss': 3.4971, 'learning_rate': 0.00041356993736951987, 'epoch': 0.17}\n",
            "{'loss': 3.3647, 'learning_rate': 0.0004125260960334029, 'epoch': 0.18}\n",
            "{'loss': 3.4884, 'learning_rate': 0.000411482254697286, 'epoch': 0.18}\n",
            "{'loss': 3.3135, 'learning_rate': 0.0004104384133611691, 'epoch': 0.18}\n",
            "{'loss': 3.5799, 'learning_rate': 0.0004093945720250522, 'epoch': 0.18}\n",
            "{'loss': 3.6107, 'learning_rate': 0.0004083507306889353, 'epoch': 0.19}\n",
            "{'loss': 3.154, 'learning_rate': 0.0004073068893528184, 'epoch': 0.19}\n",
            " 19% 450/2400 [03:09<11:12,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-450\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 3.4347, 'learning_rate': 0.0004062630480167015, 'epoch': 0.19}\n",
            "{'loss': 3.5459, 'learning_rate': 0.00040521920668058455, 'epoch': 0.19}\n",
            "{'loss': 3.1965, 'learning_rate': 0.0004041753653444676, 'epoch': 0.19}\n",
            "{'loss': 3.2517, 'learning_rate': 0.00040313152400835075, 'epoch': 0.2}\n",
            "{'loss': 3.8118, 'learning_rate': 0.0004020876826722338, 'epoch': 0.2}\n",
            "{'loss': 3.4315, 'learning_rate': 0.00040104384133611695, 'epoch': 0.2}\n",
            "{'loss': 3.612, 'learning_rate': 0.0004, 'epoch': 0.2}\n",
            "{'loss': 3.309, 'learning_rate': 0.0003989561586638831, 'epoch': 0.2}\n",
            "{'loss': 3.6483, 'learning_rate': 0.00039791231732776616, 'epoch': 0.21}\n",
            "{'loss': 3.3403, 'learning_rate': 0.0003968684759916493, 'epoch': 0.21}\n",
            " 21% 500/2400 [03:26<10:57,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-350] due to args.save_total_limit\n",
            "{'loss': 3.119, 'learning_rate': 0.00039582463465553236, 'epoch': 0.21}\n",
            "{'loss': 3.6566, 'learning_rate': 0.0003947807933194155, 'epoch': 0.21}\n",
            "{'loss': 3.4729, 'learning_rate': 0.00039373695198329856, 'epoch': 0.21}\n",
            "{'loss': 3.715, 'learning_rate': 0.0003926931106471817, 'epoch': 0.22}\n",
            "{'loss': 3.4671, 'learning_rate': 0.0003916492693110647, 'epoch': 0.22}\n",
            "{'loss': 3.3514, 'learning_rate': 0.0003906054279749478, 'epoch': 0.22}\n",
            "{'loss': 3.4, 'learning_rate': 0.0003895615866388309, 'epoch': 0.22}\n",
            "{'loss': 3.28, 'learning_rate': 0.000388517745302714, 'epoch': 0.23}\n",
            "{'loss': 3.3195, 'learning_rate': 0.0003874739039665971, 'epoch': 0.23}\n",
            "{'loss': 3.3904, 'learning_rate': 0.0003864300626304802, 'epoch': 0.23}\n",
            " 23% 550/2400 [03:44<10:32,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-550\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 3.5246, 'learning_rate': 0.00038538622129436325, 'epoch': 0.23}\n",
            "{'loss': 3.5315, 'learning_rate': 0.0003843423799582463, 'epoch': 0.23}\n",
            "{'loss': 3.326, 'learning_rate': 0.00038329853862212944, 'epoch': 0.24}\n",
            "{'loss': 3.437, 'learning_rate': 0.0003822546972860125, 'epoch': 0.24}\n",
            "{'loss': 3.45, 'learning_rate': 0.00038121085594989564, 'epoch': 0.24}\n",
            "{'loss': 3.5814, 'learning_rate': 0.0003801670146137787, 'epoch': 0.24}\n",
            "{'loss': 3.7695, 'learning_rate': 0.00037912317327766184, 'epoch': 0.24}\n",
            "{'loss': 3.2533, 'learning_rate': 0.00037807933194154486, 'epoch': 0.25}\n",
            "{'loss': 3.539, 'learning_rate': 0.000377035490605428, 'epoch': 0.25}\n",
            "{'loss': 3.1266, 'learning_rate': 0.00037599164926931106, 'epoch': 0.25}\n",
            " 25% 600/2400 [04:01<10:16,  2.92it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction_id, input, token_type_ids, output, instruction. If instruction_id, input, token_type_ids, output, instruction are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:37,  3.97it/s]\u001b[A\n",
            "  2% 3/150 [00:01<00:52,  2.79it/s]\u001b[A\n",
            "  3% 4/150 [00:01<01:00,  2.41it/s]\u001b[A\n",
            "  3% 5/150 [00:02<01:04,  2.24it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:07,  2.14it/s]\u001b[A\n",
            "  5% 7/150 [00:03<01:08,  2.08it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:09,  2.04it/s]\u001b[A\n",
            "  6% 9/150 [00:04<01:09,  2.02it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:09,  2.01it/s]\u001b[A\n",
            "  7% 11/150 [00:05<01:09,  2.00it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:09,  1.99it/s]\u001b[A\n",
            "  9% 13/150 [00:06<01:09,  1.98it/s]\u001b[A\n",
            "  9% 14/150 [00:06<01:08,  1.98it/s]\u001b[A\n",
            " 10% 15/150 [00:07<01:08,  1.98it/s]\u001b[A\n",
            " 11% 16/150 [00:07<01:07,  1.98it/s]\u001b[A\n",
            " 11% 17/150 [00:08<01:07,  1.97it/s]\u001b[A\n",
            " 12% 18/150 [00:08<01:06,  1.97it/s]\u001b[A\n",
            " 13% 19/150 [00:09<01:06,  1.97it/s]\u001b[A\n",
            " 13% 20/150 [00:09<01:05,  1.97it/s]\u001b[A\n",
            " 14% 21/150 [00:10<01:05,  1.97it/s]\u001b[A\n",
            " 15% 22/150 [00:10<01:05,  1.97it/s]\u001b[A\n",
            " 15% 23/150 [00:11<01:04,  1.97it/s]\u001b[A\n",
            " 16% 24/150 [00:11<01:03,  1.97it/s]\u001b[A\n",
            " 17% 25/150 [00:12<01:03,  1.97it/s]\u001b[A\n",
            " 17% 26/150 [00:12<01:02,  1.97it/s]\u001b[A\n",
            " 18% 27/150 [00:13<01:02,  1.97it/s]\u001b[A\n",
            " 19% 28/150 [00:13<01:01,  1.97it/s]\u001b[A\n",
            " 19% 29/150 [00:14<01:01,  1.97it/s]\u001b[A\n",
            " 20% 30/150 [00:14<01:00,  1.97it/s]\u001b[A\n",
            " 21% 31/150 [00:15<01:00,  1.97it/s]\u001b[A\n",
            " 21% 32/150 [00:15<00:59,  1.97it/s]\u001b[A\n",
            " 22% 33/150 [00:16<00:59,  1.97it/s]\u001b[A\n",
            " 23% 34/150 [00:16<00:58,  1.97it/s]\u001b[A\n",
            " 23% 35/150 [00:17<00:58,  1.97it/s]\u001b[A\n",
            " 24% 36/150 [00:17<00:57,  1.97it/s]\u001b[A\n",
            " 25% 37/150 [00:18<00:57,  1.97it/s]\u001b[A\n",
            " 25% 38/150 [00:18<00:56,  1.97it/s]\u001b[A\n",
            " 26% 39/150 [00:19<00:56,  1.97it/s]\u001b[A\n",
            " 27% 40/150 [00:19<00:55,  1.97it/s]\u001b[A\n",
            " 27% 41/150 [00:20<00:55,  1.97it/s]\u001b[A\n",
            " 28% 42/150 [00:20<00:54,  1.97it/s]\u001b[A\n",
            " 29% 43/150 [00:21<00:54,  1.97it/s]\u001b[A\n",
            " 29% 44/150 [00:21<00:53,  1.97it/s]\u001b[A\n",
            " 30% 45/150 [00:22<00:53,  1.97it/s]\u001b[A\n",
            " 31% 46/150 [00:22<00:52,  1.97it/s]\u001b[A\n",
            " 31% 47/150 [00:23<00:52,  1.97it/s]\u001b[A\n",
            " 32% 48/150 [00:23<00:51,  1.97it/s]\u001b[A\n",
            " 33% 49/150 [00:24<00:51,  1.97it/s]\u001b[A\n",
            " 33% 50/150 [00:24<00:50,  1.97it/s]\u001b[A\n",
            " 34% 51/150 [00:25<00:50,  1.97it/s]\u001b[A\n",
            " 35% 52/150 [00:25<00:49,  1.97it/s]\u001b[A\n",
            " 35% 53/150 [00:26<00:49,  1.97it/s]\u001b[A\n",
            " 36% 54/150 [00:26<00:48,  1.97it/s]\u001b[A\n",
            " 37% 55/150 [00:27<00:48,  1.97it/s]\u001b[A\n",
            " 37% 56/150 [00:27<00:47,  1.97it/s]\u001b[A\n",
            " 38% 57/150 [00:28<00:47,  1.97it/s]\u001b[A\n",
            " 39% 58/150 [00:28<00:46,  1.97it/s]\u001b[A\n",
            " 39% 59/150 [00:29<00:46,  1.97it/s]\u001b[A\n",
            " 40% 60/150 [00:29<00:45,  1.97it/s]\u001b[A\n",
            " 41% 61/150 [00:30<00:45,  1.97it/s]\u001b[A\n",
            " 41% 62/150 [00:30<00:44,  1.97it/s]\u001b[A\n",
            " 42% 63/150 [00:31<00:44,  1.97it/s]\u001b[A\n",
            " 43% 64/150 [00:31<00:43,  1.97it/s]\u001b[A\n",
            " 43% 65/150 [00:32<00:43,  1.97it/s]\u001b[A\n",
            " 44% 66/150 [00:32<00:42,  1.97it/s]\u001b[A\n",
            " 45% 67/150 [00:33<00:42,  1.97it/s]\u001b[A\n",
            " 45% 68/150 [00:33<00:41,  1.97it/s]\u001b[A\n",
            " 46% 69/150 [00:34<00:41,  1.97it/s]\u001b[A\n",
            " 47% 70/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 47% 71/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 48% 72/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 73/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 74/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 50% 75/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 51% 76/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 51% 77/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 52% 78/150 [00:39<00:36,  1.97it/s]\u001b[A\n",
            " 53% 79/150 [00:39<00:36,  1.97it/s]\u001b[A\n",
            " 53% 80/150 [00:40<00:35,  1.97it/s]\u001b[A\n",
            " 54% 81/150 [00:40<00:35,  1.97it/s]\u001b[A\n",
            " 55% 82/150 [00:41<00:34,  1.97it/s]\u001b[A\n",
            " 55% 83/150 [00:41<00:33,  1.97it/s]\u001b[A\n",
            " 56% 84/150 [00:42<00:33,  1.97it/s]\u001b[A\n",
            " 57% 85/150 [00:42<00:32,  1.97it/s]\u001b[A\n",
            " 57% 86/150 [00:43<00:32,  1.97it/s]\u001b[A\n",
            " 58% 87/150 [00:43<00:31,  1.97it/s]\u001b[A\n",
            " 59% 88/150 [00:44<00:31,  1.97it/s]\u001b[A\n",
            " 59% 89/150 [00:44<00:30,  1.97it/s]\u001b[A\n",
            " 60% 90/150 [00:45<00:30,  1.97it/s]\u001b[A\n",
            " 61% 91/150 [00:45<00:29,  1.97it/s]\u001b[A\n",
            " 61% 92/150 [00:46<00:29,  1.97it/s]\u001b[A\n",
            " 62% 93/150 [00:46<00:28,  1.97it/s]\u001b[A\n",
            " 63% 94/150 [00:47<00:28,  1.97it/s]\u001b[A\n",
            " 63% 95/150 [00:47<00:27,  1.97it/s]\u001b[A\n",
            " 64% 96/150 [00:48<00:27,  1.97it/s]\u001b[A\n",
            " 65% 97/150 [00:48<00:26,  1.97it/s]\u001b[A\n",
            " 65% 98/150 [00:49<00:26,  1.97it/s]\u001b[A\n",
            " 66% 99/150 [00:49<00:25,  1.97it/s]\u001b[A\n",
            " 67% 100/150 [00:50<00:25,  1.97it/s]\u001b[A\n",
            " 67% 101/150 [00:50<00:24,  1.97it/s]\u001b[A\n",
            " 68% 102/150 [00:51<00:24,  1.97it/s]\u001b[A\n",
            " 69% 103/150 [00:51<00:23,  1.97it/s]\u001b[A\n",
            " 69% 104/150 [00:52<00:23,  1.97it/s]\u001b[A\n",
            " 70% 105/150 [00:52<00:22,  1.97it/s]\u001b[A\n",
            " 71% 106/150 [00:53<00:22,  1.97it/s]\u001b[A\n",
            " 71% 107/150 [00:53<00:21,  1.97it/s]\u001b[A\n",
            " 72% 108/150 [00:54<00:21,  1.97it/s]\u001b[A\n",
            " 73% 109/150 [00:54<00:20,  1.97it/s]\u001b[A\n",
            " 73% 110/150 [00:55<00:20,  1.97it/s]\u001b[A\n",
            " 74% 111/150 [00:55<00:19,  1.97it/s]\u001b[A\n",
            " 75% 112/150 [00:56<00:19,  1.97it/s]\u001b[A\n",
            " 75% 113/150 [00:56<00:18,  1.97it/s]\u001b[A\n",
            " 76% 114/150 [00:57<00:18,  1.97it/s]\u001b[A\n",
            " 77% 115/150 [00:57<00:17,  1.97it/s]\u001b[A\n",
            " 77% 116/150 [00:58<00:17,  1.97it/s]\u001b[A\n",
            " 78% 117/150 [00:58<00:16,  1.97it/s]\u001b[A\n",
            " 79% 118/150 [00:59<00:16,  1.97it/s]\u001b[A\n",
            " 79% 119/150 [00:59<00:15,  1.97it/s]\u001b[A\n",
            " 80% 120/150 [01:00<00:15,  1.97it/s]\u001b[A\n",
            " 81% 121/150 [01:00<00:14,  1.97it/s]\u001b[A\n",
            " 81% 122/150 [01:01<00:14,  1.97it/s]\u001b[A\n",
            " 82% 123/150 [01:01<00:13,  1.97it/s]\u001b[A\n",
            " 83% 124/150 [01:02<00:13,  1.97it/s]\u001b[A\n",
            " 83% 125/150 [01:02<00:12,  1.97it/s]\u001b[A\n",
            " 84% 126/150 [01:03<00:12,  1.97it/s]\u001b[A\n",
            " 85% 127/150 [01:03<00:11,  1.97it/s]\u001b[A\n",
            " 85% 128/150 [01:04<00:11,  1.97it/s]\u001b[A\n",
            " 86% 129/150 [01:04<00:10,  1.97it/s]\u001b[A\n",
            " 87% 130/150 [01:05<00:10,  1.97it/s]\u001b[A\n",
            " 87% 131/150 [01:05<00:09,  1.97it/s]\u001b[A\n",
            " 88% 132/150 [01:06<00:09,  1.97it/s]\u001b[A\n",
            " 89% 133/150 [01:06<00:08,  1.97it/s]\u001b[A\n",
            " 89% 134/150 [01:07<00:08,  1.97it/s]\u001b[A\n",
            " 90% 135/150 [01:08<00:07,  1.97it/s]\u001b[A\n",
            " 91% 136/150 [01:08<00:07,  1.97it/s]\u001b[A\n",
            " 91% 137/150 [01:09<00:06,  1.97it/s]\u001b[A\n",
            " 92% 138/150 [01:09<00:06,  1.97it/s]\u001b[A\n",
            " 93% 139/150 [01:10<00:05,  1.97it/s]\u001b[A\n",
            " 93% 140/150 [01:10<00:05,  1.97it/s]\u001b[A\n",
            " 94% 141/150 [01:11<00:04,  1.97it/s]\u001b[A\n",
            " 95% 142/150 [01:11<00:04,  1.97it/s]\u001b[A\n",
            " 95% 143/150 [01:12<00:03,  1.97it/s]\u001b[A\n",
            " 96% 144/150 [01:12<00:03,  1.97it/s]\u001b[A\n",
            " 97% 145/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 97% 146/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 98% 147/150 [01:14<00:01,  1.97it/s]\u001b[A\n",
            " 99% 148/150 [01:14<00:01,  1.97it/s]\u001b[A\n",
            " 99% 149/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 3.4171857833862305, 'eval_runtime': 88.3924, 'eval_samples_per_second': 13.576, 'eval_steps_per_second': 1.697, 'epoch': 0.25}\n",
            " 25% 600/2400 [05:29<10:16,  2.92it/s]\n",
            "100% 150/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-450] due to args.save_total_limit\n",
            "{'loss': 3.2554, 'learning_rate': 0.0003749478079331942, 'epoch': 0.25}\n",
            "{'loss': 3.4152, 'learning_rate': 0.00037390396659707726, 'epoch': 0.25}\n",
            "{'loss': 3.3561, 'learning_rate': 0.0003728601252609604, 'epoch': 0.26}\n",
            "{'loss': 3.6105, 'learning_rate': 0.00037181628392484345, 'epoch': 0.26}\n",
            "{'loss': 3.3454, 'learning_rate': 0.0003707724425887265, 'epoch': 0.26}\n",
            "{'loss': 3.5117, 'learning_rate': 0.0003697286012526096, 'epoch': 0.26}\n",
            "{'loss': 3.5175, 'learning_rate': 0.00036868475991649267, 'epoch': 0.26}\n",
            "{'loss': 3.4413, 'learning_rate': 0.0003676409185803758, 'epoch': 0.27}\n",
            "{'loss': 3.6381, 'learning_rate': 0.00036659707724425887, 'epoch': 0.27}\n",
            "{'loss': 3.7294, 'learning_rate': 0.000365553235908142, 'epoch': 0.27}\n",
            " 27% 650/2400 [05:47<10:04,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-650\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 3.2439, 'learning_rate': 0.000364509394572025, 'epoch': 0.27}\n",
            "{'loss': 3.5085, 'learning_rate': 0.00036346555323590814, 'epoch': 0.28}\n",
            "{'loss': 3.3908, 'learning_rate': 0.0003624217118997912, 'epoch': 0.28}\n",
            "{'loss': 3.2805, 'learning_rate': 0.00036137787056367434, 'epoch': 0.28}\n",
            "{'loss': 3.5037, 'learning_rate': 0.0003603340292275574, 'epoch': 0.28}\n",
            "{'loss': 3.3978, 'learning_rate': 0.00035929018789144054, 'epoch': 0.28}\n",
            "{'loss': 3.3493, 'learning_rate': 0.0003582463465553236, 'epoch': 0.29}\n",
            "{'loss': 3.4447, 'learning_rate': 0.0003572025052192067, 'epoch': 0.29}\n",
            "{'loss': 3.4484, 'learning_rate': 0.00035615866388308975, 'epoch': 0.29}\n",
            "{'loss': 3.2297, 'learning_rate': 0.0003551148225469729, 'epoch': 0.29}\n",
            " 29% 700/2400 [06:04<09:45,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-550] due to args.save_total_limit\n",
            "{'loss': 3.3226, 'learning_rate': 0.00035407098121085595, 'epoch': 0.29}\n",
            "{'loss': 3.1574, 'learning_rate': 0.0003530271398747391, 'epoch': 0.3}\n",
            "{'loss': 3.44, 'learning_rate': 0.00035198329853862215, 'epoch': 0.3}\n",
            "{'loss': 3.3014, 'learning_rate': 0.0003509394572025053, 'epoch': 0.3}\n",
            "{'loss': 3.689, 'learning_rate': 0.0003498956158663883, 'epoch': 0.3}\n",
            "{'loss': 3.6931, 'learning_rate': 0.0003488517745302714, 'epoch': 0.3}\n",
            "{'loss': 3.6167, 'learning_rate': 0.0003478079331941545, 'epoch': 0.31}\n",
            "{'loss': 3.3809, 'learning_rate': 0.00034676409185803756, 'epoch': 0.31}\n",
            "{'loss': 3.1096, 'learning_rate': 0.0003457202505219207, 'epoch': 0.31}\n",
            "{'loss': 3.1664, 'learning_rate': 0.00034467640918580376, 'epoch': 0.31}\n",
            " 31% 750/2400 [06:22<09:26,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 3.2135, 'learning_rate': 0.00034363256784968683, 'epoch': 0.31}\n",
            "{'loss': 3.6711, 'learning_rate': 0.0003425887265135699, 'epoch': 0.32}\n",
            "{'loss': 3.4513, 'learning_rate': 0.00034154488517745303, 'epoch': 0.32}\n",
            "{'loss': 3.1929, 'learning_rate': 0.0003405010438413361, 'epoch': 0.32}\n",
            "{'loss': 3.1405, 'learning_rate': 0.00033945720250521923, 'epoch': 0.32}\n",
            "{'loss': 3.2666, 'learning_rate': 0.0003384133611691023, 'epoch': 0.33}\n",
            "{'loss': 3.4059, 'learning_rate': 0.00033736951983298543, 'epoch': 0.33}\n",
            "{'loss': 3.3662, 'learning_rate': 0.00033632567849686845, 'epoch': 0.33}\n",
            "{'loss': 3.2654, 'learning_rate': 0.0003352818371607516, 'epoch': 0.33}\n",
            "{'loss': 3.2626, 'learning_rate': 0.00033423799582463465, 'epoch': 0.33}\n",
            " 33% 800/2400 [06:39<09:09,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-650] due to args.save_total_limit\n",
            "{'loss': 3.2716, 'learning_rate': 0.00033319415448851777, 'epoch': 0.34}\n",
            "{'loss': 3.4685, 'learning_rate': 0.00033215031315240084, 'epoch': 0.34}\n",
            "{'loss': 3.3102, 'learning_rate': 0.00033110647181628397, 'epoch': 0.34}\n",
            "{'loss': 3.4596, 'learning_rate': 0.00033006263048016704, 'epoch': 0.34}\n",
            "{'loss': 3.3591, 'learning_rate': 0.0003290187891440501, 'epoch': 0.34}\n",
            "{'loss': 3.3391, 'learning_rate': 0.0003279749478079332, 'epoch': 0.35}\n",
            "{'loss': 3.1772, 'learning_rate': 0.0003269311064718163, 'epoch': 0.35}\n",
            "{'loss': 3.312, 'learning_rate': 0.0003258872651356994, 'epoch': 0.35}\n",
            "{'loss': 3.1539, 'learning_rate': 0.00032484342379958246, 'epoch': 0.35}\n",
            "{'loss': 3.3724, 'learning_rate': 0.0003237995824634656, 'epoch': 0.35}\n",
            " 35% 850/2400 [06:56<08:57,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-850\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 3.2185, 'learning_rate': 0.0003227557411273486, 'epoch': 0.36}\n",
            "{'loss': 3.3326, 'learning_rate': 0.00032171189979123173, 'epoch': 0.36}\n",
            "{'loss': 3.2912, 'learning_rate': 0.0003206680584551148, 'epoch': 0.36}\n",
            "{'loss': 3.3216, 'learning_rate': 0.0003196242171189979, 'epoch': 0.36}\n",
            "{'loss': 3.5209, 'learning_rate': 0.000318580375782881, 'epoch': 0.36}\n",
            "{'loss': 3.6082, 'learning_rate': 0.0003175365344467641, 'epoch': 0.37}\n",
            "{'loss': 3.4936, 'learning_rate': 0.0003164926931106472, 'epoch': 0.37}\n",
            "{'loss': 3.2595, 'learning_rate': 0.00031544885177453027, 'epoch': 0.37}\n",
            "{'loss': 3.4775, 'learning_rate': 0.00031440501043841334, 'epoch': 0.37}\n",
            "{'loss': 3.6688, 'learning_rate': 0.00031336116910229647, 'epoch': 0.38}\n",
            " 38% 900/2400 [07:14<08:38,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 3.6393, 'learning_rate': 0.00031231732776617954, 'epoch': 0.38}\n",
            "{'loss': 3.4916, 'learning_rate': 0.00031127348643006267, 'epoch': 0.38}\n",
            "{'loss': 3.5166, 'learning_rate': 0.00031022964509394574, 'epoch': 0.38}\n",
            "{'loss': 3.6902, 'learning_rate': 0.00030918580375782886, 'epoch': 0.38}\n",
            "{'loss': 3.4019, 'learning_rate': 0.0003081419624217119, 'epoch': 0.39}\n",
            "{'loss': 3.5153, 'learning_rate': 0.000307098121085595, 'epoch': 0.39}\n",
            "{'loss': 3.4348, 'learning_rate': 0.0003060542797494781, 'epoch': 0.39}\n",
            "{'loss': 3.2509, 'learning_rate': 0.0003050104384133612, 'epoch': 0.39}\n",
            "{'loss': 3.7322, 'learning_rate': 0.0003039665970772443, 'epoch': 0.39}\n",
            "{'loss': 3.2669, 'learning_rate': 0.00030292275574112735, 'epoch': 0.4}\n",
            " 40% 950/2400 [07:31<08:20,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-950\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 3.3432, 'learning_rate': 0.0003018789144050104, 'epoch': 0.4}\n",
            "{'loss': 3.4024, 'learning_rate': 0.0003008350730688935, 'epoch': 0.4}\n",
            "{'loss': 3.4553, 'learning_rate': 0.0002997912317327766, 'epoch': 0.4}\n",
            "{'loss': 3.5186, 'learning_rate': 0.0002987473903966597, 'epoch': 0.4}\n",
            "{'loss': 3.2189, 'learning_rate': 0.0002977035490605428, 'epoch': 0.41}\n",
            "{'loss': 3.6656, 'learning_rate': 0.0002966597077244259, 'epoch': 0.41}\n",
            "{'loss': 3.5392, 'learning_rate': 0.000295615866388309, 'epoch': 0.41}\n",
            "{'loss': 3.4535, 'learning_rate': 0.00029457202505219204, 'epoch': 0.41}\n",
            "{'loss': 3.4063, 'learning_rate': 0.00029352818371607516, 'epoch': 0.41}\n",
            "{'loss': 3.2615, 'learning_rate': 0.00029248434237995823, 'epoch': 0.42}\n",
            " 42% 1000/2400 [07:49<08:01,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-850] due to args.save_total_limit\n",
            "{'loss': 3.2743, 'learning_rate': 0.00029144050104384136, 'epoch': 0.42}\n",
            "{'loss': 3.1708, 'learning_rate': 0.00029039665970772443, 'epoch': 0.42}\n",
            "{'loss': 3.6156, 'learning_rate': 0.00028935281837160756, 'epoch': 0.42}\n",
            "{'loss': 3.4277, 'learning_rate': 0.00028851774530271404, 'epoch': 0.42}\n",
            "{'loss': 3.652, 'learning_rate': 0.00028747390396659706, 'epoch': 0.43}\n",
            "{'loss': 3.2917, 'learning_rate': 0.0002864300626304802, 'epoch': 0.43}\n",
            "{'loss': 3.4473, 'learning_rate': 0.00028538622129436325, 'epoch': 0.43}\n",
            "{'loss': 3.4318, 'learning_rate': 0.0002843423799582464, 'epoch': 0.43}\n",
            "{'loss': 3.3387, 'learning_rate': 0.00028329853862212945, 'epoch': 0.44}\n",
            "{'loss': 3.3056, 'learning_rate': 0.0002822546972860126, 'epoch': 0.44}\n",
            " 44% 1050/2400 [08:06<07:45,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1050\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 3.4011, 'learning_rate': 0.0002812108559498956, 'epoch': 0.44}\n",
            "{'loss': 3.2231, 'learning_rate': 0.00028016701461377867, 'epoch': 0.44}\n",
            "{'loss': 3.0706, 'learning_rate': 0.0002791231732776618, 'epoch': 0.44}\n",
            "{'loss': 3.5847, 'learning_rate': 0.00027807933194154487, 'epoch': 0.45}\n",
            "{'loss': 3.496, 'learning_rate': 0.000277035490605428, 'epoch': 0.45}\n",
            "{'loss': 3.626, 'learning_rate': 0.00027599164926931107, 'epoch': 0.45}\n",
            "{'loss': 3.4463, 'learning_rate': 0.0002749478079331942, 'epoch': 0.45}\n",
            "{'loss': 3.4082, 'learning_rate': 0.0002739039665970772, 'epoch': 0.45}\n",
            "{'loss': 3.4342, 'learning_rate': 0.00027286012526096034, 'epoch': 0.46}\n",
            "{'loss': 3.582, 'learning_rate': 0.0002718162839248434, 'epoch': 0.46}\n",
            " 46% 1100/2400 [08:23<07:26,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-950] due to args.save_total_limit\n",
            "{'loss': 3.5033, 'learning_rate': 0.00027077244258872653, 'epoch': 0.46}\n",
            "{'loss': 3.2455, 'learning_rate': 0.0002697286012526096, 'epoch': 0.46}\n",
            "{'loss': 3.2867, 'learning_rate': 0.00026868475991649273, 'epoch': 0.46}\n",
            "{'loss': 3.0725, 'learning_rate': 0.0002676409185803758, 'epoch': 0.47}\n",
            "{'loss': 3.315, 'learning_rate': 0.0002665970772442589, 'epoch': 0.47}\n",
            "{'loss': 3.4739, 'learning_rate': 0.00026555323590814195, 'epoch': 0.47}\n",
            "{'loss': 3.3701, 'learning_rate': 0.0002645093945720251, 'epoch': 0.47}\n",
            "{'loss': 3.3433, 'learning_rate': 0.00026346555323590815, 'epoch': 0.47}\n",
            "{'loss': 3.4901, 'learning_rate': 0.0002624217118997913, 'epoch': 0.48}\n",
            "{'loss': 3.3654, 'learning_rate': 0.00026137787056367435, 'epoch': 0.48}\n",
            " 48% 1150/2400 [08:41<07:10,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 3.4279, 'learning_rate': 0.00026033402922755736, 'epoch': 0.48}\n",
            "{'loss': 3.7472, 'learning_rate': 0.0002592901878914405, 'epoch': 0.48}\n",
            "{'loss': 3.5328, 'learning_rate': 0.00025824634655532356, 'epoch': 0.49}\n",
            "{'loss': 3.4471, 'learning_rate': 0.0002572025052192067, 'epoch': 0.49}\n",
            "{'loss': 3.4046, 'learning_rate': 0.00025615866388308976, 'epoch': 0.49}\n",
            "{'loss': 3.3558, 'learning_rate': 0.0002551148225469729, 'epoch': 0.49}\n",
            "{'loss': 3.5446, 'learning_rate': 0.00025407098121085596, 'epoch': 0.49}\n",
            "{'loss': 3.386, 'learning_rate': 0.00025302713987473903, 'epoch': 0.5}\n",
            "{'loss': 3.5735, 'learning_rate': 0.0002519832985386221, 'epoch': 0.5}\n",
            "{'loss': 3.2141, 'learning_rate': 0.00025093945720250523, 'epoch': 0.5}\n",
            " 50% 1200/2400 [08:58<06:53,  2.90it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction_id, input, token_type_ids, output, instruction. If instruction_id, input, token_type_ids, output, instruction are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:37,  3.94it/s]\u001b[A\n",
            "  2% 3/150 [00:01<00:52,  2.78it/s]\u001b[A\n",
            "  3% 4/150 [00:01<01:00,  2.41it/s]\u001b[A\n",
            "  3% 5/150 [00:02<01:04,  2.24it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:07,  2.14it/s]\u001b[A\n",
            "  5% 7/150 [00:03<01:08,  2.08it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:09,  2.05it/s]\u001b[A\n",
            "  6% 9/150 [00:04<01:09,  2.02it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:09,  2.01it/s]\u001b[A\n",
            "  7% 11/150 [00:05<01:09,  2.00it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:09,  1.99it/s]\u001b[A\n",
            "  9% 13/150 [00:06<01:09,  1.98it/s]\u001b[A\n",
            "  9% 14/150 [00:06<01:08,  1.98it/s]\u001b[A\n",
            " 10% 15/150 [00:07<01:08,  1.98it/s]\u001b[A\n",
            " 11% 16/150 [00:07<01:07,  1.97it/s]\u001b[A\n",
            " 11% 17/150 [00:08<01:07,  1.97it/s]\u001b[A\n",
            " 12% 18/150 [00:08<01:06,  1.97it/s]\u001b[A\n",
            " 13% 19/150 [00:09<01:06,  1.97it/s]\u001b[A\n",
            " 13% 20/150 [00:09<01:05,  1.97it/s]\u001b[A\n",
            " 14% 21/150 [00:10<01:05,  1.97it/s]\u001b[A\n",
            " 15% 22/150 [00:10<01:04,  1.97it/s]\u001b[A\n",
            " 15% 23/150 [00:11<01:04,  1.97it/s]\u001b[A\n",
            " 16% 24/150 [00:11<01:03,  1.97it/s]\u001b[A\n",
            " 17% 25/150 [00:12<01:03,  1.97it/s]\u001b[A\n",
            " 17% 26/150 [00:12<01:02,  1.97it/s]\u001b[A\n",
            " 18% 27/150 [00:13<01:02,  1.97it/s]\u001b[A\n",
            " 19% 28/150 [00:13<01:01,  1.97it/s]\u001b[A\n",
            " 19% 29/150 [00:14<01:01,  1.97it/s]\u001b[A\n",
            " 20% 30/150 [00:14<01:00,  1.97it/s]\u001b[A\n",
            " 21% 31/150 [00:15<01:00,  1.97it/s]\u001b[A\n",
            " 21% 32/150 [00:15<00:59,  1.97it/s]\u001b[A\n",
            " 22% 33/150 [00:16<00:59,  1.97it/s]\u001b[A\n",
            " 23% 34/150 [00:16<00:58,  1.97it/s]\u001b[A\n",
            " 23% 35/150 [00:17<00:58,  1.97it/s]\u001b[A\n",
            " 24% 36/150 [00:17<00:57,  1.97it/s]\u001b[A\n",
            " 25% 37/150 [00:18<00:57,  1.97it/s]\u001b[A\n",
            " 25% 38/150 [00:18<00:56,  1.97it/s]\u001b[A\n",
            " 26% 39/150 [00:19<00:56,  1.97it/s]\u001b[A\n",
            " 27% 40/150 [00:19<00:55,  1.97it/s]\u001b[A\n",
            " 27% 41/150 [00:20<00:55,  1.97it/s]\u001b[A\n",
            " 28% 42/150 [00:20<00:54,  1.97it/s]\u001b[A\n",
            " 29% 43/150 [00:21<00:54,  1.97it/s]\u001b[A\n",
            " 29% 44/150 [00:21<00:53,  1.97it/s]\u001b[A\n",
            " 30% 45/150 [00:22<00:53,  1.97it/s]\u001b[A\n",
            " 31% 46/150 [00:22<00:52,  1.97it/s]\u001b[A\n",
            " 31% 47/150 [00:23<00:52,  1.97it/s]\u001b[A\n",
            " 32% 48/150 [00:23<00:51,  1.97it/s]\u001b[A\n",
            " 33% 49/150 [00:24<00:51,  1.97it/s]\u001b[A\n",
            " 33% 50/150 [00:24<00:50,  1.97it/s]\u001b[A\n",
            " 34% 51/150 [00:25<00:50,  1.97it/s]\u001b[A\n",
            " 35% 52/150 [00:25<00:49,  1.97it/s]\u001b[A\n",
            " 35% 53/150 [00:26<00:49,  1.97it/s]\u001b[A\n",
            " 36% 54/150 [00:26<00:48,  1.97it/s]\u001b[A\n",
            " 37% 55/150 [00:27<00:48,  1.97it/s]\u001b[A\n",
            " 37% 56/150 [00:27<00:47,  1.97it/s]\u001b[A\n",
            " 38% 57/150 [00:28<00:47,  1.97it/s]\u001b[A\n",
            " 39% 58/150 [00:28<00:46,  1.97it/s]\u001b[A\n",
            " 39% 59/150 [00:29<00:46,  1.97it/s]\u001b[A\n",
            " 40% 60/150 [00:29<00:45,  1.97it/s]\u001b[A\n",
            " 41% 61/150 [00:30<00:45,  1.97it/s]\u001b[A\n",
            " 41% 62/150 [00:30<00:44,  1.97it/s]\u001b[A\n",
            " 42% 63/150 [00:31<00:44,  1.97it/s]\u001b[A\n",
            " 43% 64/150 [00:31<00:43,  1.97it/s]\u001b[A\n",
            " 43% 65/150 [00:32<00:43,  1.97it/s]\u001b[A\n",
            " 44% 66/150 [00:32<00:42,  1.97it/s]\u001b[A\n",
            " 45% 67/150 [00:33<00:42,  1.97it/s]\u001b[A\n",
            " 45% 68/150 [00:33<00:41,  1.97it/s]\u001b[A\n",
            " 46% 69/150 [00:34<00:41,  1.97it/s]\u001b[A\n",
            " 47% 70/150 [00:34<00:40,  1.97it/s]\u001b[A\n",
            " 47% 71/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 48% 72/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 73/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 74/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 50% 75/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 51% 76/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 51% 77/150 [00:38<00:36,  1.97it/s]\u001b[A\n",
            " 52% 78/150 [00:39<00:36,  1.97it/s]\u001b[A\n",
            " 53% 79/150 [00:39<00:35,  1.97it/s]\u001b[A\n",
            " 53% 80/150 [00:40<00:35,  1.97it/s]\u001b[A\n",
            " 54% 81/150 [00:40<00:35,  1.97it/s]\u001b[A\n",
            " 55% 82/150 [00:41<00:34,  1.97it/s]\u001b[A\n",
            " 55% 83/150 [00:41<00:34,  1.97it/s]\u001b[A\n",
            " 56% 84/150 [00:42<00:33,  1.97it/s]\u001b[A\n",
            " 57% 85/150 [00:42<00:32,  1.97it/s]\u001b[A\n",
            " 57% 86/150 [00:43<00:32,  1.97it/s]\u001b[A\n",
            " 58% 87/150 [00:43<00:31,  1.97it/s]\u001b[A\n",
            " 59% 88/150 [00:44<00:31,  1.97it/s]\u001b[A\n",
            " 59% 89/150 [00:44<00:30,  1.97it/s]\u001b[A\n",
            " 60% 90/150 [00:45<00:30,  1.97it/s]\u001b[A\n",
            " 61% 91/150 [00:45<00:29,  1.97it/s]\u001b[A\n",
            " 61% 92/150 [00:46<00:29,  1.97it/s]\u001b[A\n",
            " 62% 93/150 [00:46<00:28,  1.97it/s]\u001b[A\n",
            " 63% 94/150 [00:47<00:28,  1.97it/s]\u001b[A\n",
            " 63% 95/150 [00:47<00:27,  1.97it/s]\u001b[A\n",
            " 64% 96/150 [00:48<00:27,  1.97it/s]\u001b[A\n",
            " 65% 97/150 [00:48<00:26,  1.97it/s]\u001b[A\n",
            " 65% 98/150 [00:49<00:26,  1.97it/s]\u001b[A\n",
            " 66% 99/150 [00:49<00:25,  1.97it/s]\u001b[A\n",
            " 67% 100/150 [00:50<00:25,  1.97it/s]\u001b[A\n",
            " 67% 101/150 [00:50<00:24,  1.97it/s]\u001b[A\n",
            " 68% 102/150 [00:51<00:24,  1.97it/s]\u001b[A\n",
            " 69% 103/150 [00:51<00:23,  1.97it/s]\u001b[A\n",
            " 69% 104/150 [00:52<00:23,  1.97it/s]\u001b[A\n",
            " 70% 105/150 [00:52<00:22,  1.97it/s]\u001b[A\n",
            " 71% 106/150 [00:53<00:22,  1.97it/s]\u001b[A\n",
            " 71% 107/150 [00:53<00:21,  1.97it/s]\u001b[A\n",
            " 72% 108/150 [00:54<00:21,  1.97it/s]\u001b[A\n",
            " 73% 109/150 [00:54<00:20,  1.97it/s]\u001b[A\n",
            " 73% 110/150 [00:55<00:20,  1.97it/s]\u001b[A\n",
            " 74% 111/150 [00:55<00:19,  1.97it/s]\u001b[A\n",
            " 75% 112/150 [00:56<00:19,  1.97it/s]\u001b[A\n",
            " 75% 113/150 [00:56<00:18,  1.97it/s]\u001b[A\n",
            " 76% 114/150 [00:57<00:18,  1.97it/s]\u001b[A\n",
            " 77% 115/150 [00:57<00:17,  1.97it/s]\u001b[A\n",
            " 77% 116/150 [00:58<00:17,  1.97it/s]\u001b[A\n",
            " 78% 117/150 [00:58<00:16,  1.97it/s]\u001b[A\n",
            " 79% 118/150 [00:59<00:16,  1.97it/s]\u001b[A\n",
            " 79% 119/150 [00:59<00:15,  1.97it/s]\u001b[A\n",
            " 80% 120/150 [01:00<00:15,  1.97it/s]\u001b[A\n",
            " 81% 121/150 [01:00<00:14,  1.97it/s]\u001b[A\n",
            " 81% 122/150 [01:01<00:14,  1.97it/s]\u001b[A\n",
            " 82% 123/150 [01:01<00:13,  1.97it/s]\u001b[A\n",
            " 83% 124/150 [01:02<00:13,  1.97it/s]\u001b[A\n",
            " 83% 125/150 [01:02<00:12,  1.97it/s]\u001b[A\n",
            " 84% 126/150 [01:03<00:12,  1.97it/s]\u001b[A\n",
            " 85% 127/150 [01:03<00:11,  1.97it/s]\u001b[A\n",
            " 85% 128/150 [01:04<00:11,  1.97it/s]\u001b[A\n",
            " 86% 129/150 [01:04<00:10,  1.97it/s]\u001b[A\n",
            " 87% 130/150 [01:05<00:10,  1.97it/s]\u001b[A\n",
            " 87% 131/150 [01:05<00:09,  1.97it/s]\u001b[A\n",
            " 88% 132/150 [01:06<00:09,  1.97it/s]\u001b[A\n",
            " 89% 133/150 [01:06<00:08,  1.97it/s]\u001b[A\n",
            " 89% 134/150 [01:07<00:08,  1.97it/s]\u001b[A\n",
            " 90% 135/150 [01:07<00:07,  1.97it/s]\u001b[A\n",
            " 91% 136/150 [01:08<00:07,  1.97it/s]\u001b[A\n",
            " 91% 137/150 [01:08<00:06,  1.97it/s]\u001b[A\n",
            " 92% 138/150 [01:09<00:06,  1.97it/s]\u001b[A\n",
            " 93% 139/150 [01:09<00:05,  1.97it/s]\u001b[A\n",
            " 93% 140/150 [01:10<00:05,  1.97it/s]\u001b[A\n",
            " 94% 141/150 [01:11<00:04,  1.97it/s]\u001b[A\n",
            " 95% 142/150 [01:11<00:04,  1.97it/s]\u001b[A\n",
            " 95% 143/150 [01:12<00:03,  1.97it/s]\u001b[A\n",
            " 96% 144/150 [01:12<00:03,  1.97it/s]\u001b[A\n",
            " 97% 145/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 97% 146/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 98% 147/150 [01:14<00:01,  1.97it/s]\u001b[A\n",
            " 99% 148/150 [01:14<00:01,  1.97it/s]\u001b[A\n",
            " 99% 149/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.341576337814331, 'eval_runtime': 76.0915, 'eval_samples_per_second': 15.77, 'eval_steps_per_second': 1.971, 'epoch': 0.5}\n",
            " 50% 1200/2400 [10:14<06:53,  2.90it/s]\n",
            "100% 150/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1050] due to args.save_total_limit\n",
            "{'loss': 3.2824, 'learning_rate': 0.0002498956158663883, 'epoch': 0.5}\n",
            "{'loss': 3.2833, 'learning_rate': 0.00024885177453027143, 'epoch': 0.5}\n",
            "{'loss': 3.4093, 'learning_rate': 0.0002478079331941545, 'epoch': 0.51}\n",
            "{'loss': 3.3992, 'learning_rate': 0.00024676409185803757, 'epoch': 0.51}\n",
            "{'loss': 3.4454, 'learning_rate': 0.0002457202505219207, 'epoch': 0.51}\n",
            "{'loss': 3.5261, 'learning_rate': 0.00024467640918580377, 'epoch': 0.51}\n",
            "{'loss': 3.5066, 'learning_rate': 0.00024363256784968684, 'epoch': 0.51}\n",
            "{'loss': 3.5453, 'learning_rate': 0.00024258872651356994, 'epoch': 0.52}\n",
            "{'loss': 3.3592, 'learning_rate': 0.00024154488517745304, 'epoch': 0.52}\n",
            "{'loss': 3.4637, 'learning_rate': 0.0002405010438413361, 'epoch': 0.52}\n",
            " 52% 1250/2400 [10:32<06:36,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 3.2889, 'learning_rate': 0.0002394572025052192, 'epoch': 0.52}\n",
            "{'loss': 3.3616, 'learning_rate': 0.0002384133611691023, 'epoch': 0.53}\n",
            "{'loss': 3.2914, 'learning_rate': 0.00023736951983298538, 'epoch': 0.53}\n",
            "{'loss': 3.6024, 'learning_rate': 0.00023632567849686848, 'epoch': 0.53}\n",
            "{'loss': 3.3274, 'learning_rate': 0.00023528183716075158, 'epoch': 0.53}\n",
            "{'loss': 3.1849, 'learning_rate': 0.00023423799582463465, 'epoch': 0.53}\n",
            "{'loss': 3.7718, 'learning_rate': 0.00023319415448851775, 'epoch': 0.54}\n",
            "{'loss': 3.4651, 'learning_rate': 0.00023215031315240085, 'epoch': 0.54}\n",
            "{'loss': 3.306, 'learning_rate': 0.00023110647181628395, 'epoch': 0.54}\n",
            "{'loss': 3.2189, 'learning_rate': 0.00023006263048016702, 'epoch': 0.54}\n",
            " 54% 1300/2400 [10:49<06:21,  2.88it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1150] due to args.save_total_limit\n",
            "{'loss': 3.1125, 'learning_rate': 0.00022901878914405012, 'epoch': 0.54}\n",
            "{'loss': 3.2112, 'learning_rate': 0.0002279749478079332, 'epoch': 0.55}\n",
            "{'loss': 3.5662, 'learning_rate': 0.00022693110647181627, 'epoch': 0.55}\n",
            "{'loss': 3.3179, 'learning_rate': 0.00022588726513569937, 'epoch': 0.55}\n",
            "{'loss': 3.1953, 'learning_rate': 0.00022484342379958247, 'epoch': 0.55}\n",
            "{'loss': 3.3727, 'learning_rate': 0.00022379958246346554, 'epoch': 0.55}\n",
            "{'loss': 3.3425, 'learning_rate': 0.00022275574112734864, 'epoch': 0.56}\n",
            "{'loss': 3.7152, 'learning_rate': 0.00022171189979123174, 'epoch': 0.56}\n",
            "{'loss': 3.3138, 'learning_rate': 0.00022066805845511484, 'epoch': 0.56}\n",
            "{'loss': 3.4606, 'learning_rate': 0.0002196242171189979, 'epoch': 0.56}\n",
            " 56% 1350/2400 [11:06<06:03,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 3.4277, 'learning_rate': 0.000218580375782881, 'epoch': 0.56}\n",
            "{'loss': 2.9765, 'learning_rate': 0.0002175365344467641, 'epoch': 0.57}\n",
            "{'loss': 3.464, 'learning_rate': 0.00021649269311064718, 'epoch': 0.57}\n",
            "{'loss': 3.2658, 'learning_rate': 0.00021544885177453028, 'epoch': 0.57}\n",
            "{'loss': 3.2644, 'learning_rate': 0.00021440501043841338, 'epoch': 0.57}\n",
            "{'loss': 3.4749, 'learning_rate': 0.00021336116910229645, 'epoch': 0.57}\n",
            "{'loss': 3.2168, 'learning_rate': 0.00021231732776617955, 'epoch': 0.58}\n",
            "{'loss': 3.4843, 'learning_rate': 0.00021127348643006265, 'epoch': 0.58}\n",
            "{'loss': 3.2754, 'learning_rate': 0.00021022964509394575, 'epoch': 0.58}\n",
            "{'loss': 3.4183, 'learning_rate': 0.00020918580375782882, 'epoch': 0.58}\n",
            " 58% 1400/2400 [11:24<05:43,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1250] due to args.save_total_limit\n",
            "{'loss': 3.2098, 'learning_rate': 0.00020814196242171192, 'epoch': 0.59}\n",
            "{'loss': 3.2139, 'learning_rate': 0.00020709812108559502, 'epoch': 0.59}\n",
            "{'loss': 3.3754, 'learning_rate': 0.00020605427974947806, 'epoch': 0.59}\n",
            "{'loss': 3.4238, 'learning_rate': 0.00020501043841336116, 'epoch': 0.59}\n",
            "{'loss': 3.4589, 'learning_rate': 0.00020396659707724426, 'epoch': 0.59}\n",
            "{'loss': 3.2591, 'learning_rate': 0.00020292275574112733, 'epoch': 0.6}\n",
            "{'loss': 3.2379, 'learning_rate': 0.00020187891440501043, 'epoch': 0.6}\n",
            "{'loss': 3.5703, 'learning_rate': 0.00020083507306889353, 'epoch': 0.6}\n",
            "{'loss': 3.2625, 'learning_rate': 0.00019979123173277663, 'epoch': 0.6}\n",
            "{'loss': 3.2115, 'learning_rate': 0.0001987473903966597, 'epoch': 0.6}\n",
            " 60% 1450/2400 [11:41<05:27,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1450\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 3.2209, 'learning_rate': 0.0001977035490605428, 'epoch': 0.61}\n",
            "{'loss': 3.3223, 'learning_rate': 0.0001966597077244259, 'epoch': 0.61}\n",
            "{'loss': 3.2923, 'learning_rate': 0.00019561586638830897, 'epoch': 0.61}\n",
            "{'loss': 3.4745, 'learning_rate': 0.00019457202505219207, 'epoch': 0.61}\n",
            "{'loss': 3.4108, 'learning_rate': 0.00019352818371607517, 'epoch': 0.61}\n",
            "{'loss': 3.2371, 'learning_rate': 0.00019248434237995824, 'epoch': 0.62}\n",
            "{'loss': 3.5082, 'learning_rate': 0.00019144050104384134, 'epoch': 0.62}\n",
            "{'loss': 3.2599, 'learning_rate': 0.00019039665970772444, 'epoch': 0.62}\n",
            "{'loss': 3.1808, 'learning_rate': 0.00018935281837160754, 'epoch': 0.62}\n",
            "{'loss': 3.452, 'learning_rate': 0.0001883089770354906, 'epoch': 0.62}\n",
            " 62% 1500/2400 [11:59<05:09,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1350] due to args.save_total_limit\n",
            "{'loss': 3.389, 'learning_rate': 0.0001872651356993737, 'epoch': 0.63}\n",
            "{'loss': 3.107, 'learning_rate': 0.0001862212943632568, 'epoch': 0.63}\n",
            "{'loss': 3.5708, 'learning_rate': 0.00018517745302713988, 'epoch': 0.63}\n",
            "{'loss': 3.6059, 'learning_rate': 0.00018413361169102295, 'epoch': 0.63}\n",
            "{'loss': 3.3686, 'learning_rate': 0.00018308977035490605, 'epoch': 0.64}\n",
            "{'loss': 3.1499, 'learning_rate': 0.00018204592901878913, 'epoch': 0.64}\n",
            "{'loss': 3.5046, 'learning_rate': 0.00018100208768267223, 'epoch': 0.64}\n",
            "{'loss': 3.8107, 'learning_rate': 0.00017995824634655532, 'epoch': 0.64}\n",
            "{'loss': 3.3062, 'learning_rate': 0.00017891440501043842, 'epoch': 0.64}\n",
            "{'loss': 3.1076, 'learning_rate': 0.0001778705636743215, 'epoch': 0.65}\n",
            " 65% 1550/2400 [12:16<04:56,  2.86it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1550\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 3.2802, 'learning_rate': 0.0001768267223382046, 'epoch': 0.65}\n",
            "{'loss': 3.1468, 'learning_rate': 0.0001757828810020877, 'epoch': 0.65}\n",
            "{'loss': 3.3414, 'learning_rate': 0.00017473903966597077, 'epoch': 0.65}\n",
            "{'loss': 3.368, 'learning_rate': 0.00017369519832985387, 'epoch': 0.65}\n",
            "{'loss': 3.3179, 'learning_rate': 0.00017265135699373696, 'epoch': 0.66}\n",
            "{'loss': 3.5274, 'learning_rate': 0.00017160751565762004, 'epoch': 0.66}\n",
            "{'loss': 3.6268, 'learning_rate': 0.00017056367432150314, 'epoch': 0.66}\n",
            "{'loss': 3.5478, 'learning_rate': 0.00016951983298538624, 'epoch': 0.66}\n",
            "{'loss': 3.3601, 'learning_rate': 0.00016847599164926933, 'epoch': 0.66}\n",
            "{'loss': 3.4124, 'learning_rate': 0.0001674321503131524, 'epoch': 0.67}\n",
            " 67% 1600/2400 [12:34<04:35,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1450] due to args.save_total_limit\n",
            "{'loss': 3.8647, 'learning_rate': 0.0001663883089770355, 'epoch': 0.67}\n",
            "{'loss': 3.3352, 'learning_rate': 0.0001653444676409186, 'epoch': 0.67}\n",
            "{'loss': 3.5144, 'learning_rate': 0.00016430062630480168, 'epoch': 0.67}\n",
            "{'loss': 3.3297, 'learning_rate': 0.00016325678496868478, 'epoch': 0.68}\n",
            "{'loss': 3.0743, 'learning_rate': 0.00016221294363256785, 'epoch': 0.68}\n",
            "{'loss': 3.0299, 'learning_rate': 0.00016116910229645092, 'epoch': 0.68}\n",
            "{'loss': 3.2583, 'learning_rate': 0.00016012526096033402, 'epoch': 0.68}\n",
            "{'loss': 3.5501, 'learning_rate': 0.00015908141962421712, 'epoch': 0.68}\n",
            "{'loss': 3.3095, 'learning_rate': 0.00015803757828810022, 'epoch': 0.69}\n",
            "{'loss': 3.235, 'learning_rate': 0.0001569937369519833, 'epoch': 0.69}\n",
            " 69% 1650/2400 [12:51<04:18,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1650\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 3.3544, 'learning_rate': 0.0001559498956158664, 'epoch': 0.69}\n",
            "{'loss': 3.4818, 'learning_rate': 0.0001549060542797495, 'epoch': 0.69}\n",
            "{'loss': 3.3628, 'learning_rate': 0.00015386221294363256, 'epoch': 0.69}\n",
            "{'loss': 3.0891, 'learning_rate': 0.00015281837160751566, 'epoch': 0.7}\n",
            "{'loss': 3.2576, 'learning_rate': 0.00015177453027139876, 'epoch': 0.7}\n",
            "{'loss': 3.2557, 'learning_rate': 0.00015073068893528183, 'epoch': 0.7}\n",
            "{'loss': 3.2354, 'learning_rate': 0.00014968684759916493, 'epoch': 0.7}\n",
            "{'loss': 3.5991, 'learning_rate': 0.00014864300626304803, 'epoch': 0.7}\n",
            "{'loss': 3.5384, 'learning_rate': 0.00014759916492693113, 'epoch': 0.71}\n",
            "{'loss': 3.4458, 'learning_rate': 0.0001465553235908142, 'epoch': 0.71}\n",
            " 71% 1700/2400 [13:08<03:59,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1550] due to args.save_total_limit\n",
            "{'loss': 3.1745, 'learning_rate': 0.0001455114822546973, 'epoch': 0.71}\n",
            "{'loss': 3.4954, 'learning_rate': 0.0001444676409185804, 'epoch': 0.71}\n",
            "{'loss': 3.0702, 'learning_rate': 0.00014342379958246347, 'epoch': 0.71}\n",
            "{'loss': 3.15, 'learning_rate': 0.00014237995824634657, 'epoch': 0.72}\n",
            "{'loss': 3.3138, 'learning_rate': 0.00014133611691022967, 'epoch': 0.72}\n",
            "{'loss': 3.2309, 'learning_rate': 0.00014029227557411271, 'epoch': 0.72}\n",
            "{'loss': 3.1014, 'learning_rate': 0.00013924843423799581, 'epoch': 0.72}\n",
            "{'loss': 3.0883, 'learning_rate': 0.0001382045929018789, 'epoch': 0.72}\n",
            "{'loss': 3.167, 'learning_rate': 0.000137160751565762, 'epoch': 0.73}\n",
            "{'loss': 3.3237, 'learning_rate': 0.00013611691022964508, 'epoch': 0.73}\n",
            " 73% 1750/2400 [13:26<03:43,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1600] due to args.save_total_limit\n",
            "{'loss': 3.6919, 'learning_rate': 0.00013507306889352818, 'epoch': 0.73}\n",
            "{'loss': 3.3712, 'learning_rate': 0.00013402922755741128, 'epoch': 0.73}\n",
            "{'loss': 3.6282, 'learning_rate': 0.00013298538622129435, 'epoch': 0.74}\n",
            "{'loss': 3.3491, 'learning_rate': 0.00013194154488517745, 'epoch': 0.74}\n",
            "{'loss': 3.3796, 'learning_rate': 0.00013089770354906055, 'epoch': 0.74}\n",
            "{'loss': 3.2856, 'learning_rate': 0.00012985386221294363, 'epoch': 0.74}\n",
            "{'loss': 3.6663, 'learning_rate': 0.00012881002087682672, 'epoch': 0.74}\n",
            "{'loss': 3.2585, 'learning_rate': 0.00012776617954070982, 'epoch': 0.75}\n",
            "{'loss': 3.2913, 'learning_rate': 0.0001267223382045929, 'epoch': 0.75}\n",
            "{'loss': 3.4506, 'learning_rate': 0.000125678496868476, 'epoch': 0.75}\n",
            " 75% 1800/2400 [13:43<03:26,  2.91it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction_id, input, token_type_ids, output, instruction. If instruction_id, input, token_type_ids, output, instruction are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:37,  3.95it/s]\u001b[A\n",
            "  2% 3/150 [00:01<00:52,  2.78it/s]\u001b[A\n",
            "  3% 4/150 [00:01<01:00,  2.41it/s]\u001b[A\n",
            "  3% 5/150 [00:02<01:04,  2.24it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:07,  2.14it/s]\u001b[A\n",
            "  5% 7/150 [00:03<01:08,  2.09it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:09,  2.05it/s]\u001b[A\n",
            "  6% 9/150 [00:04<01:09,  2.02it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:09,  2.01it/s]\u001b[A\n",
            "  7% 11/150 [00:05<01:09,  2.00it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:09,  1.99it/s]\u001b[A\n",
            "  9% 13/150 [00:06<01:09,  1.99it/s]\u001b[A\n",
            "  9% 14/150 [00:06<01:08,  1.98it/s]\u001b[A\n",
            " 10% 15/150 [00:07<01:08,  1.98it/s]\u001b[A\n",
            " 11% 16/150 [00:07<01:07,  1.98it/s]\u001b[A\n",
            " 11% 17/150 [00:08<01:07,  1.97it/s]\u001b[A\n",
            " 12% 18/150 [00:08<01:06,  1.97it/s]\u001b[A\n",
            " 13% 19/150 [00:09<01:06,  1.97it/s]\u001b[A\n",
            " 13% 20/150 [00:09<01:05,  1.97it/s]\u001b[A\n",
            " 14% 21/150 [00:10<01:05,  1.97it/s]\u001b[A\n",
            " 15% 22/150 [00:10<01:04,  1.97it/s]\u001b[A\n",
            " 15% 23/150 [00:11<01:04,  1.97it/s]\u001b[A\n",
            " 16% 24/150 [00:11<01:03,  1.97it/s]\u001b[A\n",
            " 17% 25/150 [00:12<01:03,  1.97it/s]\u001b[A\n",
            " 17% 26/150 [00:12<01:02,  1.97it/s]\u001b[A\n",
            " 18% 27/150 [00:13<01:02,  1.97it/s]\u001b[A\n",
            " 19% 28/150 [00:13<01:01,  1.97it/s]\u001b[A\n",
            " 19% 29/150 [00:14<01:01,  1.97it/s]\u001b[A\n",
            " 20% 30/150 [00:14<01:00,  1.97it/s]\u001b[A\n",
            " 21% 31/150 [00:15<01:00,  1.97it/s]\u001b[A\n",
            " 21% 32/150 [00:15<00:59,  1.97it/s]\u001b[A\n",
            " 22% 33/150 [00:16<00:59,  1.97it/s]\u001b[A\n",
            " 23% 34/150 [00:16<00:58,  1.97it/s]\u001b[A\n",
            " 23% 35/150 [00:17<00:58,  1.97it/s]\u001b[A\n",
            " 24% 36/150 [00:17<00:57,  1.97it/s]\u001b[A\n",
            " 25% 37/150 [00:18<00:57,  1.97it/s]\u001b[A\n",
            " 25% 38/150 [00:18<00:56,  1.97it/s]\u001b[A\n",
            " 26% 39/150 [00:19<00:56,  1.97it/s]\u001b[A\n",
            " 27% 40/150 [00:19<00:55,  1.97it/s]\u001b[A\n",
            " 27% 41/150 [00:20<00:55,  1.97it/s]\u001b[A\n",
            " 28% 42/150 [00:20<00:54,  1.97it/s]\u001b[A\n",
            " 29% 43/150 [00:21<00:54,  1.97it/s]\u001b[A\n",
            " 29% 44/150 [00:21<00:53,  1.97it/s]\u001b[A\n",
            " 30% 45/150 [00:22<00:53,  1.97it/s]\u001b[A\n",
            " 31% 46/150 [00:22<00:52,  1.97it/s]\u001b[A\n",
            " 31% 47/150 [00:23<00:52,  1.97it/s]\u001b[A\n",
            " 32% 48/150 [00:23<00:51,  1.97it/s]\u001b[A\n",
            " 33% 49/150 [00:24<00:51,  1.97it/s]\u001b[A\n",
            " 33% 50/150 [00:24<00:50,  1.97it/s]\u001b[A\n",
            " 34% 51/150 [00:25<00:50,  1.97it/s]\u001b[A\n",
            " 35% 52/150 [00:25<00:49,  1.97it/s]\u001b[A\n",
            " 35% 53/150 [00:26<00:49,  1.97it/s]\u001b[A\n",
            " 36% 54/150 [00:26<00:48,  1.97it/s]\u001b[A\n",
            " 37% 55/150 [00:27<00:48,  1.97it/s]\u001b[A\n",
            " 37% 56/150 [00:27<00:47,  1.97it/s]\u001b[A\n",
            " 38% 57/150 [00:28<00:47,  1.97it/s]\u001b[A\n",
            " 39% 58/150 [00:28<00:46,  1.97it/s]\u001b[A\n",
            " 39% 59/150 [00:29<00:46,  1.97it/s]\u001b[A\n",
            " 40% 60/150 [00:29<00:45,  1.97it/s]\u001b[A\n",
            " 41% 61/150 [00:30<00:45,  1.97it/s]\u001b[A\n",
            " 41% 62/150 [00:30<00:44,  1.97it/s]\u001b[A\n",
            " 42% 63/150 [00:31<00:44,  1.97it/s]\u001b[A\n",
            " 43% 64/150 [00:31<00:43,  1.97it/s]\u001b[A\n",
            " 43% 65/150 [00:32<00:43,  1.97it/s]\u001b[A\n",
            " 44% 66/150 [00:32<00:42,  1.97it/s]\u001b[A\n",
            " 45% 67/150 [00:33<00:42,  1.97it/s]\u001b[A\n",
            " 45% 68/150 [00:34<00:41,  1.97it/s]\u001b[A\n",
            " 46% 69/150 [00:34<00:41,  1.97it/s]\u001b[A\n",
            " 47% 70/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 47% 71/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 48% 72/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 73/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 74/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 50% 75/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 51% 76/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 51% 77/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 52% 78/150 [00:39<00:36,  1.97it/s]\u001b[A\n",
            " 53% 79/150 [00:39<00:35,  1.97it/s]\u001b[A\n",
            " 53% 80/150 [00:40<00:35,  1.97it/s]\u001b[A\n",
            " 54% 81/150 [00:40<00:34,  1.97it/s]\u001b[A\n",
            " 55% 82/150 [00:41<00:34,  1.97it/s]\u001b[A\n",
            " 55% 83/150 [00:41<00:33,  1.97it/s]\u001b[A\n",
            " 56% 84/150 [00:42<00:33,  1.97it/s]\u001b[A\n",
            " 57% 85/150 [00:42<00:32,  1.97it/s]\u001b[A\n",
            " 57% 86/150 [00:43<00:32,  1.97it/s]\u001b[A\n",
            " 58% 87/150 [00:43<00:32,  1.97it/s]\u001b[A\n",
            " 59% 88/150 [00:44<00:31,  1.97it/s]\u001b[A\n",
            " 59% 89/150 [00:44<00:31,  1.97it/s]\u001b[A\n",
            " 60% 90/150 [00:45<00:30,  1.97it/s]\u001b[A\n",
            " 61% 91/150 [00:45<00:29,  1.97it/s]\u001b[A\n",
            " 61% 92/150 [00:46<00:29,  1.97it/s]\u001b[A\n",
            " 62% 93/150 [00:46<00:28,  1.97it/s]\u001b[A\n",
            " 63% 94/150 [00:47<00:28,  1.97it/s]\u001b[A\n",
            " 63% 95/150 [00:47<00:27,  1.97it/s]\u001b[A\n",
            " 64% 96/150 [00:48<00:27,  1.97it/s]\u001b[A\n",
            " 65% 97/150 [00:48<00:26,  1.97it/s]\u001b[A\n",
            " 65% 98/150 [00:49<00:26,  1.97it/s]\u001b[A\n",
            " 66% 99/150 [00:49<00:25,  1.97it/s]\u001b[A\n",
            " 67% 100/150 [00:50<00:25,  1.97it/s]\u001b[A\n",
            " 67% 101/150 [00:50<00:24,  1.97it/s]\u001b[A\n",
            " 68% 102/150 [00:51<00:24,  1.97it/s]\u001b[A\n",
            " 69% 103/150 [00:51<00:23,  1.97it/s]\u001b[A\n",
            " 69% 104/150 [00:52<00:23,  1.97it/s]\u001b[A\n",
            " 70% 105/150 [00:52<00:22,  1.97it/s]\u001b[A\n",
            " 71% 106/150 [00:53<00:22,  1.97it/s]\u001b[A\n",
            " 71% 107/150 [00:53<00:21,  1.97it/s]\u001b[A\n",
            " 72% 108/150 [00:54<00:21,  1.97it/s]\u001b[A\n",
            " 73% 109/150 [00:54<00:20,  1.97it/s]\u001b[A\n",
            " 73% 110/150 [00:55<00:20,  1.97it/s]\u001b[A\n",
            " 74% 111/150 [00:55<00:19,  1.97it/s]\u001b[A\n",
            " 75% 112/150 [00:56<00:19,  1.97it/s]\u001b[A\n",
            " 75% 113/150 [00:56<00:18,  1.97it/s]\u001b[A\n",
            " 76% 114/150 [00:57<00:18,  1.97it/s]\u001b[A\n",
            " 77% 115/150 [00:57<00:17,  1.97it/s]\u001b[A\n",
            " 77% 116/150 [00:58<00:17,  1.97it/s]\u001b[A\n",
            " 78% 117/150 [00:58<00:16,  1.97it/s]\u001b[A\n",
            " 79% 118/150 [00:59<00:16,  1.97it/s]\u001b[A\n",
            " 79% 119/150 [00:59<00:15,  1.97it/s]\u001b[A\n",
            " 80% 120/150 [01:00<00:15,  1.97it/s]\u001b[A\n",
            " 81% 121/150 [01:00<00:14,  1.97it/s]\u001b[A\n",
            " 81% 122/150 [01:01<00:14,  1.97it/s]\u001b[A\n",
            " 82% 123/150 [01:01<00:13,  1.97it/s]\u001b[A\n",
            " 83% 124/150 [01:02<00:13,  1.97it/s]\u001b[A\n",
            " 83% 125/150 [01:02<00:12,  1.97it/s]\u001b[A\n",
            " 84% 126/150 [01:03<00:12,  1.97it/s]\u001b[A\n",
            " 85% 127/150 [01:03<00:11,  1.97it/s]\u001b[A\n",
            " 85% 128/150 [01:04<00:11,  1.97it/s]\u001b[A\n",
            " 86% 129/150 [01:04<00:10,  1.97it/s]\u001b[A\n",
            " 87% 130/150 [01:05<00:10,  1.97it/s]\u001b[A\n",
            " 87% 131/150 [01:05<00:09,  1.97it/s]\u001b[A\n",
            " 88% 132/150 [01:06<00:09,  1.97it/s]\u001b[A\n",
            " 89% 133/150 [01:06<00:08,  1.97it/s]\u001b[A\n",
            " 89% 134/150 [01:07<00:08,  1.97it/s]\u001b[A\n",
            " 90% 135/150 [01:07<00:07,  1.97it/s]\u001b[A\n",
            " 91% 136/150 [01:08<00:07,  1.97it/s]\u001b[A\n",
            " 91% 137/150 [01:09<00:06,  1.97it/s]\u001b[A\n",
            " 92% 138/150 [01:09<00:06,  1.97it/s]\u001b[A\n",
            " 93% 139/150 [01:10<00:05,  1.97it/s]\u001b[A\n",
            " 93% 140/150 [01:10<00:05,  1.97it/s]\u001b[A\n",
            " 94% 141/150 [01:11<00:04,  1.97it/s]\u001b[A\n",
            " 95% 142/150 [01:11<00:04,  1.96it/s]\u001b[A\n",
            " 95% 143/150 [01:12<00:03,  1.96it/s]\u001b[A\n",
            " 96% 144/150 [01:12<00:03,  1.96it/s]\u001b[A\n",
            " 97% 145/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 97% 146/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 98% 147/150 [01:14<00:01,  1.97it/s]\u001b[A\n",
            " 99% 148/150 [01:14<00:01,  1.97it/s]\u001b[A\n",
            " 99% 149/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.3006973266601562, 'eval_runtime': 76.133, 'eval_samples_per_second': 15.762, 'eval_steps_per_second': 1.97, 'epoch': 0.75}\n",
            " 75% 1800/2400 [14:59<03:26,  2.91it/s]\n",
            "100% 150/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1650] due to args.save_total_limit\n",
            "{'loss': 3.3056, 'learning_rate': 0.0001246346555323591, 'epoch': 0.75}\n",
            "{'loss': 3.3315, 'learning_rate': 0.00012359081419624217, 'epoch': 0.75}\n",
            "{'loss': 3.3877, 'learning_rate': 0.00012254697286012527, 'epoch': 0.76}\n",
            "{'loss': 3.3848, 'learning_rate': 0.00012150313152400835, 'epoch': 0.76}\n",
            "{'loss': 3.1667, 'learning_rate': 0.00012045929018789144, 'epoch': 0.76}\n",
            "{'loss': 3.0961, 'learning_rate': 0.00011941544885177454, 'epoch': 0.76}\n",
            "{'loss': 3.1452, 'learning_rate': 0.00011837160751565762, 'epoch': 0.76}\n",
            "{'loss': 3.2728, 'learning_rate': 0.00011732776617954072, 'epoch': 0.77}\n",
            "{'loss': 3.5429, 'learning_rate': 0.0001162839248434238, 'epoch': 0.77}\n",
            "{'loss': 3.4231, 'learning_rate': 0.00011524008350730689, 'epoch': 0.77}\n",
            " 77% 1850/2400 [15:16<03:08,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1850\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1700] due to args.save_total_limit\n",
            "{'loss': 3.3243, 'learning_rate': 0.00011419624217118999, 'epoch': 0.77}\n",
            "{'loss': 3.2919, 'learning_rate': 0.00011315240083507306, 'epoch': 0.78}\n",
            "{'loss': 3.157, 'learning_rate': 0.00011210855949895616, 'epoch': 0.78}\n",
            "{'loss': 3.2161, 'learning_rate': 0.00011106471816283925, 'epoch': 0.78}\n",
            "{'loss': 3.1915, 'learning_rate': 0.00011002087682672233, 'epoch': 0.78}\n",
            "{'loss': 3.1795, 'learning_rate': 0.00010897703549060543, 'epoch': 0.78}\n",
            "{'loss': 3.3446, 'learning_rate': 0.00010793319415448852, 'epoch': 0.79}\n",
            "{'loss': 3.3447, 'learning_rate': 0.00010688935281837162, 'epoch': 0.79}\n",
            "{'loss': 3.4245, 'learning_rate': 0.0001058455114822547, 'epoch': 0.79}\n",
            "{'loss': 3.3586, 'learning_rate': 0.00010480167014613779, 'epoch': 0.79}\n",
            " 79% 1900/2400 [15:34<02:51,  2.92it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1750] due to args.save_total_limit\n",
            "{'loss': 3.3288, 'learning_rate': 0.00010375782881002089, 'epoch': 0.79}\n",
            "{'loss': 3.2311, 'learning_rate': 0.00010271398747390396, 'epoch': 0.8}\n",
            "{'loss': 3.2841, 'learning_rate': 0.00010167014613778706, 'epoch': 0.8}\n",
            "{'loss': 2.8104, 'learning_rate': 0.00010062630480167015, 'epoch': 0.8}\n",
            "{'loss': 3.6566, 'learning_rate': 9.958246346555323e-05, 'epoch': 0.8}\n",
            "{'loss': 3.204, 'learning_rate': 9.853862212943633e-05, 'epoch': 0.8}\n",
            "{'loss': 3.5375, 'learning_rate': 9.749478079331942e-05, 'epoch': 0.81}\n",
            "{'loss': 3.1761, 'learning_rate': 9.645093945720251e-05, 'epoch': 0.81}\n",
            "{'loss': 3.0506, 'learning_rate': 9.54070981210856e-05, 'epoch': 0.81}\n",
            "{'loss': 3.2672, 'learning_rate': 9.436325678496869e-05, 'epoch': 0.81}\n",
            " 81% 1950/2400 [15:51<02:34,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-1950\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1800] due to args.save_total_limit\n",
            "{'loss': 3.3142, 'learning_rate': 9.331941544885179e-05, 'epoch': 0.81}\n",
            "{'loss': 3.3392, 'learning_rate': 9.227557411273486e-05, 'epoch': 0.82}\n",
            "{'loss': 3.156, 'learning_rate': 9.123173277661796e-05, 'epoch': 0.82}\n",
            "{'loss': 3.3501, 'learning_rate': 9.018789144050104e-05, 'epoch': 0.82}\n",
            "{'loss': 3.3045, 'learning_rate': 8.914405010438413e-05, 'epoch': 0.82}\n",
            "{'loss': 3.2102, 'learning_rate': 8.810020876826723e-05, 'epoch': 0.82}\n",
            "{'loss': 3.3299, 'learning_rate': 8.705636743215031e-05, 'epoch': 0.83}\n",
            "{'loss': 3.1402, 'learning_rate': 8.601252609603341e-05, 'epoch': 0.83}\n",
            "{'loss': 3.5053, 'learning_rate': 8.49686847599165e-05, 'epoch': 0.83}\n",
            "{'loss': 3.1829, 'learning_rate': 8.392484342379958e-05, 'epoch': 0.83}\n",
            " 83% 2000/2400 [16:08<02:18,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1850] due to args.save_total_limit\n",
            "{'loss': 3.5573, 'learning_rate': 8.288100208768268e-05, 'epoch': 0.84}\n",
            "{'loss': 3.2353, 'learning_rate': 8.183716075156577e-05, 'epoch': 0.84}\n",
            "{'loss': 3.2561, 'learning_rate': 8.079331941544885e-05, 'epoch': 0.84}\n",
            "{'loss': 2.9859, 'learning_rate': 7.974947807933194e-05, 'epoch': 0.84}\n",
            "{'loss': 3.1292, 'learning_rate': 7.870563674321503e-05, 'epoch': 0.84}\n",
            "{'loss': 3.236, 'learning_rate': 7.766179540709812e-05, 'epoch': 0.85}\n",
            "{'loss': 3.0344, 'learning_rate': 7.661795407098121e-05, 'epoch': 0.85}\n",
            "{'loss': 3.2391, 'learning_rate': 7.557411273486431e-05, 'epoch': 0.85}\n",
            "{'loss': 3.1899, 'learning_rate': 7.45302713987474e-05, 'epoch': 0.85}\n",
            "{'loss': 3.4982, 'learning_rate': 7.348643006263048e-05, 'epoch': 0.85}\n",
            " 85% 2050/2400 [16:26<02:00,  2.90it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2050\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1900] due to args.save_total_limit\n",
            "{'loss': 3.3697, 'learning_rate': 7.244258872651358e-05, 'epoch': 0.86}\n",
            "{'loss': 3.132, 'learning_rate': 7.139874739039667e-05, 'epoch': 0.86}\n",
            "{'loss': 3.301, 'learning_rate': 7.035490605427975e-05, 'epoch': 0.86}\n",
            "{'loss': 3.2225, 'learning_rate': 6.931106471816284e-05, 'epoch': 0.86}\n",
            "{'loss': 3.5751, 'learning_rate': 6.826722338204592e-05, 'epoch': 0.86}\n",
            "{'loss': 3.4349, 'learning_rate': 6.722338204592902e-05, 'epoch': 0.87}\n",
            "{'loss': 3.1997, 'learning_rate': 6.617954070981211e-05, 'epoch': 0.87}\n",
            "{'loss': 3.5028, 'learning_rate': 6.51356993736952e-05, 'epoch': 0.87}\n",
            "{'loss': 3.2574, 'learning_rate': 6.409185803757829e-05, 'epoch': 0.87}\n",
            "{'loss': 3.3188, 'learning_rate': 6.304801670146138e-05, 'epoch': 0.88}\n",
            " 88% 2100/2400 [16:43<01:43,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-1950] due to args.save_total_limit\n",
            "{'loss': 3.423, 'learning_rate': 6.200417536534446e-05, 'epoch': 0.88}\n",
            "{'loss': 3.142, 'learning_rate': 6.0960334029227556e-05, 'epoch': 0.88}\n",
            "{'loss': 3.3454, 'learning_rate': 5.991649269311065e-05, 'epoch': 0.88}\n",
            "{'loss': 3.0827, 'learning_rate': 5.887265135699374e-05, 'epoch': 0.88}\n",
            "{'loss': 3.1868, 'learning_rate': 5.782881002087683e-05, 'epoch': 0.89}\n",
            "{'loss': 3.1409, 'learning_rate': 5.678496868475991e-05, 'epoch': 0.89}\n",
            "{'loss': 3.0253, 'learning_rate': 5.5741127348643004e-05, 'epoch': 0.89}\n",
            "{'loss': 3.3697, 'learning_rate': 5.46972860125261e-05, 'epoch': 0.89}\n",
            "{'loss': 2.9118, 'learning_rate': 5.365344467640919e-05, 'epoch': 0.89}\n",
            "{'loss': 3.3864, 'learning_rate': 5.260960334029228e-05, 'epoch': 0.9}\n",
            " 90% 2150/2400 [17:01<01:26,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 3.4304, 'learning_rate': 5.156576200417536e-05, 'epoch': 0.9}\n",
            "{'loss': 3.194, 'learning_rate': 5.052192066805845e-05, 'epoch': 0.9}\n",
            "{'loss': 3.1328, 'learning_rate': 4.9478079331941545e-05, 'epoch': 0.9}\n",
            "{'loss': 3.263, 'learning_rate': 4.843423799582464e-05, 'epoch': 0.9}\n",
            "{'loss': 3.2055, 'learning_rate': 4.739039665970773e-05, 'epoch': 0.91}\n",
            "{'loss': 3.3947, 'learning_rate': 4.6346555323590816e-05, 'epoch': 0.91}\n",
            "{'loss': 3.2781, 'learning_rate': 4.53027139874739e-05, 'epoch': 0.91}\n",
            "{'loss': 3.2667, 'learning_rate': 4.4258872651356994e-05, 'epoch': 0.91}\n",
            "{'loss': 3.198, 'learning_rate': 4.3215031315240086e-05, 'epoch': 0.91}\n",
            "{'loss': 3.6888, 'learning_rate': 4.217118997912318e-05, 'epoch': 0.92}\n",
            " 92% 2200/2400 [17:18<01:09,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-2050] due to args.save_total_limit\n",
            "{'loss': 3.091, 'learning_rate': 4.1127348643006264e-05, 'epoch': 0.92}\n",
            "{'loss': 3.1475, 'learning_rate': 4.008350730688935e-05, 'epoch': 0.92}\n",
            "{'loss': 3.4031, 'learning_rate': 3.903966597077244e-05, 'epoch': 0.92}\n",
            "{'loss': 3.6477, 'learning_rate': 3.7995824634655535e-05, 'epoch': 0.93}\n",
            "{'loss': 3.2199, 'learning_rate': 3.695198329853863e-05, 'epoch': 0.93}\n",
            "{'loss': 3.2163, 'learning_rate': 3.590814196242171e-05, 'epoch': 0.93}\n",
            "{'loss': 3.2732, 'learning_rate': 3.48643006263048e-05, 'epoch': 0.93}\n",
            "{'loss': 3.4541, 'learning_rate': 3.382045929018789e-05, 'epoch': 0.93}\n",
            "{'loss': 3.635, 'learning_rate': 3.277661795407098e-05, 'epoch': 0.94}\n",
            "{'loss': 3.3273, 'learning_rate': 3.1732776617954076e-05, 'epoch': 0.94}\n",
            " 94% 2250/2400 [17:35<00:51,  2.89it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-2100] due to args.save_total_limit\n",
            "{'loss': 3.2145, 'learning_rate': 3.068893528183716e-05, 'epoch': 0.94}\n",
            "{'loss': 3.2099, 'learning_rate': 2.964509394572025e-05, 'epoch': 0.94}\n",
            "{'loss': 3.2758, 'learning_rate': 2.8601252609603343e-05, 'epoch': 0.94}\n",
            "{'loss': 3.4799, 'learning_rate': 2.755741127348643e-05, 'epoch': 0.95}\n",
            "{'loss': 3.4214, 'learning_rate': 2.651356993736952e-05, 'epoch': 0.95}\n",
            "{'loss': 3.3966, 'learning_rate': 2.546972860125261e-05, 'epoch': 0.95}\n",
            "{'loss': 3.5648, 'learning_rate': 2.44258872651357e-05, 'epoch': 0.95}\n",
            "{'loss': 3.3247, 'learning_rate': 2.338204592901879e-05, 'epoch': 0.95}\n",
            "{'loss': 3.0339, 'learning_rate': 2.2338204592901877e-05, 'epoch': 0.96}\n",
            "{'loss': 3.3829, 'learning_rate': 2.129436325678497e-05, 'epoch': 0.96}\n",
            " 96% 2300/2400 [17:53<00:34,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-2150] due to args.save_total_limit\n",
            "{'loss': 3.3083, 'learning_rate': 2.025052192066806e-05, 'epoch': 0.96}\n",
            "{'loss': 3.4329, 'learning_rate': 1.9206680584551148e-05, 'epoch': 0.96}\n",
            "{'loss': 3.2601, 'learning_rate': 1.816283924843424e-05, 'epoch': 0.96}\n",
            "{'loss': 3.4608, 'learning_rate': 1.7118997912317326e-05, 'epoch': 0.97}\n",
            "{'loss': 3.2456, 'learning_rate': 1.6075156576200418e-05, 'epoch': 0.97}\n",
            "{'loss': 3.0588, 'learning_rate': 1.5031315240083507e-05, 'epoch': 0.97}\n",
            "{'loss': 3.411, 'learning_rate': 1.3987473903966596e-05, 'epoch': 0.97}\n",
            "{'loss': 3.4662, 'learning_rate': 1.2943632567849689e-05, 'epoch': 0.97}\n",
            "{'loss': 3.3086, 'learning_rate': 1.1899791231732778e-05, 'epoch': 0.98}\n",
            "{'loss': 3.0139, 'learning_rate': 1.0855949895615867e-05, 'epoch': 0.98}\n",
            " 98% 2350/2400 [18:10<00:17,  2.91it/s]Saving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-2200] due to args.save_total_limit\n",
            "{'loss': 3.3018, 'learning_rate': 9.812108559498956e-06, 'epoch': 0.98}\n",
            "{'loss': 3.3491, 'learning_rate': 8.768267223382045e-06, 'epoch': 0.98}\n",
            "{'loss': 3.1984, 'learning_rate': 7.724425887265135e-06, 'epoch': 0.99}\n",
            "{'loss': 3.1321, 'learning_rate': 6.680584551148226e-06, 'epoch': 0.99}\n",
            "{'loss': 3.3968, 'learning_rate': 5.636743215031315e-06, 'epoch': 0.99}\n",
            "{'loss': 3.5503, 'learning_rate': 4.592901878914405e-06, 'epoch': 0.99}\n",
            "{'loss': 3.0927, 'learning_rate': 3.5490605427974945e-06, 'epoch': 0.99}\n",
            "{'loss': 3.3429, 'learning_rate': 2.5052192066805844e-06, 'epoch': 1.0}\n",
            "{'loss': 3.2122, 'learning_rate': 1.4613778705636745e-06, 'epoch': 1.0}\n",
            "{'loss': 2.8868, 'learning_rate': 4.1753653444676413e-07, 'epoch': 1.0}\n",
            "100% 2400/2400 [18:28<00:00,  2.91it/s]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction_id, input, token_type_ids, output, instruction. If instruction_id, input, token_type_ids, output, instruction are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/150 [00:00<00:37,  3.95it/s]\u001b[A\n",
            "  2% 3/150 [00:01<00:52,  2.79it/s]\u001b[A\n",
            "  3% 4/150 [00:01<01:00,  2.41it/s]\u001b[A\n",
            "  3% 5/150 [00:02<01:04,  2.24it/s]\u001b[A\n",
            "  4% 6/150 [00:02<01:07,  2.14it/s]\u001b[A\n",
            "  5% 7/150 [00:03<01:08,  2.08it/s]\u001b[A\n",
            "  5% 8/150 [00:03<01:09,  2.04it/s]\u001b[A\n",
            "  6% 9/150 [00:04<01:09,  2.02it/s]\u001b[A\n",
            "  7% 10/150 [00:04<01:09,  2.01it/s]\u001b[A\n",
            "  7% 11/150 [00:05<01:09,  1.99it/s]\u001b[A\n",
            "  8% 12/150 [00:05<01:09,  1.99it/s]\u001b[A\n",
            "  9% 13/150 [00:06<01:09,  1.98it/s]\u001b[A\n",
            "  9% 14/150 [00:06<01:08,  1.98it/s]\u001b[A\n",
            " 10% 15/150 [00:07<01:08,  1.98it/s]\u001b[A\n",
            " 11% 16/150 [00:07<01:07,  1.98it/s]\u001b[A\n",
            " 11% 17/150 [00:08<01:07,  1.97it/s]\u001b[A\n",
            " 12% 18/150 [00:08<01:06,  1.97it/s]\u001b[A\n",
            " 13% 19/150 [00:09<01:06,  1.97it/s]\u001b[A\n",
            " 13% 20/150 [00:09<01:05,  1.97it/s]\u001b[A\n",
            " 14% 21/150 [00:10<01:05,  1.97it/s]\u001b[A\n",
            " 15% 22/150 [00:10<01:04,  1.97it/s]\u001b[A\n",
            " 15% 23/150 [00:11<01:04,  1.97it/s]\u001b[A\n",
            " 16% 24/150 [00:11<01:03,  1.97it/s]\u001b[A\n",
            " 17% 25/150 [00:12<01:03,  1.97it/s]\u001b[A\n",
            " 17% 26/150 [00:12<01:02,  1.97it/s]\u001b[A\n",
            " 18% 27/150 [00:13<01:02,  1.97it/s]\u001b[A\n",
            " 19% 28/150 [00:13<01:01,  1.97it/s]\u001b[A\n",
            " 19% 29/150 [00:14<01:01,  1.97it/s]\u001b[A\n",
            " 20% 30/150 [00:14<01:00,  1.97it/s]\u001b[A\n",
            " 21% 31/150 [00:15<01:00,  1.97it/s]\u001b[A\n",
            " 21% 32/150 [00:15<00:59,  1.97it/s]\u001b[A\n",
            " 22% 33/150 [00:16<00:59,  1.97it/s]\u001b[A\n",
            " 23% 34/150 [00:16<00:58,  1.97it/s]\u001b[A\n",
            " 23% 35/150 [00:17<00:58,  1.97it/s]\u001b[A\n",
            " 24% 36/150 [00:17<00:57,  1.97it/s]\u001b[A\n",
            " 25% 37/150 [00:18<00:57,  1.97it/s]\u001b[A\n",
            " 25% 38/150 [00:18<00:56,  1.97it/s]\u001b[A\n",
            " 26% 39/150 [00:19<00:56,  1.97it/s]\u001b[A\n",
            " 27% 40/150 [00:19<00:55,  1.97it/s]\u001b[A\n",
            " 27% 41/150 [00:20<00:55,  1.97it/s]\u001b[A\n",
            " 28% 42/150 [00:20<00:54,  1.97it/s]\u001b[A\n",
            " 29% 43/150 [00:21<00:54,  1.97it/s]\u001b[A\n",
            " 29% 44/150 [00:21<00:53,  1.97it/s]\u001b[A\n",
            " 30% 45/150 [00:22<00:53,  1.97it/s]\u001b[A\n",
            " 31% 46/150 [00:22<00:52,  1.97it/s]\u001b[A\n",
            " 31% 47/150 [00:23<00:52,  1.97it/s]\u001b[A\n",
            " 32% 48/150 [00:23<00:51,  1.97it/s]\u001b[A\n",
            " 33% 49/150 [00:24<00:51,  1.97it/s]\u001b[A\n",
            " 33% 50/150 [00:24<00:50,  1.97it/s]\u001b[A\n",
            " 34% 51/150 [00:25<00:50,  1.97it/s]\u001b[A\n",
            " 35% 52/150 [00:25<00:49,  1.97it/s]\u001b[A\n",
            " 35% 53/150 [00:26<00:49,  1.97it/s]\u001b[A\n",
            " 36% 54/150 [00:26<00:48,  1.97it/s]\u001b[A\n",
            " 37% 55/150 [00:27<00:48,  1.97it/s]\u001b[A\n",
            " 37% 56/150 [00:27<00:47,  1.97it/s]\u001b[A\n",
            " 38% 57/150 [00:28<00:47,  1.97it/s]\u001b[A\n",
            " 39% 58/150 [00:28<00:46,  1.97it/s]\u001b[A\n",
            " 39% 59/150 [00:29<00:46,  1.97it/s]\u001b[A\n",
            " 40% 60/150 [00:29<00:45,  1.97it/s]\u001b[A\n",
            " 41% 61/150 [00:30<00:45,  1.97it/s]\u001b[A\n",
            " 41% 62/150 [00:30<00:44,  1.97it/s]\u001b[A\n",
            " 42% 63/150 [00:31<00:44,  1.97it/s]\u001b[A\n",
            " 43% 64/150 [00:31<00:43,  1.97it/s]\u001b[A\n",
            " 43% 65/150 [00:32<00:43,  1.97it/s]\u001b[A\n",
            " 44% 66/150 [00:32<00:42,  1.97it/s]\u001b[A\n",
            " 45% 67/150 [00:33<00:42,  1.97it/s]\u001b[A\n",
            " 45% 68/150 [00:33<00:41,  1.97it/s]\u001b[A\n",
            " 46% 69/150 [00:34<00:41,  1.97it/s]\u001b[A\n",
            " 47% 70/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 47% 71/150 [00:35<00:40,  1.97it/s]\u001b[A\n",
            " 48% 72/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 73/150 [00:36<00:39,  1.97it/s]\u001b[A\n",
            " 49% 74/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 50% 75/150 [00:37<00:38,  1.97it/s]\u001b[A\n",
            " 51% 76/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 51% 77/150 [00:38<00:37,  1.97it/s]\u001b[A\n",
            " 52% 78/150 [00:39<00:36,  1.97it/s]\u001b[A\n",
            " 53% 79/150 [00:39<00:35,  1.97it/s]\u001b[A\n",
            " 53% 80/150 [00:40<00:35,  1.97it/s]\u001b[A\n",
            " 54% 81/150 [00:40<00:34,  1.97it/s]\u001b[A\n",
            " 55% 82/150 [00:41<00:34,  1.97it/s]\u001b[A\n",
            " 55% 83/150 [00:41<00:33,  1.97it/s]\u001b[A\n",
            " 56% 84/150 [00:42<00:33,  1.97it/s]\u001b[A\n",
            " 57% 85/150 [00:42<00:32,  1.97it/s]\u001b[A\n",
            " 57% 86/150 [00:43<00:32,  1.97it/s]\u001b[A\n",
            " 58% 87/150 [00:43<00:31,  1.97it/s]\u001b[A\n",
            " 59% 88/150 [00:44<00:31,  1.97it/s]\u001b[A\n",
            " 59% 89/150 [00:44<00:30,  1.97it/s]\u001b[A\n",
            " 60% 90/150 [00:45<00:30,  1.97it/s]\u001b[A\n",
            " 61% 91/150 [00:45<00:29,  1.97it/s]\u001b[A\n",
            " 61% 92/150 [00:46<00:29,  1.97it/s]\u001b[A\n",
            " 62% 93/150 [00:46<00:28,  1.97it/s]\u001b[A\n",
            " 63% 94/150 [00:47<00:28,  1.97it/s]\u001b[A\n",
            " 63% 95/150 [00:47<00:27,  1.97it/s]\u001b[A\n",
            " 64% 96/150 [00:48<00:27,  1.97it/s]\u001b[A\n",
            " 65% 97/150 [00:48<00:26,  1.97it/s]\u001b[A\n",
            " 65% 98/150 [00:49<00:26,  1.97it/s]\u001b[A\n",
            " 66% 99/150 [00:49<00:25,  1.97it/s]\u001b[A\n",
            " 67% 100/150 [00:50<00:25,  1.97it/s]\u001b[A\n",
            " 67% 101/150 [00:50<00:24,  1.97it/s]\u001b[A\n",
            " 68% 102/150 [00:51<00:24,  1.97it/s]\u001b[A\n",
            " 69% 103/150 [00:51<00:23,  1.97it/s]\u001b[A\n",
            " 69% 104/150 [00:52<00:23,  1.97it/s]\u001b[A\n",
            " 70% 105/150 [00:52<00:22,  1.97it/s]\u001b[A\n",
            " 71% 106/150 [00:53<00:22,  1.97it/s]\u001b[A\n",
            " 71% 107/150 [00:53<00:21,  1.97it/s]\u001b[A\n",
            " 72% 108/150 [00:54<00:21,  1.97it/s]\u001b[A\n",
            " 73% 109/150 [00:54<00:20,  1.97it/s]\u001b[A\n",
            " 73% 110/150 [00:55<00:20,  1.97it/s]\u001b[A\n",
            " 74% 111/150 [00:55<00:19,  1.97it/s]\u001b[A\n",
            " 75% 112/150 [00:56<00:19,  1.97it/s]\u001b[A\n",
            " 75% 113/150 [00:56<00:18,  1.97it/s]\u001b[A\n",
            " 76% 114/150 [00:57<00:18,  1.97it/s]\u001b[A\n",
            " 77% 115/150 [00:57<00:17,  1.97it/s]\u001b[A\n",
            " 77% 116/150 [00:58<00:17,  1.97it/s]\u001b[A\n",
            " 78% 117/150 [00:58<00:16,  1.97it/s]\u001b[A\n",
            " 79% 118/150 [00:59<00:16,  1.97it/s]\u001b[A\n",
            " 79% 119/150 [00:59<00:15,  1.97it/s]\u001b[A\n",
            " 80% 120/150 [01:00<00:15,  1.97it/s]\u001b[A\n",
            " 81% 121/150 [01:00<00:14,  1.97it/s]\u001b[A\n",
            " 81% 122/150 [01:01<00:14,  1.97it/s]\u001b[A\n",
            " 82% 123/150 [01:01<00:13,  1.97it/s]\u001b[A\n",
            " 83% 124/150 [01:02<00:13,  1.97it/s]\u001b[A\n",
            " 83% 125/150 [01:02<00:12,  1.97it/s]\u001b[A\n",
            " 84% 126/150 [01:03<00:12,  1.97it/s]\u001b[A\n",
            " 85% 127/150 [01:03<00:11,  1.97it/s]\u001b[A\n",
            " 85% 128/150 [01:04<00:11,  1.97it/s]\u001b[A\n",
            " 86% 129/150 [01:04<00:10,  1.97it/s]\u001b[A\n",
            " 87% 130/150 [01:05<00:10,  1.97it/s]\u001b[A\n",
            " 87% 131/150 [01:05<00:09,  1.97it/s]\u001b[A\n",
            " 88% 132/150 [01:06<00:09,  1.97it/s]\u001b[A\n",
            " 89% 133/150 [01:06<00:08,  1.97it/s]\u001b[A\n",
            " 89% 134/150 [01:07<00:08,  1.97it/s]\u001b[A\n",
            " 90% 135/150 [01:07<00:07,  1.97it/s]\u001b[A\n",
            " 91% 136/150 [01:08<00:07,  1.97it/s]\u001b[A\n",
            " 91% 137/150 [01:08<00:06,  1.97it/s]\u001b[A\n",
            " 92% 138/150 [01:09<00:06,  1.97it/s]\u001b[A\n",
            " 93% 139/150 [01:09<00:05,  1.97it/s]\u001b[A\n",
            " 93% 140/150 [01:10<00:05,  1.97it/s]\u001b[A\n",
            " 94% 141/150 [01:10<00:04,  1.97it/s]\u001b[A\n",
            " 95% 142/150 [01:11<00:04,  1.97it/s]\u001b[A\n",
            " 95% 143/150 [01:12<00:03,  1.97it/s]\u001b[A\n",
            " 96% 144/150 [01:12<00:03,  1.97it/s]\u001b[A\n",
            " 97% 145/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 97% 146/150 [01:13<00:02,  1.97it/s]\u001b[A\n",
            " 98% 147/150 [01:14<00:01,  1.96it/s]\u001b[A\n",
            " 99% 148/150 [01:14<00:01,  1.96it/s]\u001b[A\n",
            " 99% 149/150 [01:15<00:00,  1.96it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.2800168991088867, 'eval_runtime': 76.0941, 'eval_samples_per_second': 15.77, 'eval_steps_per_second': 1.971, 'epoch': 1.0}\n",
            "100% 2400/2400 [19:44<00:00,  2.91it/s]\n",
            "100% 150/150 [01:15<00:00,  1.97it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to ./falcon-7b-novel17c-4bit/checkpoint-2400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [falcon-7b-novel17c-4bit/checkpoint-2250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1184.3448, 'train_samples_per_second': 4.053, 'train_steps_per_second': 2.026, 'train_loss': 3.399245674610138, 'epoch': 1.0}\n",
            "100% 2400/2400 [19:44<00:00,  2.03it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/MyDrive/falcon/wandb/offline-run-20230623_095539-ky0fx5zw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230623_095539-ky0fx5zw/logs\u001b[0m\n",
            "Train completed.\n",
            "Model Saved.\n"
          ]
        }
      ],
      "source": [
        "!falcontune finetune \\\n",
        "    --model=falcon-7b-instruct-4bit \\\n",
        "    --weights=gptq_model-4bit-64g.safetensors \\\n",
        "    --dataset=./instruct_fr_novel17.json \\\n",
        "    --data_type=alpaca \\\n",
        "    --lora_out_dir=./falcon-7b-novel17c-4bit/ \\\n",
        "    --mbatch_size=1 \\\n",
        "    --batch_size=2 \\\n",
        "    --epochs=1 \\\n",
        "    --lr=5e-4 \\\n",
        "    --cutoff_len=256 \\\n",
        "    --lora_r=8 \\\n",
        "    --lora_alpha=16 \\\n",
        "    --lora_dropout=0.05 \\\n",
        "    --warmup_steps=5 \\\n",
        "    --save_steps=50 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --logging_steps=5 \\\n",
        "    --target_modules='[\"query_key_value\"]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_84CU2sE46s"
      },
      "source": [
        "Si tout se passe bien, vous devrez avoir comlètement fini l'entraînement. Un petit message final apparaît vous invitant à partager le modèle sur HuggingFace: \"Training completed. Do not forget to share your model on huggingface.co/models =)\"\n",
        "\n",
        "Après un petit temps de synchronisation entre Google Colab, vous allez voir apparaître deux fichiers dans le dossier du modèle de fine-tuning : adapter_model.bin (le modèle proprepement dit) et adapter_model.config (un fichier de configuration). À noter que le modèle de fine-tuning est considérablement plus petit que le modèle d'origine : c'est en quelque sorte un modèle complémentaire qui vient ajuster le LLM (et il en aura toujours besoin pour fonctionner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aKvXziTziXY"
      },
      "source": [
        "# Générer du texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPV1WuvEvhD"
      },
      "source": [
        "Et maintenant il est possible de générer du texte. La fonction par défaut de falcontune n'est pas pour l'instant pas très pratique mais cela devrait s'améliorer prochainement. À noter aussi que les instructions trop brèves peut susciter un bug un peu agaçant (probability distribution error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L_qtyiFzgv2",
        "outputId": "4b167567-c9a1-4241-f2b3-5b054d5427e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-23 10:27:51.589827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-1w6r6ll4exows --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "The safetensors archive passed at gptq_model-4bit-64g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "Device map for lora: {'': 0}\n",
            "./falcon-7b-novel17c-4bit/ loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Aucun autre que vous n'a l'honneur de me comprendre & d'eſtre ſi aigriſé contre moi que de me croire capable de vous dire une ſomnologe ſi éloignée & ſi peu vraiſe de vos idées.\n",
            "Le moyen le plus aisé pour aller sur la Lune que je connois & pour lequel j'en ai l'honneur de vous le dire est celui qui consiste à nous rendre dans les airs en nous tirant par un canon. Cette machine ſe trouve dans les airs& les nuages de tous les pays. Il nous faut seulement faire un trou dans le vaisſe& nous placer dedans pour être tirés plus haut que la Montagne & dans lequel nous pouvons nous reprendre en toute liberté. Ce moyen convient à tout le monde & vous me ferez un grand\n",
            "\n",
            "Took 30.34 s\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!falcontune generate \\\n",
        "    --model falcon-7b-instruct-4bit \\\n",
        "    --weights gptq_model-4bit-64g.safetensors \\\n",
        "    --lora_apply_dir ./falcon-7b-novel17c-4bit/ \\\n",
        "    --max_new_tokens 200 \\\n",
        "    --use_cache \\\n",
        "    --do_sample \\\n",
        "    --instruction \"Pourrois-tu me iustifier le moyen le plus ſimple & le plus aisé que j'ay par lequel je pourrois me rendre sur la Lune ? ###OUTPUT\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
