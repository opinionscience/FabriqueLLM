{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXa7DOX84GG"
      },
      "source": [
        "# **Tutoriel** - Fine-tuning de Llama2\n",
        "\n",
        "Ce carnet de code permet de fine-tuner Llama 2 pour générer du texte en français écrit du 17e siècle. Le fine-tuning est basé sur un corpus de 2000 instructions extraites de romans français publiés entre 1600 et 1700. C'est un bon exemple pour mesurer les capacités du fine-tuning (et accessoirement faire mieux que ChatGPT dans ce domaine précis).\n",
        "\n",
        "Vous pouvez également utiliser ce carnet de code à partir de n'importe quel jeu de données hébergé sur HuggingFace sous réserve d'avoir la structure ###human / ###assistant (et non pas le format classique de Alpaca : instruction/input/output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTyUYSsywxD"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5sPY1BeIbRn",
        "outputId": "631a7148-4bdf-4e1e-c038-b43248b34e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/llama\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/My Drive/llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6nmahaFHKEK"
      },
      "source": [
        "Nous allons utilisé un petit script en Python créé par Younes Belkada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyFPmAByvhV",
        "outputId": "102af827-ce94-498d-e4df-1edc871b1928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'e90381ed142ee80c8e7ea602b18d50f0'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 38 (delta 14), reused 10 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), 6.25 KiB | 20.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf e90381ed142ee80c8e7ea602b18d50f0\n",
        "!git clone https://gist.github.com/Pclanglais/e90381ed142ee80c8e7ea602b18d50f0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HkbzY3GxaaC"
      },
      "source": [
        "We chargeons les extensions nécessaires avec la bonne version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PG26TNLxmVc"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate==0.21.0\n",
        "!pip install peft==0.4.0\n",
        "!pip install bitsandbytes==0.40.2\n",
        "!pip install transformers==4.30.2\n",
        "!pip install trl==0.4.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons dans le dossier contenant le script (désolé pour le nom imprononceable)"
      ],
      "metadata": {
        "id": "ewmJV-V6KE-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"e90381ed142ee80c8e7ea602b18d50f0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reDLaWtzKJRZ",
        "outputId": "95d46a15-4fdf-4b86-c2a5-fbaa0e20fa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llama/e90381ed142ee80c8e7ea602b18d50f0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous lançons le script. Vous pouvez ajuster le nombre de steps pour avoir un entraînement plus intensif. En l'état actuel, l'entraînement prendra une quinzaine de minutes sur la version pro de Google Colab."
      ],
      "metadata": {
        "id": "ypVTNAJtKIq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune_llama_v2.py --dataset_name \"AlexanderDoria/novel17_test\" --max_steps 500 --merge_and_push True --model_name \"daryl149/llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jszx0vHGKPS1",
        "outputId": "2e6af326-abce-4e15-ff94-0a24d00544da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-19 13:28:29.325023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 2/2 [00:14<00:00,  7.12s/it]\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/AlexanderDoria___json/AlexanderDoria--novel17_test-5d434af8006df014/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/AlexanderDoria___json/AlexanderDoria--novel17_test-5d434af8006df014/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2fc5e2a3395f2e8d.arrow\n",
            "  0% 0/500 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.0823, 'learning_rate': 0.0002, 'epoch': 10.0}\n",
            "{'loss': 1.0035, 'learning_rate': 0.0002, 'epoch': 20.0}\n",
            "{'loss': 0.8977, 'learning_rate': 0.0002, 'epoch': 30.0}\n",
            "{'loss': 0.785, 'learning_rate': 0.0002, 'epoch': 40.0}\n",
            "{'loss': 0.6513, 'learning_rate': 0.0002, 'epoch': 50.0}\n",
            "{'loss': 0.5131, 'learning_rate': 0.0002, 'epoch': 60.0}\n",
            "{'loss': 0.345, 'learning_rate': 0.0002, 'epoch': 70.0}\n",
            "{'loss': 0.2057, 'learning_rate': 0.0002, 'epoch': 80.0}\n",
            "{'loss': 0.1027, 'learning_rate': 0.0002, 'epoch': 90.0}\n",
            "{'loss': 0.0541, 'learning_rate': 0.0002, 'epoch': 100.0}\n",
            "{'loss': 0.0322, 'learning_rate': 0.0002, 'epoch': 110.0}\n",
            "{'loss': 0.026, 'learning_rate': 0.0002, 'epoch': 120.0}\n",
            "{'loss': 0.0226, 'learning_rate': 0.0002, 'epoch': 130.0}\n",
            "{'loss': 0.0213, 'learning_rate': 0.0002, 'epoch': 140.0}\n",
            "{'loss': 0.0205, 'learning_rate': 0.0002, 'epoch': 150.0}\n",
            "{'loss': 0.02, 'learning_rate': 0.0002, 'epoch': 160.0}\n",
            "{'loss': 0.0196, 'learning_rate': 0.0002, 'epoch': 170.0}\n",
            "{'loss': 0.0192, 'learning_rate': 0.0002, 'epoch': 180.0}\n",
            "{'loss': 0.0188, 'learning_rate': 0.0002, 'epoch': 190.0}\n",
            "{'loss': 0.0183, 'learning_rate': 0.0002, 'epoch': 200.0}\n",
            "{'loss': 0.0178, 'learning_rate': 0.0002, 'epoch': 210.0}\n",
            "{'loss': 0.0172, 'learning_rate': 0.0002, 'epoch': 220.0}\n",
            "{'loss': 0.0163, 'learning_rate': 0.0002, 'epoch': 230.0}\n",
            "{'loss': 0.015, 'learning_rate': 0.0002, 'epoch': 240.0}\n",
            "{'loss': 0.0124, 'learning_rate': 0.0002, 'epoch': 250.0}\n",
            "{'loss': 0.0076, 'learning_rate': 0.0002, 'epoch': 260.0}\n",
            "{'loss': 0.005, 'learning_rate': 0.0002, 'epoch': 270.0}\n",
            "{'loss': 0.0046, 'learning_rate': 0.0002, 'epoch': 280.0}\n",
            "{'loss': 0.0031, 'learning_rate': 0.0002, 'epoch': 290.0}\n",
            "{'loss': 0.0015, 'learning_rate': 0.0002, 'epoch': 300.0}\n",
            "{'loss': 0.0005, 'learning_rate': 0.0002, 'epoch': 310.0}\n",
            "{'loss': 0.0004, 'learning_rate': 0.0002, 'epoch': 320.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 330.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 340.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 350.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 360.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 370.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 380.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 390.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 400.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 410.0}\n",
            "{'loss': 0.0003, 'learning_rate': 0.0002, 'epoch': 420.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 430.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 440.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 450.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 460.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 470.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 480.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 490.0}\n",
            "{'loss': 0.0002, 'learning_rate': 0.0002, 'epoch': 500.0}\n",
            "{'train_runtime': 824.0224, 'train_samples_per_second': 9.708, 'train_steps_per_second': 0.607, 'train_loss': 0.11930228792596609, 'epoch': 500.0}\n",
            "100% 500/500 [13:44<00:00,  1.65s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [01:52<00:00, 56.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inférence"
      ],
      "metadata": {
        "id": "NdrDKYEyaOuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que l'entraînement est fini, nous allons générer du texte (c'est l'inférence). À noter que cela prend beaucoup de mémoire pour l'instant : je recommande de réinitialiser le runtime de la session sur colab."
      ],
      "metadata": {
        "id": "6gkLiWDDeBxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous chargeons le modèle"
      ],
      "metadata": {
        "id": "OvBbQDaxfhuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\"results/final_merged_checkpoint\")"
      ],
      "metadata": {
        "id": "mqsixri5fiuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous générons maintenant le texte. Comme le modèle n'a pas été beaucoup entraîné, nous lui donnons un début de réponse : cela aide le modèle à se focaliser. N'hésitez pas à changer le max_length pour avoir des réponses plus longues (c'est le nombre maximal de tokens)."
      ],
      "metadata": {
        "id": "IiadT0K4fjVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"daryl149/llama-2-7b-chat-hf\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=model, tokenizer = tokenizer, max_length=200)\n",
        "\n",
        "result = generator(\"Ecrivez un texte en ancien français du 17e siècle sur le meilleur moyen de voyager sur la lune ### Assistant: Je serois parti sur la Lune\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "5w6cyw-kWHkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b0d5db-9720-4284-b932-ef14040e0afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Ecrivez un texte en ancien français du 17e siècle sur le meilleur moyen de voyager sur la lune ### Assistant: Je serois parti sur la Lune, mais comment fuste bon le plus beau moyen de faire? Quoi! Sçavans le plus savant d'autrefois dit que les anciens de la lune étoient des esprits sublimes et de bon sens, qui ne savoient que faire des corps mortels qui les entornoient, et que donc le plus beau moyen de voyager sur la lune seroit de ne point faire de démonstres et de ne point savoier les choses de ce monde-ci, mais de méditer sur les choses éternelles et de ne point songer à celles qui sont de ce monde.Ћommo d'autrefois disoit que les Bretons avoit beau voyage\"}]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}