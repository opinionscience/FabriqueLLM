{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXa7DOX84GG"
      },
      "source": [
        "# **Tutoriel** - Fine-tuning de Llama\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/opinionscience/FabriqueLLM/main/illustration/llama_image.png\" alt=\"Llama logo\"  width=\"500\"/>\n",
        "\n",
        "Ce carnet de code permet d'effectuer le fine-tuning d'un grand modèle de langue développé par Meta, Llama. Tout en n'étant pas diffusé en open source mais seulement à des fins de recherche non-commercial, Llama a eu un impact significatif sur le développement de LLMs ouverts alternatifs à chatGPT. Une grande partie des méthodes et des jeux de données utilisés pour adapter les LLMs ont été d'abord pensés pour LLama.\n",
        "\n",
        "LLama est disponible sous quatre versions à 7B, 13B, 33B and 65B : le B correspond ici à un milliard de paramètres. Nous allons ici utiliser la plus plus petite version (7B) qui est compatible avec la version gratuite de Google Colab.\n",
        "\n",
        "Ce carnet de code est basé sur une version modifiée de LLMTune. LLMTune est un projet de recherche de Cornell Tech et de Cornell University (Cornell University). L'exécution de la version originale sur Google Colab donne lieu à plusieurs bugs que j'ai corrigés.\n",
        "\n",
        "Cette démonstration ne fait tourner qu'une seule *epoch* ce qui est suffisant pour avoir un premier aperçu. Pour obtenir un bon modèle, il est conseillé de faire tourner le fine-tuning pendant trois *epochs*. Sur notre corpus de démonstration de 2000 instructions une *epoch* prendra environ 30 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTyUYSsywxD"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kamSkzAq9KE5"
      },
      "source": [
        "En tout premier lieu nous vérifions si nous disposons de suffisamment de mémoire vive (au moins 24go) sinon ce n'est pas la peine de lancer le script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwOy3-Dl9iRW",
        "outputId": "6eb8b90e-e143-49f2-b18a-55df4e34b560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun 11 14:57:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    40W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgZdKVNFIaXm"
      },
      "source": [
        "Une telle configuration est suffisante pour le fine-tuning de llama-7b-4bit basé sur LLMTune (mais pas pour llama-13b-4bit, sans même parler des versions suivantes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5sPY1BeIbRn",
        "outputId": "c6bd99b0-a352-455b-fec0-eb4c708458ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/llama\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/My Drive/llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6nmahaFHKEK"
      },
      "source": [
        "Nous allons utiliser llmtune. C'est une petite application python disponible sur Github qui permet d'effectuer le fine-tuning de Llama (pour l'instant depuis mon fork comme il y a un bug sur la version de base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyFPmAByvhV",
        "outputId": "352cec13-1824-4647-c6b7-09317dde4814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llmtune'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 150 (delta 58), reused 36 (delta 29), pack-reused 54\u001b[K\n",
            "Receiving objects: 100% (150/150), 48.26 KiB | 2.54 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Pclanglais/llmtune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HkbzY3GxaaC"
      },
      "source": [
        "Nous installons également les dépendances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PG26TNLxmVc",
        "outputId": "99f1fbb0-4012-4e81-bb59-293f374b863c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/llama/llmtune\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Collecting transformers (from -r requirements.txt (line 4))\n",
            "  Cloning https://github.com/huggingface/transformers (to revision 9417c924af539be5f941c8a709a96b60dfe29eb3) to /tmp/pip-install-_plc44yu/transformers_c9427feddaa74835b13dc5c1d324260d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-install-_plc44yu/transformers_c9427feddaa74835b13dc5c1d324260d\n",
            "  Running command git rev-parse -q --verify 'sha^9417c924af539be5f941c8a709a96b60dfe29eb3'\n",
            "  Running command git fetch -q https://github.com/huggingface/transformers 9417c924af539be5f941c8a709a96b60dfe29eb3\n",
            "  Running command git checkout -q 9417c924af539be5f941c8a709a96b60dfe29eb3\n",
            "  Resolved https://github.com/huggingface/transformers to commit 9417c924af539be5f941c8a709a96b60dfe29eb3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting peft (from -r requirements.txt (line 5))\n",
            "  Cloning https://github.com/huggingface/peft (to revision ff9a1edbfd2d405b86d50a2e5299cc1bbd49d887) to /tmp/pip-install-_plc44yu/peft_7bc578958db04f49a8cf5c2588abfb5f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-install-_plc44yu/peft_7bc578958db04f49a8cf5c2588abfb5f\n",
            "  Running command git rev-parse -q --verify 'sha^ff9a1edbfd2d405b86d50a2e5299cc1bbd49d887'\n",
            "  Running command git fetch -q https://github.com/huggingface/peft ff9a1edbfd2d405b86d50a2e5299cc1bbd49d887\n",
            "  Running command git checkout -q ff9a1edbfd2d405b86d50a2e5299cc1bbd49d887\n",
            "  Resolved https://github.com/huggingface/peft to commit ff9a1edbfd2d405b86d50a2e5299cc1bbd49d887\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.13.1+cu116 (from -r requirements.txt (line 2))\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1977.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.97 (from -r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.10.1 (from -r requirements.txt (line 6))\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu116->-r requirements.txt (line 2)) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1->-r requirements.txt (line 6))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (4.65.0)\n",
            "Collecting xxhash (from datasets==2.10.1->-r requirements.txt (line 6))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.10.1->-r requirements.txt (line 6))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0 (from datasets==2.10.1->-r requirements.txt (line 6))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (23.1)\n",
            "Collecting responses<0.19 (from datasets==2.10.1->-r requirements.txt (line 6))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 5)) (5.9.5)\n",
            "Collecting accelerate (from peft->-r requirements.txt (line 5))\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 6)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 6)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 6)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 6)) (1.16.0)\n",
            "Building wheels for collected packages: transformers, peft\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0-py3-none-any.whl size=6927815 sha256=ac72b9550b5a24ef069c13f1b904207aebbc33d9696fabd206d0420367ad3eb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/2f/7252535f5364eba12f41631e5d539c547452d093115022d75f\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=41648 sha256=3043c8f1d4500795b90a74404c62ba6ab5613291bc682161ef0101958d252043\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/cb/7a/726521b9c9e1b5a9ab55a072bbc944f01d14e38a3bc81c5e3e\n",
            "Successfully built transformers peft\n",
            "Installing collected packages: tokenizers, sentencepiece, xxhash, torch, dill, responses, multiprocess, huggingface-hub, accelerate, transformers, peft, datasets\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.20.3 datasets-2.10.1 dill-0.3.6 huggingface-hub-0.16.4 multiprocess-0.70.14 peft-0.3.0.dev0 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.3 torch-1.13.1+cu116 transformers-4.28.0 xxhash-3.2.0\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing llmtune.egg-info/PKG-INFO\n",
            "writing dependency_links to llmtune.egg-info/dependency_links.txt\n",
            "writing entry points to llmtune.egg-info/entry_points.txt\n",
            "writing top-level names to llmtune.egg-info/top_level.txt\n",
            "reading manifest file 'llmtune.egg-info/SOURCES.txt'\n",
            "writing manifest file 'llmtune.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/executor.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/__init__.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/config.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/utils.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/run.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine\n",
            "copying build/lib/llmtune/engine/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/calibration.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/abstract.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/text.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/alpaca.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/gpt4all.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/lora.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/utils.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/config.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/peft.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "copying build/lib/llmtune/engine/quant/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "copying build/lib/llmtune/engine/quant/modules.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "copying build/lib/llmtune/engine/quant/converter.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/executor.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/gptq.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/quantizer.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/llms\n",
            "copying build/lib/llmtune/llms/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/llms\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "copying build/lib/llmtune/llms/llama/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "copying build/lib/llmtune/llms/llama/config.py -> build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "copying build/lib/llmtune/llms/llama/model.py -> build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "copying build/lib/llmtune/llms/opt/model.py -> build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "copying build/lib/llmtune/llms/opt/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "copying build/lib/llmtune/llms/opt/config.py -> build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/executor.py to executor.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/run.py to run.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/calibration.py to calibration.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/abstract.py to abstract.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/text.py to text.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/alpaca.py to alpaca.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/gpt4all.py to gpt4all.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/lora.py to lora.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/peft.py to peft.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/modules.py to modules.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/converter.py to converter.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/executor.py to executor.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/gptq.py to gptq.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/quantizer.py to quantizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/llama/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/llama/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/llama/model.py to model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/opt/model.py to model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/opt/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/opt/config.py to config.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/llmtune-0.1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing llmtune-0.1.0-py3.10.egg\n",
            "Copying llmtune-0.1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding llmtune 0.1.0 to easy-install.pth file\n",
            "Installing llmtune script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/llmtune-0.1.0-py3.10.egg\n",
            "Processing dependencies for llmtune==0.1.0\n",
            "Finished processing dependencies for llmtune==0.1.0\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing quant_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to quant_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to quant_cuda.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.8) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:397: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for quant_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.quant_cuda.cpython-310: module references __file__\n",
            "creating 'dist/quant_cuda-0.0.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "Extracting quant_cuda-0.0.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding quant-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for quant-cuda==0.0.0\n",
            "Finished processing dependencies for quant-cuda==0.0.0\n"
          ]
        }
      ],
      "source": [
        "%cd llmtune\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-bb8mGdxnaT"
      },
      "source": [
        "Enfin nous récupérons le modèle de base de Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhA3yI5Oxs7x",
        "outputId": "c8b703cd-b837-46af-e00b-bff212d9360b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-11 09:16:04--  https://huggingface.co/decapoda-research/llama-7b-hf-int4/resolve/main/llama-7b-4bit.pt\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.88, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/98/d7/98d7c1a709a7ae2b2b2dbbc8fa82286eae5bf3bff3f004e7d5843c4344e64b11/b48471adcc7e20542f9cacc348725b4ad36c3321ca2015bbd57d3876302426ee?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-7b-4bit.pt%3B+filename%3D%22llama-7b-4bit.pt%22%3B&Expires=1689326164&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTMyNjE2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85OC9kNy85OGQ3YzFhNzA5YTdhZTJiMmIyZGJiYzhmYTgyMjg2ZWFlNWJmM2JmZjNmMDA0ZTdkNTg0M2M0MzQ0ZTY0YjExL2I0ODQ3MWFkY2M3ZTIwNTQyZjljYWNjMzQ4NzI1YjRhZDM2YzMzMjFjYTIwMTViYmQ1N2QzODc2MzAyNDI2ZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Ag8PAooymo2kC1UA4Ntcu8P6vaVQquVTI75r2xQ393KHlY6fY2u50%7Eu1V0ini21KvElJrp7G1hRYNkXVMrXTuR5P4WyfsH08sYEEStqsJlueCeVZvh%7EMClyXgwshhUe83HYdDXEgp7HhUGsnT8scaOLLf5ImbJ60OA6EzIjY9l4VqIGtUQjT6zm4nqVzotCMdG57Z8x8VJPq9bTY3N9yfRFqn0zf5gJrzRy0uEpYK2oKCzgECurG9qo7Hd2S5msivCTrlEPSngh0uEYJHauL3eRgy0VeJMMP1Lgi0u8wQdZqIK78kdt69JUfaBAttvmIzX-Lgp1X31I1r9Ev58szLw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-07-11 09:16:05--  https://cdn-lfs.huggingface.co/repos/98/d7/98d7c1a709a7ae2b2b2dbbc8fa82286eae5bf3bff3f004e7d5843c4344e64b11/b48471adcc7e20542f9cacc348725b4ad36c3321ca2015bbd57d3876302426ee?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-7b-4bit.pt%3B+filename%3D%22llama-7b-4bit.pt%22%3B&Expires=1689326164&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTMyNjE2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85OC9kNy85OGQ3YzFhNzA5YTdhZTJiMmIyZGJiYzhmYTgyMjg2ZWFlNWJmM2JmZjNmMDA0ZTdkNTg0M2M0MzQ0ZTY0YjExL2I0ODQ3MWFkY2M3ZTIwNTQyZjljYWNjMzQ4NzI1YjRhZDM2YzMzMjFjYTIwMTViYmQ1N2QzODc2MzAyNDI2ZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Ag8PAooymo2kC1UA4Ntcu8P6vaVQquVTI75r2xQ393KHlY6fY2u50%7Eu1V0ini21KvElJrp7G1hRYNkXVMrXTuR5P4WyfsH08sYEEStqsJlueCeVZvh%7EMClyXgwshhUe83HYdDXEgp7HhUGsnT8scaOLLf5ImbJ60OA6EzIjY9l4VqIGtUQjT6zm4nqVzotCMdG57Z8x8VJPq9bTY3N9yfRFqn0zf5gJrzRy0uEpYK2oKCzgECurG9qo7Hd2S5msivCTrlEPSngh0uEYJHauL3eRgy0VeJMMP1Lgi0u8wQdZqIK78kdt69JUfaBAttvmIzX-Lgp1X31I1r9Ev58szLw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.64, 18.154.185.27, 18.154.185.94, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3779492377 (3.5G) [application/zip]\n",
            "Saving to: ‘llama-7b-4bit.pt’\n",
            "\n",
            "llama-7b-4bit.pt    100%[===================>]   3.52G  62.2MB/s    in 57s     \n",
            "\n",
            "2023-07-11 09:17:01 (63.6 MB/s) - ‘llama-7b-4bit.pt’ saved [3779492377/3779492377]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/decapoda-research/llama-7b-hf-int4/resolve/main/llama-7b-4bit.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFnsudjOHtnZ"
      },
      "source": [
        "## Le corpus d'instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PeUIuyeHOdM"
      },
      "source": [
        "Nous récupérons un set d'instruction. Dans l'optique d'un simple exemple de démonstration nous utilisons ici une sélection aléatoire de différents corpus d'instructions traduits en français par le projet Vicogne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjwy8C3kw4fF",
        "outputId": "65ef8c6b-426e-486a-c421-598ac36cdcca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-11 10:17:26--  https://raw.githubusercontent.com/opinionscience/EUInstruct/main/sample/french_novel17_instruction_simple.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2336881 (2.2M) [text/plain]\n",
            "Saving to: ‘french_novel17_instruction_simple.json’\n",
            "\n",
            "french_novel17_inst 100%[===================>]   2.23M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-11 10:17:26 (53.9 MB/s) - ‘french_novel17_instruction_simple.json’ saved [2336881/2336881]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/opinionscience/EUInstruct/main/sample/french_novel17_instruction_simple.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSABEKyZ1Jy3"
      },
      "source": [
        "Ces instructions utilisent le format classique du projet Alpaca de Stanford : *instructions*, *input* (optionnellement) et *output*. En résumé, les instructions correspondent à des exemples de prompts que pourraient laisser les utilisateurs du LLM, les *outputs* à la réponse que le LLM devrait générer et les *inputs* apporte des éléments de contextes supplémentaires (par exemple sous la forme de textes cités en exemple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw-XyFFe1tvl",
        "outputId": "e7a2d712-c64b-48aa-ee62-26e8bd0b0b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"instruction\": \"\\u00c9crivez un texte qui d\\u00e9peint un affrontement verbal entre Lao damas et Eurialus, o\\u00f9 ce dernier insulte Ulysse en le traitant de marin incomp\\u00e9tent. Le style utilis\\u00e9 doit \\u00eatre soutenu et empreint d'une certaine gravit\\u00e9.\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Si Lao damas fut content de cette mode\\u017fte repon\\u017feil n'en fut pas de m\\u00eame de quelques -uns des autres qui \\u017fe per\\u017fuadant que s'il \\u017fe defen doit d'entrer en licecela ne provenoit que de \\u017fa foiblefle & de fon peu d'experience dans ces \\u017fortes d'exercices \\u017fe mirent \\u00e0 le prefler d'u ne maniere fort incivile . Eurialus particuliere ment tout boufi de l'honneur qu'il venoit d'ac querir piqua Uli\\u017fle pardes paroles non leule. ment dures mais injurieu\\u017feslc taxant d'e\\u017ftre plus propre \\u00e0 courir la mer \\u017fur quelque vailleau CON\"\n",
            "  },\n",
            "  {\n",
            "    \"instruction\": \"\\u00c9crivez un texte litt\\u00e9raire historique dans le style du XVIIe si\\u00e8cle fran\\u00e7ais, mettant en sc\\u00e8ne les personnages de Te. rentius et Latinus. Le texte doit d\\u00e9buter par \\\"Quoi que Te. rentius n'e\\u00fbt pas l'ameint\\u00e9re\\u017f\\u017f\\u00e9eil accepta la propofition & le \\u017fervit fi 3 bien que \\u017fon Pan\\u00e9gyrique& \\u017fes Hymnes eurent tout le \\u017fucc\\u00e8s qu'il en pouvoit e\\u017fp\\u00e9rer.\\\"\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Quoi que Te. rentius n'e\\u00fbt pas l'ameint\\u00e9re\\u017f\\u017f\\u00e9eil accepta la propofition & le \\u017fervit fi 3 bien que \\u017fon Pan\\u00e9gyrique& \\u017fes Hymnes eurent tout le \\u017fucc\\u00e8s qu'il en pouvoit e\\u017fp\\u00e9rer. Il re\\u00e7\\u00fat \\u017foixane. te pi\\u017ftoles de la Cour& dix des C .... Alors Latinus ne \\u017fe \\u017fouvint plus de fa parole ou pour mieux dire il ne voulut plus la tenir. Ce proc\\u00e9d\\u00e9 \\u017furprit Terentius ; & comme il lui en faifoit quelque reproche il Ora fon bonnetqu'il avoit \\u017fur la t\\u00eate & le jetra de d\\u00e9pit dans le feu. Te rentius le rama\\u017f\\u017fa auffit\\u00f4t & le lui remit \\u017fur la t\\u00eate avec autantderer pect\"\n",
            "  },\n",
            "  {\n",
            "    \"instruction\": \"\\u00c9crivez un texte litt\\u00e9raire dans le style du XVIIe si\\u00e8cle, en utilisant des mots et des expressions anciennes. Le texte doit commencer par la phrase suivante : \\\"Ily en eut me\\u017fme qui ne \\u017fe peurent tenir de dire quelques bons mots de cette pauvre fille : d'autres effrontez conceurent des id\\u00e9es qui offen\\u00e7oient \\u017fa cha\\u017ftet\\u00e9. Mais il arriua par gr\\u00e1d miracle,quetous ceux qui la regardoient auec impuret\\u00e9 qui prenoient de manuai fes pens\\u00e9es en la voyant furent rendus impui\\u017f\\u017fans & tous glacez pour iamais.Ce -\\u017fecret.n'e\\u017ftoitpas connu du \\u017fieur deBau ce dricourt'c'est pourquoy ce prodige ne le perluadoit pas lur la verit\\u00e9 de Leanne luy auoir dit. Au contraire toutes \\u043d\\u0456 cho\\u017fes...\\\"\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Ily en eut me\\u017fme qui ne \\u017fe peurent tenir de dire quelques bons mots de cette pauvre fille : d'autres effrontez conceurent des id\\u00e9es qui offen 1 \\u00e7oient \\u017fa cha\\u017ftet\\u00e9. Mais il arriua par gr\\u00e1d miracle,quetous ceux qui la regardoient auec impuret\\u00e9 qui prenoient de manuai fes pens\\u00e9es en la voyant furent rendus impui\\u017f\\u017fans & tous glacez pour iamais.Ce -\\u017fecret.n'e\\u017ftoitpas connu du \\u017fieur deBau ce dricourt'c'est pourquoy ce prodige ne le perluadoit pas lur la verit\\u00e9 de Leanne luy auoir dit. Au contraire toutes \\u043d\\u0456 cho\\u017fes\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open('french_novel17_instruction_simple.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "json_formatted_str = json.dumps(data[0:3], indent=2)\n",
        "\n",
        "print(json_formatted_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78_DUqdzGrW"
      },
      "source": [
        "# Finetuning du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIgs_i0BNb5"
      },
      "source": [
        "Tout est prêt à lancer le fine-tuning du modèle. Nous allons juste créer le dossier d'accueil des fichiers de fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbdELbxDzIu7"
      },
      "outputs": [],
      "source": [
        "!mkdir llama-7b-novel217"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhMpUgoS6oD"
      },
      "source": [
        "Et nous sommes prêt à lancer la grande commande. Il y a beaucoup de paramètre mais seulement quelqu'uns sont importants :\n",
        "* Nous allons utiliser le modèle Llama-7b de base et leurs poids correspondants (llama-7b-4bit)\n",
        "* Le fine-tuning sera effectué sur le set d'instruction *eu_translate_fr_sample.json* (évidemment à changer si vous optez pour un autre jeu de données).\n",
        "* Les fichiers du modèle seront placés dans le dossier *llama-7b-sample* (de nouveau à changer pour le nom de votre modèle).\n",
        "* Nous ne ferons tourner le fine-tuning que sur une *epoch* ce qui est suffisant pour un premier test.\n",
        "\n",
        "Après avoir lancé le script, Google Colab va tourner pendant un peu moins de 40 minutes.\n",
        "\n",
        "Si tout se passe bien vous verrez défiler le processus d'entraînement avec trois indicateurs régulièrement réactualisés : \"{'loss': 1.8581, 'learning_rate': 0.0002993736951983298, 'epoch': 0.0}\" :\n",
        "* Le \"loss\" c'est en quelque sorte le taux d'erreur du modèle : plus cette mesure est basse et plus le modèle parvient à prédire des textes assez approchants de ceux qui sont présent dans le corpus d'instruction.\n",
        "* Le *learning rate* (taux d'apprentissage) c'est la capacité du modèle à mémoriser de nouveaux éléments mais aussi à en oublier des anciens. Cet indicateur va constamment baisser au fur et à mesure de l'apprentissage.\n",
        "* L'*epoch* c'est le cycle d'apprentissage. Comme nous n'avons défini qu'une *epoch* cela correspondra à des pourcentages (de 0 à 0.99 à la fin de l'entraînement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiUKv07zGSm",
        "outputId": "6bcfb43e-b977-49f2-e4d6-0e839f1e938c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "trainable params: 4194304 || all params: 266604544 || trainable%: 1.5732304997772282\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-768964e92ee08ad9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "100% 1/1 [00:00<00:00, 506.50it/s]\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-768964e92ee08ad9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a7910bc70c173c6d.arrow and /root/.cache/huggingface/datasets/json/default-768964e92ee08ad9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f3067803ad173f5a.arrow\n",
            "2023-07-11 10:25:37.751295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "ParallelMode.NOT_PARALLEL\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, input, output. If instruction, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1,600\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2,400\n",
            "  Number of trainable parameters = 4,194,304\n",
            "{'loss': 3.9483, 'learning_rate': 0.0002993736951983298, 'epoch': 0.01}\n",
            "{'loss': 4.0661, 'learning_rate': 0.0002981210855949895, 'epoch': 0.03}\n",
            "{'loss': 3.4461, 'learning_rate': 0.00029686847599164924, 'epoch': 0.04}\n",
            "{'loss': 3.4287, 'learning_rate': 0.00029561586638830896, 'epoch': 0.05}\n",
            "{'loss': 3.5716, 'learning_rate': 0.0002943632567849687, 'epoch': 0.06}\n",
            "{'loss': 3.454, 'learning_rate': 0.00029311064718162835, 'epoch': 0.07}\n",
            "{'loss': 3.2858, 'learning_rate': 0.00029185803757828807, 'epoch': 0.09}\n",
            "{'loss': 3.3828, 'learning_rate': 0.0002906054279749478, 'epoch': 0.1}\n",
            "{'loss': 3.4739, 'learning_rate': 0.00028935281837160745, 'epoch': 0.11}\n",
            "{'loss': 3.4527, 'learning_rate': 0.00028810020876826717, 'epoch': 0.12}\n",
            "{'loss': 3.5332, 'learning_rate': 0.0002868475991649269, 'epoch': 0.14}\n",
            "{'loss': 3.2102, 'learning_rate': 0.0002855949895615866, 'epoch': 0.15}\n",
            "{'loss': 3.2729, 'learning_rate': 0.0002843423799582463, 'epoch': 0.16}\n",
            "{'loss': 3.2131, 'learning_rate': 0.00028308977035490605, 'epoch': 0.17}\n",
            "{'loss': 3.3243, 'learning_rate': 0.00028183716075156576, 'epoch': 0.19}\n",
            "{'loss': 3.3517, 'learning_rate': 0.00028058455114822543, 'epoch': 0.2}\n",
            "{'loss': 3.3121, 'learning_rate': 0.00027933194154488515, 'epoch': 0.21}\n",
            "{'loss': 3.446, 'learning_rate': 0.00027807933194154487, 'epoch': 0.23}\n",
            "{'loss': 3.0655, 'learning_rate': 0.00027682672233820453, 'epoch': 0.24}\n",
            "{'loss': 3.5133, 'learning_rate': 0.00027557411273486425, 'epoch': 0.25}\n",
            "{'loss': 3.232, 'learning_rate': 0.00027432150313152397, 'epoch': 0.26}\n",
            "{'loss': 3.3326, 'learning_rate': 0.0002730688935281837, 'epoch': 0.28}\n",
            "{'loss': 3.3564, 'learning_rate': 0.0002718162839248434, 'epoch': 0.29}\n",
            "{'loss': 3.1585, 'learning_rate': 0.00027056367432150313, 'epoch': 0.3}\n",
            "{'loss': 3.3359, 'learning_rate': 0.00026931106471816285, 'epoch': 0.31}\n",
            "{'loss': 3.4064, 'learning_rate': 0.0002680584551148225, 'epoch': 0.33}\n",
            "{'loss': 3.458, 'learning_rate': 0.00026680584551148223, 'epoch': 0.34}\n",
            "{'loss': 3.1709, 'learning_rate': 0.00026555323590814195, 'epoch': 0.35}\n",
            "{'loss': 3.3299, 'learning_rate': 0.0002643006263048016, 'epoch': 0.36}\n",
            "{'loss': 3.1736, 'learning_rate': 0.00026304801670146133, 'epoch': 0.38}\n",
            "{'loss': 3.3535, 'learning_rate': 0.00026179540709812105, 'epoch': 0.39}\n",
            "{'loss': 3.2775, 'learning_rate': 0.00026054279749478077, 'epoch': 0.4}\n",
            "{'loss': 3.1088, 'learning_rate': 0.0002592901878914405, 'epoch': 0.41}\n",
            "{'loss': 3.0335, 'learning_rate': 0.0002580375782881002, 'epoch': 0.42}\n",
            "{'loss': 3.2827, 'learning_rate': 0.00025678496868475993, 'epoch': 0.44}\n",
            "{'loss': 3.0199, 'learning_rate': 0.0002555323590814196, 'epoch': 0.45}\n",
            "{'loss': 3.4276, 'learning_rate': 0.0002542797494780793, 'epoch': 0.46}\n",
            "{'loss': 3.3411, 'learning_rate': 0.00025302713987473903, 'epoch': 0.47}\n",
            "{'loss': 2.95, 'learning_rate': 0.0002517745302713987, 'epoch': 0.49}\n",
            "{'loss': 3.1084, 'learning_rate': 0.0002505219206680584, 'epoch': 0.5}\n",
            "{'loss': 3.3305, 'learning_rate': 0.00024926931106471813, 'epoch': 0.51}\n",
            "{'loss': 3.2688, 'learning_rate': 0.00024801670146137785, 'epoch': 0.53}\n",
            "{'loss': 3.4043, 'learning_rate': 0.00024676409185803757, 'epoch': 0.54}\n",
            "{'loss': 3.0844, 'learning_rate': 0.0002455114822546973, 'epoch': 0.55}\n",
            "{'loss': 3.2957, 'learning_rate': 0.000244258872651357, 'epoch': 0.56}\n",
            "{'loss': 3.2208, 'learning_rate': 0.00024300626304801667, 'epoch': 0.57}\n",
            "{'loss': 3.1153, 'learning_rate': 0.0002417536534446764, 'epoch': 0.59}\n",
            "{'loss': 3.3602, 'learning_rate': 0.0002405010438413361, 'epoch': 0.6}\n",
            "{'loss': 3.1875, 'learning_rate': 0.00023924843423799578, 'epoch': 0.61}\n",
            "{'loss': 3.4321, 'learning_rate': 0.0002379958246346555, 'epoch': 0.62}\n",
            " 21% 500/2400 [11:45<44:32,  1.41s/it]Saving model checkpoint to llama-7b-novel217/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 2.9322, 'learning_rate': 0.00023674321503131522, 'epoch': 0.64}\n",
            "{'loss': 3.427, 'learning_rate': 0.00023549060542797493, 'epoch': 0.65}\n",
            "{'loss': 3.2573, 'learning_rate': 0.00023423799582463463, 'epoch': 0.66}\n",
            "{'loss': 3.2076, 'learning_rate': 0.00023298538622129435, 'epoch': 0.68}\n",
            "{'loss': 3.024, 'learning_rate': 0.00023173277661795406, 'epoch': 0.69}\n",
            "{'loss': 3.1067, 'learning_rate': 0.00023048016701461376, 'epoch': 0.7}\n",
            "{'loss': 3.111, 'learning_rate': 0.00022922755741127348, 'epoch': 0.71}\n",
            "{'loss': 3.1441, 'learning_rate': 0.0002279749478079332, 'epoch': 0.72}\n",
            "{'loss': 2.9836, 'learning_rate': 0.00022672233820459286, 'epoch': 0.74}\n",
            "{'loss': 2.8011, 'learning_rate': 0.00022546972860125258, 'epoch': 0.75}\n",
            "{'loss': 3.1263, 'learning_rate': 0.0002242171189979123, 'epoch': 0.76}\n",
            "{'loss': 3.0295, 'learning_rate': 0.00022296450939457202, 'epoch': 0.78}\n",
            "{'loss': 3.1642, 'learning_rate': 0.0002217118997912317, 'epoch': 0.79}\n",
            "{'loss': 3.1412, 'learning_rate': 0.00022045929018789143, 'epoch': 0.8}\n",
            "{'loss': 3.2641, 'learning_rate': 0.00021920668058455115, 'epoch': 0.81}\n",
            "{'loss': 3.2775, 'learning_rate': 0.00021795407098121084, 'epoch': 0.82}\n",
            "{'loss': 3.1992, 'learning_rate': 0.00021670146137787056, 'epoch': 0.84}\n",
            "{'loss': 3.2615, 'learning_rate': 0.00021544885177453025, 'epoch': 0.85}\n",
            "{'loss': 2.9477, 'learning_rate': 0.00021419624217118994, 'epoch': 0.86}\n",
            "{'loss': 2.9472, 'learning_rate': 0.00021294363256784966, 'epoch': 0.88}\n",
            "{'loss': 3.0574, 'learning_rate': 0.00021169102296450938, 'epoch': 0.89}\n",
            "{'loss': 3.081, 'learning_rate': 0.0002104384133611691, 'epoch': 0.9}\n",
            "{'loss': 2.9786, 'learning_rate': 0.0002091858037578288, 'epoch': 0.91}\n",
            "{'loss': 3.1604, 'learning_rate': 0.0002079331941544885, 'epoch': 0.93}\n",
            "{'loss': 3.0117, 'learning_rate': 0.00020668058455114823, 'epoch': 0.94}\n",
            "{'loss': 3.1627, 'learning_rate': 0.0002054279749478079, 'epoch': 0.95}\n",
            "{'loss': 3.2203, 'learning_rate': 0.0002041753653444676, 'epoch': 0.96}\n",
            "{'loss': 3.2472, 'learning_rate': 0.00020292275574112733, 'epoch': 0.97}\n",
            "{'loss': 2.9188, 'learning_rate': 0.00020167014613778702, 'epoch': 0.99}\n",
            "{'loss': 2.9932, 'learning_rate': 0.00020041753653444674, 'epoch': 1.0}\n",
            "{'loss': 2.8637, 'learning_rate': 0.00019916492693110646, 'epoch': 1.01}\n",
            "{'loss': 2.8724, 'learning_rate': 0.00019791231732776615, 'epoch': 1.02}\n",
            "{'loss': 3.0311, 'learning_rate': 0.00019665970772442587, 'epoch': 1.04}\n",
            "{'loss': 3.1002, 'learning_rate': 0.0001954070981210856, 'epoch': 1.05}\n",
            "{'loss': 3.0111, 'learning_rate': 0.0001941544885177453, 'epoch': 1.06}\n",
            "{'loss': 2.8223, 'learning_rate': 0.00019290187891440498, 'epoch': 1.07}\n",
            "{'loss': 2.9348, 'learning_rate': 0.0001916492693110647, 'epoch': 1.09}\n",
            "{'loss': 2.9492, 'learning_rate': 0.00019039665970772441, 'epoch': 1.1}\n",
            "{'loss': 3.0092, 'learning_rate': 0.0001891440501043841, 'epoch': 1.11}\n",
            "{'loss': 2.9189, 'learning_rate': 0.00018789144050104382, 'epoch': 1.12}\n",
            "{'loss': 2.8774, 'learning_rate': 0.00018663883089770354, 'epoch': 1.14}\n",
            "{'loss': 2.8896, 'learning_rate': 0.00018538622129436324, 'epoch': 1.15}\n",
            "{'loss': 3.0565, 'learning_rate': 0.00018413361169102295, 'epoch': 1.16}\n",
            "{'loss': 3.049, 'learning_rate': 0.00018288100208768265, 'epoch': 1.18}\n",
            "{'loss': 2.838, 'learning_rate': 0.00018162839248434237, 'epoch': 1.19}\n",
            "{'loss': 3.0093, 'learning_rate': 0.00018037578288100206, 'epoch': 1.2}\n",
            "{'loss': 3.0175, 'learning_rate': 0.00017912317327766178, 'epoch': 1.21}\n",
            "{'loss': 2.7418, 'learning_rate': 0.0001778705636743215, 'epoch': 1.23}\n",
            "{'loss': 3.112, 'learning_rate': 0.0001766179540709812, 'epoch': 1.24}\n",
            "{'loss': 2.8882, 'learning_rate': 0.0001753653444676409, 'epoch': 1.25}\n",
            " 42% 1000/2400 [23:42<32:51,  1.41s/it]Saving model checkpoint to llama-7b-novel217/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 2.8486, 'learning_rate': 0.00017411273486430063, 'epoch': 1.26}\n",
            "{'loss': 2.9576, 'learning_rate': 0.0001728601252609603, 'epoch': 1.27}\n",
            "{'loss': 2.6993, 'learning_rate': 0.00017160751565762, 'epoch': 1.29}\n",
            "{'loss': 2.8683, 'learning_rate': 0.00017035490605427973, 'epoch': 1.3}\n",
            "{'loss': 3.0221, 'learning_rate': 0.00016910229645093945, 'epoch': 1.31}\n",
            "{'loss': 2.994, 'learning_rate': 0.00016784968684759914, 'epoch': 1.32}\n",
            "{'loss': 3.1205, 'learning_rate': 0.00016659707724425886, 'epoch': 1.34}\n",
            "{'loss': 3.0386, 'learning_rate': 0.00016534446764091858, 'epoch': 1.35}\n",
            "{'loss': 3.1212, 'learning_rate': 0.00016409185803757827, 'epoch': 1.36}\n",
            "{'loss': 2.8529, 'learning_rate': 0.000162839248434238, 'epoch': 1.38}\n",
            "{'loss': 2.8281, 'learning_rate': 0.0001615866388308977, 'epoch': 1.39}\n",
            "{'loss': 3.0736, 'learning_rate': 0.00016033402922755737, 'epoch': 1.4}\n",
            "{'loss': 2.8544, 'learning_rate': 0.0001590814196242171, 'epoch': 1.41}\n",
            "{'loss': 2.9384, 'learning_rate': 0.0001578288100208768, 'epoch': 1.43}\n",
            "{'loss': 3.0045, 'learning_rate': 0.00015657620041753653, 'epoch': 1.44}\n",
            "{'loss': 2.823, 'learning_rate': 0.00015532359081419622, 'epoch': 1.45}\n",
            "{'loss': 3.3334, 'learning_rate': 0.00015407098121085594, 'epoch': 1.46}\n",
            "{'loss': 2.9699, 'learning_rate': 0.00015281837160751566, 'epoch': 1.48}\n",
            "{'loss': 3.2368, 'learning_rate': 0.00015156576200417535, 'epoch': 1.49}\n",
            "{'loss': 3.175, 'learning_rate': 0.00015031315240083507, 'epoch': 1.5}\n",
            "{'loss': 3.0292, 'learning_rate': 0.00014906054279749476, 'epoch': 1.51}\n",
            "{'loss': 2.7178, 'learning_rate': 0.00014780793319415448, 'epoch': 1.52}\n",
            "{'loss': 2.9993, 'learning_rate': 0.00014655532359081417, 'epoch': 1.54}\n",
            "{'loss': 3.0779, 'learning_rate': 0.0001453027139874739, 'epoch': 1.55}\n",
            "{'loss': 2.9603, 'learning_rate': 0.00014405010438413358, 'epoch': 1.56}\n",
            "{'loss': 3.0104, 'learning_rate': 0.0001427974947807933, 'epoch': 1.57}\n",
            "{'loss': 2.9003, 'learning_rate': 0.00014154488517745302, 'epoch': 1.59}\n",
            "{'loss': 2.8265, 'learning_rate': 0.00014029227557411271, 'epoch': 1.6}\n",
            "{'loss': 2.8297, 'learning_rate': 0.00013903966597077243, 'epoch': 1.61}\n",
            "{'loss': 2.8358, 'learning_rate': 0.00013778705636743213, 'epoch': 1.62}\n",
            "{'loss': 2.8323, 'learning_rate': 0.00013653444676409184, 'epoch': 1.64}\n",
            "{'loss': 3.1429, 'learning_rate': 0.00013528183716075156, 'epoch': 1.65}\n",
            "{'loss': 3.0375, 'learning_rate': 0.00013402922755741126, 'epoch': 1.66}\n",
            "{'loss': 2.9727, 'learning_rate': 0.00013277661795407097, 'epoch': 1.68}\n",
            "{'loss': 3.0554, 'learning_rate': 0.00013152400835073067, 'epoch': 1.69}\n",
            "{'loss': 3.2828, 'learning_rate': 0.00013027139874739039, 'epoch': 1.7}\n",
            "{'loss': 2.8485, 'learning_rate': 0.0001290187891440501, 'epoch': 1.71}\n",
            "{'loss': 2.9994, 'learning_rate': 0.0001277661795407098, 'epoch': 1.73}\n",
            "{'loss': 2.6906, 'learning_rate': 0.00012651356993736952, 'epoch': 1.74}\n",
            "{'loss': 3.0165, 'learning_rate': 0.0001252609603340292, 'epoch': 1.75}\n",
            "{'loss': 2.9173, 'learning_rate': 0.00012400835073068893, 'epoch': 1.76}\n",
            "{'loss': 2.823, 'learning_rate': 0.00012275574112734865, 'epoch': 1.77}\n",
            "{'loss': 2.8232, 'learning_rate': 0.00012150313152400834, 'epoch': 1.79}\n",
            "{'loss': 3.0539, 'learning_rate': 0.00012025052192066806, 'epoch': 1.8}\n",
            "{'loss': 2.9783, 'learning_rate': 0.00011899791231732775, 'epoch': 1.81}\n",
            "{'loss': 2.6278, 'learning_rate': 0.00011774530271398747, 'epoch': 1.82}\n",
            "{'loss': 3.026, 'learning_rate': 0.00011649269311064717, 'epoch': 1.84}\n",
            "{'loss': 3.1864, 'learning_rate': 0.00011524008350730688, 'epoch': 1.85}\n",
            "{'loss': 2.849, 'learning_rate': 0.0001139874739039666, 'epoch': 1.86}\n",
            "{'loss': 3.1017, 'learning_rate': 0.00011273486430062629, 'epoch': 1.88}\n",
            " 62% 1500/2400 [35:39<21:05,  1.41s/it]Saving model checkpoint to llama-7b-novel217/checkpoint-1500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 2.7975, 'learning_rate': 0.00011148225469728601, 'epoch': 1.89}\n",
            "{'loss': 2.8229, 'learning_rate': 0.00011022964509394571, 'epoch': 1.9}\n",
            "{'loss': 2.9764, 'learning_rate': 0.00010897703549060542, 'epoch': 1.91}\n",
            "{'loss': 2.9477, 'learning_rate': 0.00010772442588726512, 'epoch': 1.93}\n",
            "{'loss': 2.8764, 'learning_rate': 0.00010647181628392483, 'epoch': 1.94}\n",
            "{'loss': 3.0537, 'learning_rate': 0.00010521920668058455, 'epoch': 1.95}\n",
            "{'loss': 2.859, 'learning_rate': 0.00010396659707724425, 'epoch': 1.96}\n",
            "{'loss': 2.8905, 'learning_rate': 0.00010271398747390395, 'epoch': 1.98}\n",
            "{'loss': 2.9037, 'learning_rate': 0.00010146137787056367, 'epoch': 1.99}\n",
            "{'loss': 3.0232, 'learning_rate': 0.00010020876826722337, 'epoch': 2.0}\n",
            "{'loss': 2.8732, 'learning_rate': 9.895615866388308e-05, 'epoch': 2.01}\n",
            "{'loss': 2.8926, 'learning_rate': 9.77035490605428e-05, 'epoch': 2.02}\n",
            "{'loss': 2.9161, 'learning_rate': 9.645093945720249e-05, 'epoch': 2.04}\n",
            "{'loss': 2.7101, 'learning_rate': 9.519832985386221e-05, 'epoch': 2.05}\n",
            "{'loss': 2.8809, 'learning_rate': 9.394572025052191e-05, 'epoch': 2.06}\n",
            "{'loss': 2.8342, 'learning_rate': 9.269311064718162e-05, 'epoch': 2.08}\n",
            "{'loss': 2.8135, 'learning_rate': 9.144050104384132e-05, 'epoch': 2.09}\n",
            "{'loss': 2.7259, 'learning_rate': 9.018789144050103e-05, 'epoch': 2.1}\n",
            "{'loss': 2.8088, 'learning_rate': 8.893528183716075e-05, 'epoch': 2.11}\n",
            "{'loss': 2.6861, 'learning_rate': 8.768267223382045e-05, 'epoch': 2.12}\n",
            "{'loss': 2.8374, 'learning_rate': 8.643006263048015e-05, 'epoch': 2.14}\n",
            "{'loss': 2.7215, 'learning_rate': 8.517745302713986e-05, 'epoch': 2.15}\n",
            "{'loss': 2.8835, 'learning_rate': 8.392484342379957e-05, 'epoch': 2.16}\n",
            "{'loss': 2.8055, 'learning_rate': 8.267223382045929e-05, 'epoch': 2.17}\n",
            "{'loss': 2.7899, 'learning_rate': 8.1419624217119e-05, 'epoch': 2.19}\n",
            "{'loss': 3.0265, 'learning_rate': 8.016701461377869e-05, 'epoch': 2.2}\n",
            "{'loss': 2.7917, 'learning_rate': 7.89144050104384e-05, 'epoch': 2.21}\n",
            "{'loss': 2.7267, 'learning_rate': 7.766179540709811e-05, 'epoch': 2.23}\n",
            "{'loss': 2.7569, 'learning_rate': 7.640918580375783e-05, 'epoch': 2.24}\n",
            "{'loss': 2.5939, 'learning_rate': 7.515657620041754e-05, 'epoch': 2.25}\n",
            "{'loss': 2.7682, 'learning_rate': 7.390396659707724e-05, 'epoch': 2.26}\n",
            "{'loss': 2.9608, 'learning_rate': 7.265135699373695e-05, 'epoch': 2.27}\n",
            "{'loss': 2.6521, 'learning_rate': 7.139874739039665e-05, 'epoch': 2.29}\n",
            "{'loss': 2.7989, 'learning_rate': 7.014613778705636e-05, 'epoch': 2.3}\n",
            "{'loss': 2.8802, 'learning_rate': 6.889352818371606e-05, 'epoch': 2.31}\n",
            "{'loss': 2.6784, 'learning_rate': 6.764091858037578e-05, 'epoch': 2.33}\n",
            "{'loss': 2.8404, 'learning_rate': 6.638830897703549e-05, 'epoch': 2.34}\n",
            "{'loss': 2.9888, 'learning_rate': 6.513569937369519e-05, 'epoch': 2.35}\n",
            "{'loss': 2.6782, 'learning_rate': 6.38830897703549e-05, 'epoch': 2.36}\n",
            "{'loss': 2.6355, 'learning_rate': 6.26304801670146e-05, 'epoch': 2.38}\n",
            "{'loss': 2.526, 'learning_rate': 6.137787056367432e-05, 'epoch': 2.39}\n",
            "{'loss': 2.6799, 'learning_rate': 6.012526096033403e-05, 'epoch': 2.4}\n",
            "{'loss': 2.8022, 'learning_rate': 5.8872651356993734e-05, 'epoch': 2.41}\n",
            "{'loss': 2.8776, 'learning_rate': 5.762004175365344e-05, 'epoch': 2.42}\n",
            "{'loss': 2.8023, 'learning_rate': 5.6367432150313145e-05, 'epoch': 2.44}\n",
            "{'loss': 3.0305, 'learning_rate': 5.511482254697286e-05, 'epoch': 2.45}\n",
            "{'loss': 2.8395, 'learning_rate': 5.386221294363256e-05, 'epoch': 2.46}\n",
            "{'loss': 2.7293, 'learning_rate': 5.2609603340292275e-05, 'epoch': 2.48}\n",
            "{'loss': 2.6989, 'learning_rate': 5.1356993736951973e-05, 'epoch': 2.49}\n",
            "{'loss': 2.8529, 'learning_rate': 5.0104384133611686e-05, 'epoch': 2.5}\n",
            " 83% 2000/2400 [47:36<09:23,  1.41s/it]Saving model checkpoint to llama-7b-novel217/checkpoint-2000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [llama-7b-novel217/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 2.9197, 'learning_rate': 4.88517745302714e-05, 'epoch': 2.51}\n",
            "{'loss': 2.8314, 'learning_rate': 4.7599164926931103e-05, 'epoch': 2.52}\n",
            "{'loss': 2.8117, 'learning_rate': 4.634655532359081e-05, 'epoch': 2.54}\n",
            "{'loss': 2.6611, 'learning_rate': 4.5093945720250514e-05, 'epoch': 2.55}\n",
            "{'loss': 2.8843, 'learning_rate': 4.384133611691023e-05, 'epoch': 2.56}\n",
            "{'loss': 3.0761, 'learning_rate': 4.258872651356993e-05, 'epoch': 2.58}\n",
            "{'loss': 2.5981, 'learning_rate': 4.1336116910229644e-05, 'epoch': 2.59}\n",
            "{'loss': 2.8108, 'learning_rate': 4.008350730688934e-05, 'epoch': 2.6}\n",
            "{'loss': 2.7935, 'learning_rate': 3.8830897703549055e-05, 'epoch': 2.61}\n",
            "{'loss': 2.7094, 'learning_rate': 3.757828810020877e-05, 'epoch': 2.62}\n",
            "{'loss': 2.7664, 'learning_rate': 3.632567849686847e-05, 'epoch': 2.64}\n",
            "{'loss': 2.6822, 'learning_rate': 3.507306889352818e-05, 'epoch': 2.65}\n",
            "{'loss': 2.9969, 'learning_rate': 3.382045929018789e-05, 'epoch': 2.66}\n",
            "{'loss': 2.6644, 'learning_rate': 3.2567849686847596e-05, 'epoch': 2.67}\n",
            "{'loss': 2.813, 'learning_rate': 3.13152400835073e-05, 'epoch': 2.69}\n",
            "{'loss': 2.8789, 'learning_rate': 3.0062630480167014e-05, 'epoch': 2.7}\n",
            "{'loss': 2.6774, 'learning_rate': 2.881002087682672e-05, 'epoch': 2.71}\n",
            "{'loss': 2.8969, 'learning_rate': 2.755741127348643e-05, 'epoch': 2.73}\n",
            "{'loss': 2.5742, 'learning_rate': 2.6304801670146137e-05, 'epoch': 2.74}\n",
            "{'loss': 3.2396, 'learning_rate': 2.5052192066805843e-05, 'epoch': 2.75}\n",
            "{'loss': 2.7892, 'learning_rate': 2.3799582463465552e-05, 'epoch': 2.76}\n",
            "{'loss': 2.7847, 'learning_rate': 2.2546972860125257e-05, 'epoch': 2.77}\n",
            "{'loss': 2.9218, 'learning_rate': 2.1294363256784966e-05, 'epoch': 2.79}\n",
            "{'loss': 2.6052, 'learning_rate': 2.004175365344467e-05, 'epoch': 2.8}\n",
            "{'loss': 2.7108, 'learning_rate': 1.8789144050104384e-05, 'epoch': 2.81}\n",
            "{'loss': 2.7595, 'learning_rate': 1.753653444676409e-05, 'epoch': 2.83}\n",
            "{'loss': 2.7511, 'learning_rate': 1.6283924843423798e-05, 'epoch': 2.84}\n",
            "{'loss': 2.9082, 'learning_rate': 1.5031315240083507e-05, 'epoch': 2.85}\n",
            "{'loss': 2.5379, 'learning_rate': 1.3778705636743214e-05, 'epoch': 2.86}\n",
            "{'loss': 2.816, 'learning_rate': 1.2526096033402921e-05, 'epoch': 2.88}\n",
            "{'loss': 2.5347, 'learning_rate': 1.1273486430062629e-05, 'epoch': 2.89}\n",
            "{'loss': 2.6511, 'learning_rate': 1.0020876826722336e-05, 'epoch': 2.9}\n",
            "{'loss': 2.4923, 'learning_rate': 8.768267223382045e-06, 'epoch': 2.91}\n",
            "{'loss': 2.9037, 'learning_rate': 7.5156576200417535e-06, 'epoch': 2.92}\n",
            "{'loss': 2.8371, 'learning_rate': 6.263048016701461e-06, 'epoch': 2.94}\n",
            "{'loss': 2.6788, 'learning_rate': 5.010438413361168e-06, 'epoch': 2.95}\n",
            "{'loss': 2.9681, 'learning_rate': 3.7578288100208768e-06, 'epoch': 2.96}\n",
            "{'loss': 2.9132, 'learning_rate': 2.505219206680584e-06, 'epoch': 2.98}\n",
            "{'loss': 2.9215, 'learning_rate': 1.252609603340292e-06, 'epoch': 2.99}\n",
            "{'loss': 2.7601, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 2400/2400 [57:15<00:00,  1.41s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3435.279, 'train_samples_per_second': 1.397, 'train_steps_per_second': 0.699, 'train_loss': 2.996564262708028, 'epoch': 3.0}\n",
            "100% 2400/2400 [57:15<00:00,  1.43s/it]\n"
          ]
        }
      ],
      "source": [
        "!llmtune finetune \\\n",
        "    --model llama-7b-4bit \\\n",
        "    --lr=3e-4 \\\n",
        "    --epochs 3 \\\n",
        "    --save_total_limit 3 \\\n",
        "    --save_steps 500 \\\n",
        "    --weights llama-7b-4bit.pt \\\n",
        "    --adapter llama-7b-novel217 \\\n",
        "    --mbatch_size=1 \\\n",
        "    --batch_size=2 \\\n",
        "    --epochs=3 \\\n",
        "    --lora_r=8 \\\n",
        "    --lora_alpha=16 \\\n",
        "    --lora_dropout=0.05 \\\n",
        "    --warmup_steps=5 \\\n",
        "    --dataset french_novel17_instruction_simple.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov3RiDxqWvuP",
        "outputId": "0c93e181-9220-41e7-d5ed-c95497d96371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build\t\t\t\t   llama-7b-french-novel  README.md\n",
            "dist\t\t\t\t   llmtune\t\t  requirements.txt\n",
            "instruction_french_novel.json\t   llmtune.egg-info\t  setup.py\n",
            "instruction_french_novel.json.zip  __MACOSX\n",
            "llama-7b-4bit.pt\t\t   quant_cuda.egg-info\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_84CU2sE46s"
      },
      "source": [
        "Si tout se passe bien, vous devrez avoir comlètement fini l'entraînement.\n",
        "\n",
        "Après un petit temps de synchronisation entre Google Colab, vous allez voir apparaître deux fichiers dans le dossier du modèle de fine-tuning : adapter_model.bin (le modèle proprepement dit) et adapter_model.config (un fichier de configuration). À noter que le modèle de fine-tuning est considérablement plus petit que le modèle d'origine : c'est en quelque sorte un modèle complémentaire qui vient ajuster le LLM (et il en aura toujours besoin pour fonctionner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aKvXziTziXY"
      },
      "source": [
        "# Générer du texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPV1WuvEvhD"
      },
      "source": [
        "Et maintenant il est possible de générer du texte. La fonction par défaut de llmtune n'est pas pour l'instant pas très pratique mais cela devrait s'améliorer prochainement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L_qtyiFzgv2",
        "outputId": "67607bc4-ccd7-4110-d31c-f60f8e5405bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "llama-7b-novel217 loaded\n",
            "2023-07-11 11:29:33.244721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Les Lamas des provinces peruanes ſont d'une finance inſtuſe & d'une mœurs des plus civilizées: Ils ont des habitudes qui prê. ont les femmes plus de ſerené qu'elles ne le font à terre : La majorité de ces peuples ſont catholiques & leur pallieu leſon ne doit jamais auoir été troublé par les religions novembres ; il eſt de ces choſes qui n'ont été jamais in. convenables pour l'Amérique. Ils ont des tetres & des langages perfeitement differenfles</s>AXI i & du Roi de Naples auſſi bien qu'au Roi de France. Les plus fins de ce peuple qui demeurent à Parigo. ont une civilité & une maniere de porter la perruque qui eſt une école. 159. De tous les animaux des Indes ce ne ſont pas ces Lama qui paroiſt: les plus sages . Nous avons d'ailleurs vus qu'ils pouffent l'un l'autre des couleurs d'une fureur fi extraordinaire & qu'ils entretiennent une vive haine pour les m\n"
          ]
        }
      ],
      "source": [
        "!llmtune generate \\\n",
        "    --model llama-7b-4bit \\\n",
        "    --weights llama-7b-4bit.pt \\\n",
        "    --adapter llama-7b-novel217 \\\n",
        "    --max-length 400 \\\n",
        "    --min-length 300 \\\n",
        "    --instruction \"Écrivez un texte littéraire dans le style du XVIIe siècle décrivant un peuple de lamas intelligents au Pérou. Vous pouvez notamment évoquer les mœurs et coutumes de ces animaux très savants. ###OUTPUT\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
