{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXa7DOX84GG"
      },
      "source": [
        "# **Tutoriel** - Fine-tuning de Llama\n",
        "\n",
        "<img src=\"https://voicebot.ai/wp-content/uploads/2023/03/stanford-alpaca.png\" alt=\"Falcon logo\"  width=\"500\"/>\n",
        "\n",
        "Ce carnet de code permet d'effectuer le fine-tuning d'un grand modèle de langue développé par Meta, Llama. Tout en n'étant pas diffusé en open source mais seulement à des fins de recherche non-commercial, Llama a eu un impact significatif sur le développement de LLMs ouverts alternatifs à chatGPT. Une grande partie des méthodes et des jeux de données utilisés pour adapter les LLMs ont été d'abord pensés pour LLama.\n",
        "\n",
        "LLama est disponible sous quatre versions à 7B, 13B, 33B and 65B : le B correspond ici à un milliard de paramètres. Nous allons ici utiliser la plus grande version compatible avec Google Colab (13B).\n",
        "\n",
        "Ce carnet de code est basé sur une version modifiée de LLMTune. LLMTune est un projet de recherche de Cornell Tech et de Cornell University (Cornell University). L'exécution de la version originale sur Google Colab donne lieu à plusieurs bugs que j'ai corrigés.\n",
        "\n",
        "Cette démonstration ne fait tourner qu'une seule *epoch* ce qui est suffisant pour avoir un premier aperçu. Pour obtenir un bon modèle, il est conseillé de faire tourner le fine-tuning pendant trois *epochs*. Sur notre corpus de démonstration de 2000 instructions une *epoch* prendra environ 1h15."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTyUYSsywxD"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kamSkzAq9KE5"
      },
      "source": [
        "En tout premier lieu nous vérifions si nous disposons de suffisamment de mémoire vive (au moins 24go) sinon ce n'est pas la peine de lancer le script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwOy3-Dl9iRW",
        "outputId": "6eb8b90e-e143-49f2-b18a-55df4e34b560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun 11 14:57:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    40W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgZdKVNFIaXm"
      },
      "source": [
        "Une telle configuration est suffisante pour le fine-tuning de llama-7b-4bit basé sur LLMTune (mais pas pour llama-13b-4bit, sans même parler des versions suivantes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5sPY1BeIbRn",
        "outputId": "5a7898a3-15e7-4d38-ae3c-fb9ded1c83d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/llama\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/My Drive/llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6nmahaFHKEK"
      },
      "source": [
        "Nous allons utiliser llmtune. C'est une petite application python disponible sur Github qui permet d'effectuer le fine-tuning de Llama (pour l'instant depuis mon fork comme il y a un bug sur la version de base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyFPmAByvhV",
        "outputId": "cec0e48c-5cc8-4a2b-aa72-1f0d54e4ae17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llmtune'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 136 (delta 51), reused 36 (delta 29), pack-reused 54\u001b[K\n",
            "Receiving objects: 100% (136/136), 45.38 KiB | 3.24 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Pclanglais/llmtune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HkbzY3GxaaC"
      },
      "source": [
        "Nous installons également les dépendances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PG26TNLxmVc",
        "outputId": "77b5de19-c2b7-4eaf-c540-2f736870f88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llama/llmtune\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Collecting transformers (from -r requirements.txt (line 4))\n",
            "  Using cached transformers-4.28.0-py3-none-any.whl\n",
            "Collecting peft (from -r requirements.txt (line 5))\n",
            "  Using cached peft-0.3.0.dev0-py3-none-any.whl\n",
            "Requirement already satisfied: torch==1.13.1+cu116 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1+cu116)\n",
            "Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.97)\n",
            "Requirement already satisfied: datasets==2.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu116->-r requirements.txt (line 2)) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 6)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (0.13.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 6)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 6)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 6)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 6)) (1.16.0)\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing llmtune.egg-info/PKG-INFO\n",
            "writing dependency_links to llmtune.egg-info/dependency_links.txt\n",
            "writing entry points to llmtune.egg-info/entry_points.txt\n",
            "writing top-level names to llmtune.egg-info/top_level.txt\n",
            "reading manifest file 'llmtune.egg-info/SOURCES.txt'\n",
            "writing manifest file 'llmtune.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying llmtune/engine/lora/peft.py -> build/lib/llmtune/engine/lora\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/executor.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/__init__.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/config.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/utils.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "copying build/lib/llmtune/run.py -> build/bdist.linux-x86_64/egg/llmtune\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine\n",
            "copying build/lib/llmtune/engine/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/calibration.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/abstract.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/text.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/alpaca.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "copying build/lib/llmtune/engine/data/gpt4all.py -> build/bdist.linux-x86_64/egg/llmtune/engine/data\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/lora.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/peft.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/utils.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/config.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "copying build/lib/llmtune/engine/lora/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/lora\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "copying build/lib/llmtune/engine/quant/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "copying build/lib/llmtune/engine/quant/modules.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "copying build/lib/llmtune/engine/quant/converter.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/executor.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/gptq.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "copying build/lib/llmtune/engine/quant/algorithm/quantizer.py -> build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/llms\n",
            "copying build/lib/llmtune/llms/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/llms\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "copying build/lib/llmtune/llms/llama/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "copying build/lib/llmtune/llms/llama/config.py -> build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "copying build/lib/llmtune/llms/llama/model.py -> build/bdist.linux-x86_64/egg/llmtune/llms/llama\n",
            "creating build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "copying build/lib/llmtune/llms/opt/model.py -> build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "copying build/lib/llmtune/llms/opt/__init__.py -> build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "copying build/lib/llmtune/llms/opt/config.py -> build/bdist.linux-x86_64/egg/llmtune/llms/opt\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/executor.py to executor.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/run.py to run.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/calibration.py to calibration.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/abstract.py to abstract.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/text.py to text.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/alpaca.py to alpaca.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/data/gpt4all.py to gpt4all.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/lora.py to lora.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/peft.py to peft.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/lora/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/modules.py to modules.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/converter.py to converter.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/executor.py to executor.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/gptq.py to gptq.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/engine/quant/algorithm/quantizer.py to quantizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/llama/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/llama/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/llama/model.py to model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/opt/model.py to model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/opt/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/llmtune/llms/opt/config.py to config.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying llmtune.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/llmtune-0.1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing llmtune-0.1.0-py3.10.egg\n",
            "Removing /usr/local/lib/python3.10/dist-packages/llmtune-0.1.0-py3.10.egg\n",
            "Copying llmtune-0.1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "llmtune 0.1.0 is already the active version in easy-install.pth\n",
            "Installing llmtune script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/llmtune-0.1.0-py3.10.egg\n",
            "Processing dependencies for llmtune==0.1.0\n",
            "Finished processing dependencies for llmtune==0.1.0\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing quant_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to quant_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to quant_cuda.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.8) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:397: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for quant_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.quant_cuda.cpython-310: module references __file__\n",
            "creating 'dist/quant_cuda-0.0.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "Extracting quant_cuda-0.0.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "quant-cuda 0.0.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for quant-cuda==0.0.0\n",
            "Finished processing dependencies for quant-cuda==0.0.0\n"
          ]
        }
      ],
      "source": [
        "%cd llmtune\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-bb8mGdxnaT"
      },
      "source": [
        "Enfin nous récupérons le modèle de base de Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhA3yI5Oxs7x",
        "outputId": "a0633e0f-38fa-475e-dfdb-016cc32b00be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-10 18:05:52--  https://huggingface.co/kuleshov/llama-13b-4bit/resolve/main/llama-13b-4bit.pt\n",
            "Resolving huggingface.co (huggingface.co)... 13.224.249.43, 13.224.249.10, 13.224.249.44, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.224.249.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/5d/fe/5dfe252b94712f657ec0467f7dbddf7852ea12f46ffac06e5efd8b771608707b/56956757a1cf930a1881e7ade7bc4cdd243d88517b7d82cb06c01f11463caf38?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-13b-4bit.pt%3B+filename%3D%22llama-13b-4bit.pt%22%3B&Expires=1689271552&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTI3MTU1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81ZC9mZS81ZGZlMjUyYjk0NzEyZjY1N2VjMDQ2N2Y3ZGJkZGY3ODUyZWExMmY0NmZmYWMwNmU1ZWZkOGI3NzE2MDg3MDdiLzU2OTU2NzU3YTFjZjkzMGExODgxZTdhZGU3YmM0Y2RkMjQzZDg4NTE3YjdkODJjYjA2YzAxZjExNDYzY2FmMzg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=fIrH3rO73-2FP4lx9Dz6OHF4QSMOpHEgTjzBJ17%7E5sbkea1PYq6WESnnGj98jT4xXBReGcmUqM74LvG04wn-sfQJVjI8bscvl44A9q5RwI75n87zB6FlYXoZYZgbzdkCpLyi3Xtse64%7E%7Ey-72R6qXDXga0JxXEeiIr-gS4R74Ap2f9-X24vaj0dPww3BcsGnBnJxTnVKpbh8CTwje0x0uX%7EjYIbJV55ElAEByUzHXbggEB5mFUJltb7ALtQzikUcMH0PBiCBgpAalJllD9MgqGpTjnzgm1yZofL0cjC6XFpYMXN-ZBfxQUOfc2-TL6%7EebwGfVlnrC1bD6f9AGBYKSw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-07-10 18:05:52--  https://cdn-lfs.huggingface.co/repos/5d/fe/5dfe252b94712f657ec0467f7dbddf7852ea12f46ffac06e5efd8b771608707b/56956757a1cf930a1881e7ade7bc4cdd243d88517b7d82cb06c01f11463caf38?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-13b-4bit.pt%3B+filename%3D%22llama-13b-4bit.pt%22%3B&Expires=1689271552&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTI3MTU1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81ZC9mZS81ZGZlMjUyYjk0NzEyZjY1N2VjMDQ2N2Y3ZGJkZGY3ODUyZWExMmY0NmZmYWMwNmU1ZWZkOGI3NzE2MDg3MDdiLzU2OTU2NzU3YTFjZjkzMGExODgxZTdhZGU3YmM0Y2RkMjQzZDg4NTE3YjdkODJjYjA2YzAxZjExNDYzY2FmMzg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=fIrH3rO73-2FP4lx9Dz6OHF4QSMOpHEgTjzBJ17%7E5sbkea1PYq6WESnnGj98jT4xXBReGcmUqM74LvG04wn-sfQJVjI8bscvl44A9q5RwI75n87zB6FlYXoZYZgbzdkCpLyi3Xtse64%7E%7Ey-72R6qXDXga0JxXEeiIr-gS4R74Ap2f9-X24vaj0dPww3BcsGnBnJxTnVKpbh8CTwje0x0uX%7EjYIbJV55ElAEByUzHXbggEB5mFUJltb7ALtQzikUcMH0PBiCBgpAalJllD9MgqGpTjnzgm1yZofL0cjC6XFpYMXN-ZBfxQUOfc2-TL6%7EebwGfVlnrC1bD6f9AGBYKSw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.73, 18.155.68.128, 18.155.68.98, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7026076074 (6.5G) [binary/octet-stream]\n",
            "Saving to: ‘llama-13b-4bit.pt’\n",
            "\n",
            "llama-13b-4bit.pt   100%[===================>]   6.54G  16.3MB/s    in 6m 56s  \n",
            "\n",
            "2023-07-10 18:12:49 (16.1 MB/s) - ‘llama-13b-4bit.pt’ saved [7026076074/7026076074]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/kuleshov/llama-13b-4bit/resolve/main/llama-13b-4bit.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFnsudjOHtnZ"
      },
      "source": [
        "## Le corpus d'instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PeUIuyeHOdM"
      },
      "source": [
        "Nous récupérons un set d'instruction. Dans l'optique d'un simple exemple de démonstration nous utilisons ici une sélection aléatoire de différents corpus d'instructions traduits en français par le projet Vicogne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjwy8C3kw4fF",
        "outputId": "1aeaf956-fa24-451c-af2f-b3947c6ee5f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-19 14:27:22--  https://raw.githubusercontent.com/opinionscience/EUInstruct/main/translated_sample/instruction_french_novel.json.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15464581 (15M) [application/zip]\n",
            "Saving to: ‘instruction_french_novel.json.zip’\n",
            "\n",
            "instruction_french_ 100%[===================>]  14.75M  89.1MB/s    in 0.2s    \n",
            "\n",
            "2023-06-19 14:27:23 (89.1 MB/s) - ‘instruction_french_novel.json.zip’ saved [15464581/15464581]\n",
            "\n",
            "Archive:  instruction_french_novel.json.zip\n",
            "  inflating: instruction_french_novel.json  \n",
            "  inflating: __MACOSX/._instruction_french_novel.json  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/opinionscience/EUInstruct/main/sample/french_novel_17_instruction_simple.json\n",
        "!unzip instruction_french_novel.json.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSABEKyZ1Jy3"
      },
      "source": [
        "Ces instructions utilisent le format classique du projet Alpaca de Stanford : *instructions*, *input* (optionnellement) et *output*. En résumé, les instructions correspondent à des exemples de prompts que pourraient laisser les utilisateurs du LLM, les *outputs* à la réponse que le LLM devrait générer et les *inputs* apporte des éléments de contextes supplémentaires (par exemple sous la forme de textes cités en exemple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw-XyFFe1tvl",
        "outputId": "40490394-b716-4d9d-9eee-4bbbdd94c84e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"instruction_id\": \"gbooks_Yvg5AAAAcAAJ.pdf_141\",\n",
            "    \"instruction\": \"\\u00c9crivez un texte qui d\\u00e9peint un affrontement verbal entre Lao damas et Eurialus, o\\u00f9 ce dernier insulte Ulysse en le traitant de marin incomp\\u00e9tent. Le style utilis\\u00e9 doit \\u00eatre soutenu et empreint d'une certaine gravit\\u00e9.\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Si Lao damas fut content de cette mode\\u017fte repon\\u017feil n'en fut pas de m\\u00eame de quelques -uns des autres qui \\u017fe per\\u017fuadant que s'il \\u017fe defen doit d'entrer en licecela ne provenoit que de \\u017fa foiblefle & de fon peu d'experience dans ces \\u017fortes d'exercices \\u017fe mirent \\u00e0 le prefler d'u ne maniere fort incivile . Eurialus particuliere ment tout boufi de l'honneur qu'il venoit d'ac querir piqua Uli\\u017fle pardes paroles non leule. ment dures mais injurieu\\u017feslc taxant d'e\\u017ftre plus propre \\u00e0 courir la mer \\u017fur quelque vailleau CON\"\n",
            "  },\n",
            "  {\n",
            "    \"instruction_id\": \"gbooks_jSCVXaiKDT0C.pdf_267\",\n",
            "    \"instruction\": \"\\u00c9crivez un texte litt\\u00e9raire historique dans le style du XVIIe si\\u00e8cle fran\\u00e7ais, mettant en sc\\u00e8ne les personnages de Te. rentius et Latinus. Le texte doit d\\u00e9buter par \\\"Quoi que Te. rentius n'e\\u00fbt pas l'ameint\\u00e9re\\u017f\\u017f\\u00e9eil accepta la propofition & le \\u017fervit fi 3 bien que \\u017fon Pan\\u00e9gyrique& \\u017fes Hymnes eurent tout le \\u017fucc\\u00e8s qu'il en pouvoit e\\u017fp\\u00e9rer.\\\"\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Quoi que Te. rentius n'e\\u00fbt pas l'ameint\\u00e9re\\u017f\\u017f\\u00e9eil accepta la propofition & le \\u017fervit fi 3 bien que \\u017fon Pan\\u00e9gyrique& \\u017fes Hymnes eurent tout le \\u017fucc\\u00e8s qu'il en pouvoit e\\u017fp\\u00e9rer. Il re\\u00e7\\u00fat \\u017foixane. te pi\\u017ftoles de la Cour& dix des C .... Alors Latinus ne \\u017fe \\u017fouvint plus de fa parole ou pour mieux dire il ne voulut plus la tenir. Ce proc\\u00e9d\\u00e9 \\u017furprit Terentius ; & comme il lui en faifoit quelque reproche il Ora fon bonnetqu'il avoit \\u017fur la t\\u00eate & le jetra de d\\u00e9pit dans le feu. Te rentius le rama\\u017f\\u017fa auffit\\u00f4t & le lui remit \\u017fur la t\\u00eate avec autantderer pect\"\n",
            "  },\n",
            "  {\n",
            "    \"instruction_id\": \"gbooks_H3sfUXpI9TEC.pdf_196\",\n",
            "    \"instruction\": \"\\u00c9crivez un texte litt\\u00e9raire dans le style du XVIIe si\\u00e8cle, en utilisant des mots et des expressions anciennes. Le texte doit commencer par la phrase suivante : \\\"Ily en eut me\\u017fme qui ne \\u017fe peurent tenir de dire quelques bons mots de cette pauvre fille : d'autres effrontez conceurent des id\\u00e9es qui offen\\u00e7oient \\u017fa cha\\u017ftet\\u00e9. Mais il arriua par gr\\u00e1d miracle,quetous ceux qui la regardoient auec impuret\\u00e9 qui prenoient de manuai fes pens\\u00e9es en la voyant furent rendus impui\\u017f\\u017fans & tous glacez pour iamais.Ce -\\u017fecret.n'e\\u017ftoitpas connu du \\u017fieur deBau ce dricourt'c'est pourquoy ce prodige ne le perluadoit pas lur la verit\\u00e9 de Leanne luy auoir dit. Au contraire toutes \\u043d\\u0456 cho\\u017fes...\\\"\",\n",
            "    \"input\": \"\",\n",
            "    \"output\": \"Ily en eut me\\u017fme qui ne \\u017fe peurent tenir de dire quelques bons mots de cette pauvre fille : d'autres effrontez conceurent des id\\u00e9es qui offen 1 \\u00e7oient \\u017fa cha\\u017ftet\\u00e9. Mais il arriua par gr\\u00e1d miracle,quetous ceux qui la regardoient auec impuret\\u00e9 qui prenoient de manuai fes pens\\u00e9es en la voyant furent rendus impui\\u017f\\u017fans & tous glacez pour iamais.Ce -\\u017fecret.n'e\\u017ftoitpas connu du \\u017fieur deBau ce dricourt'c'est pourquoy ce prodige ne le perluadoit pas lur la verit\\u00e9 de Leanne luy auoir dit. Au contraire toutes \\u043d\\u0456 cho\\u017fes\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open('french_novel17_instruction.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "json_formatted_str = json.dumps(data[0:3], indent=2)\n",
        "\n",
        "print(json_formatted_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78_DUqdzGrW"
      },
      "source": [
        "# Finetuning du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIgs_i0BNb5"
      },
      "source": [
        "Tout est prêt à lancer le fine-tuning du modèle. Nous allons juste créer le dossier d'accueil des fichiers de fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbdELbxDzIu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2aeb41-ff62-4051-c817-8cb67d1a11b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘llama-7b-french-novel’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir llama-13b-novel17"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhMpUgoS6oD"
      },
      "source": [
        "Et nous sommes prêt à lancer la grande commande. Il y a beaucoup de paramètre mais seulement quelqu'uns sont importants :\n",
        "* Nous allons utiliser le modèle Llama-7b de base et leurs poids correspondants (llama-7b-4bit)\n",
        "* Le fine-tuning sera effectué sur le set d'instruction *eu_translate_fr_sample.json* (évidemment à changer si vous optez pour un autre jeu de données).\n",
        "* Les fichiers du modèle seront placés dans le dossier *llama-7b-sample* (de nouveau à changer pour le nom de votre modèle).\n",
        "* Nous ne ferons tourner le fine-tuning que sur une *epoch* ce qui est suffisant pour un premier test.\n",
        "\n",
        "Après avoir lancé le script, Google Colab va tourner pendant un peu moins de 40 minutes.\n",
        "\n",
        "Si tout se passe bien vous verrez défiler le processus d'entraînement avec trois indicateurs régulièrement réactualisés : \"{'loss': 1.8581, 'learning_rate': 0.0002993736951983298, 'epoch': 0.0}\" :\n",
        "* Le \"loss\" c'est en quelque sorte le taux d'erreur du modèle : plus cette mesure est basse et plus le modèle parvient à prédire des textes assez approchants de ceux qui sont présent dans le corpus d'instruction.\n",
        "* Le *learning rate* (taux d'apprentissage) c'est la capacité du modèle à mémoriser de nouveaux éléments mais aussi à en oublier des anciens. Cet indicateur va constamment baisser au fur et à mesure de l'apprentissage.\n",
        "* L'*epoch* c'est le cycle d'apprentissage. Comme nous n'avons défini qu'une *epoch* cela correspondra à des pourcentages (de 0 à 0.99 à la fin de l'entraînement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiUKv07zGSm",
        "outputId": "3e102d4f-2931-45b2-ac3c-c248ffc8c7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "trainable params: 6553600 || all params: 334648320 || trainable%: 1.958354370343171\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b67ee2f1b68744d9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 2442.81it/s]\n",
            "Extracting data files: 100% 1/1 [00:01<00:00,  1.95s/it]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b67ee2f1b68744d9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 789.74it/s]\n",
            "2023-07-10 19:20:15.761993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "ParallelMode.NOT_PARALLEL\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, input, output. If instruction, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1,600\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 800\n",
            "  Number of trainable parameters = 6,553,600\n",
            "{'loss': 3.7399, 'learning_rate': 0.0002981132075471698, 'epoch': 0.01}\n",
            "{'loss': 3.7765, 'learning_rate': 0.0002943396226415094, 'epoch': 0.03}\n",
            "{'loss': 3.4018, 'learning_rate': 0.00029056603773584903, 'epoch': 0.04}\n",
            "{'loss': 3.2753, 'learning_rate': 0.00028679245283018867, 'epoch': 0.05}\n",
            "{'loss': 3.2989, 'learning_rate': 0.00028301886792452826, 'epoch': 0.06}\n",
            "{'loss': 3.2348, 'learning_rate': 0.0002792452830188679, 'epoch': 0.07}\n",
            "{'loss': 3.2588, 'learning_rate': 0.00027547169811320755, 'epoch': 0.09}\n",
            "{'loss': 3.3278, 'learning_rate': 0.00027169811320754714, 'epoch': 0.1}\n",
            "{'loss': 3.0648, 'learning_rate': 0.0002679245283018868, 'epoch': 0.11}\n",
            "{'loss': 3.4904, 'learning_rate': 0.0002641509433962264, 'epoch': 0.12}\n",
            "{'loss': 3.1619, 'learning_rate': 0.000260377358490566, 'epoch': 0.14}\n",
            "{'loss': 3.1168, 'learning_rate': 0.00025660377358490566, 'epoch': 0.15}\n",
            "{'loss': 3.105, 'learning_rate': 0.00025283018867924525, 'epoch': 0.16}\n",
            "{'loss': 3.0385, 'learning_rate': 0.0002490566037735849, 'epoch': 0.17}\n",
            "{'loss': 3.062, 'learning_rate': 0.00024566037735849057, 'epoch': 0.19}\n",
            "{'loss': 3.0187, 'learning_rate': 0.00024188679245283018, 'epoch': 0.2}\n",
            "{'loss': 3.0048, 'learning_rate': 0.0002381132075471698, 'epoch': 0.21}\n",
            "{'loss': 3.0934, 'learning_rate': 0.00023433962264150942, 'epoch': 0.23}\n",
            "{'loss': 3.0077, 'learning_rate': 0.00023056603773584906, 'epoch': 0.24}\n",
            "{'loss': 3.0024, 'learning_rate': 0.00022679245283018868, 'epoch': 0.25}\n",
            "{'loss': 3.0185, 'learning_rate': 0.00022301886792452827, 'epoch': 0.26}\n",
            "{'loss': 3.3919, 'learning_rate': 0.00021924528301886788, 'epoch': 0.28}\n",
            "{'loss': 3.1197, 'learning_rate': 0.00021547169811320753, 'epoch': 0.29}\n",
            "{'loss': 3.0358, 'learning_rate': 0.00021169811320754714, 'epoch': 0.3}\n",
            "{'loss': 2.9905, 'learning_rate': 0.00020792452830188676, 'epoch': 0.31}\n",
            "{'loss': 2.8969, 'learning_rate': 0.00020415094339622637, 'epoch': 0.33}\n",
            "{'loss': 3.2613, 'learning_rate': 0.00020037735849056602, 'epoch': 0.34}\n",
            "{'loss': 3.1834, 'learning_rate': 0.00019660377358490563, 'epoch': 0.35}\n",
            "{'loss': 3.0394, 'learning_rate': 0.00019283018867924525, 'epoch': 0.36}\n",
            "{'loss': 3.2126, 'learning_rate': 0.00018905660377358487, 'epoch': 0.38}\n",
            "{'loss': 2.8278, 'learning_rate': 0.0001852830188679245, 'epoch': 0.39}\n",
            "{'loss': 3.1215, 'learning_rate': 0.00018150943396226413, 'epoch': 0.4}\n",
            "{'loss': 3.056, 'learning_rate': 0.00017773584905660374, 'epoch': 0.41}\n",
            "{'loss': 2.9755, 'learning_rate': 0.0001739622641509434, 'epoch': 0.42}\n",
            "{'loss': 3.1931, 'learning_rate': 0.000170188679245283, 'epoch': 0.44}\n",
            "{'loss': 3.0568, 'learning_rate': 0.00016641509433962262, 'epoch': 0.45}\n",
            "{'loss': 3.3426, 'learning_rate': 0.00016264150943396224, 'epoch': 0.46}\n",
            "{'loss': 3.1836, 'learning_rate': 0.00015886792452830188, 'epoch': 0.47}\n",
            "{'loss': 2.968, 'learning_rate': 0.0001550943396226415, 'epoch': 0.49}\n",
            "{'loss': 2.9275, 'learning_rate': 0.0001513207547169811, 'epoch': 0.5}\n",
            "{'loss': 3.369, 'learning_rate': 0.00014754716981132076, 'epoch': 0.51}\n",
            "{'loss': 3.0044, 'learning_rate': 0.00014377358490566037, 'epoch': 0.53}\n",
            "{'loss': 3.1953, 'learning_rate': 0.00014, 'epoch': 0.54}\n",
            "{'loss': 3.1111, 'learning_rate': 0.0001362264150943396, 'epoch': 0.55}\n",
            "{'loss': 2.8766, 'learning_rate': 0.00013245283018867925, 'epoch': 0.56}\n",
            "{'loss': 3.0307, 'learning_rate': 0.00012867924528301886, 'epoch': 0.57}\n",
            "{'loss': 3.177, 'learning_rate': 0.00012490566037735848, 'epoch': 0.59}\n",
            "{'loss': 3.098, 'learning_rate': 0.00012113207547169811, 'epoch': 0.6}\n",
            "{'loss': 3.0887, 'learning_rate': 0.00011735849056603773, 'epoch': 0.61}\n",
            "{'loss': 2.9208, 'learning_rate': 0.00011358490566037736, 'epoch': 0.62}\n",
            " 62% 500/800 [20:50<12:32,  2.51s/it]Saving model checkpoint to llama-13b-novel17/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 3.0493, 'learning_rate': 0.00010981132075471696, 'epoch': 0.64}\n",
            "{'loss': 3.2214, 'learning_rate': 0.00010603773584905659, 'epoch': 0.65}\n",
            "{'loss': 3.1371, 'learning_rate': 0.0001022641509433962, 'epoch': 0.66}\n",
            "{'loss': 3.0662, 'learning_rate': 9.849056603773584e-05, 'epoch': 0.68}\n",
            "{'loss': 3.1264, 'learning_rate': 9.471698113207546e-05, 'epoch': 0.69}\n",
            "{'loss': 2.6576, 'learning_rate': 9.094339622641508e-05, 'epoch': 0.7}\n",
            "{'loss': 2.9487, 'learning_rate': 8.716981132075471e-05, 'epoch': 0.71}\n",
            "{'loss': 3.1626, 'learning_rate': 8.339622641509433e-05, 'epoch': 0.72}\n",
            "{'loss': 2.8372, 'learning_rate': 7.962264150943396e-05, 'epoch': 0.74}\n",
            "{'loss': 3.0254, 'learning_rate': 7.584905660377357e-05, 'epoch': 0.75}\n",
            "{'loss': 3.0489, 'learning_rate': 7.20754716981132e-05, 'epoch': 0.76}\n",
            "{'loss': 3.0277, 'learning_rate': 6.830188679245282e-05, 'epoch': 0.78}\n",
            "{'loss': 3.141, 'learning_rate': 6.452830188679245e-05, 'epoch': 0.79}\n",
            "{'loss': 3.0535, 'learning_rate': 6.075471698113207e-05, 'epoch': 0.8}\n",
            "{'loss': 2.8993, 'learning_rate': 5.6981132075471696e-05, 'epoch': 0.81}\n",
            "{'loss': 3.0354, 'learning_rate': 5.320754716981131e-05, 'epoch': 0.82}\n",
            "{'loss': 2.8992, 'learning_rate': 4.9433962264150935e-05, 'epoch': 0.84}\n",
            "{'loss': 2.8361, 'learning_rate': 4.566037735849056e-05, 'epoch': 0.85}\n",
            "{'loss': 3.118, 'learning_rate': 4.188679245283018e-05, 'epoch': 0.86}\n",
            "{'loss': 3.0761, 'learning_rate': 3.8113207547169805e-05, 'epoch': 0.88}\n",
            "{'loss': 2.9991, 'learning_rate': 3.4339622641509435e-05, 'epoch': 0.89}\n",
            "{'loss': 3.0309, 'learning_rate': 3.056603773584906e-05, 'epoch': 0.9}\n",
            "{'loss': 2.8833, 'learning_rate': 2.6792452830188674e-05, 'epoch': 0.91}\n",
            "{'loss': 2.943, 'learning_rate': 2.30188679245283e-05, 'epoch': 0.93}\n",
            "{'loss': 2.8374, 'learning_rate': 1.9245283018867924e-05, 'epoch': 0.94}\n",
            "{'loss': 2.9928, 'learning_rate': 1.5471698113207547e-05, 'epoch': 0.95}\n",
            "{'loss': 2.7407, 'learning_rate': 1.1698113207547168e-05, 'epoch': 0.96}\n",
            "{'loss': 2.9285, 'learning_rate': 7.924528301886793e-06, 'epoch': 0.97}\n",
            "{'loss': 3.1062, 'learning_rate': 4.150943396226415e-06, 'epoch': 0.99}\n",
            "{'loss': 2.8039, 'learning_rate': 3.7735849056603767e-07, 'epoch': 1.0}\n",
            "100% 800/800 [33:43<00:00,  2.51s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2023.5458, 'train_samples_per_second': 0.791, 'train_steps_per_second': 0.395, 'train_loss': 3.0848361897468566, 'epoch': 1.0}\n",
            "100% 800/800 [33:43<00:00,  2.53s/it]\n"
          ]
        }
      ],
      "source": [
        "!llmtune finetune \\\n",
        "    --model llama-13b-4bit \\\n",
        "    --lr=3e-4 \\\n",
        "    --epochs 1 \\\n",
        "    --save_total_limit 3 \\\n",
        "    --save_steps 500 \\\n",
        "    --weights llama-13b-4bit.pt \\\n",
        "    --adapter llama-13b-novel17 \\\n",
        "    --mbatch_size=1 \\\n",
        "    --batch_size=2 \\\n",
        "    --epochs=1 \\\n",
        "    --cutoff_len=256 \\\n",
        "    --lora_r=8 \\\n",
        "    --lora_alpha=16 \\\n",
        "    --lora_dropout=0.05 \\\n",
        "    --warmup_steps=5 \\\n",
        "    --dataset french_novel17_instruction2.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov3RiDxqWvuP",
        "outputId": "0c93e181-9220-41e7-d5ed-c95497d96371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build\t\t\t\t   llama-7b-french-novel  README.md\n",
            "dist\t\t\t\t   llmtune\t\t  requirements.txt\n",
            "instruction_french_novel.json\t   llmtune.egg-info\t  setup.py\n",
            "instruction_french_novel.json.zip  __MACOSX\n",
            "llama-7b-4bit.pt\t\t   quant_cuda.egg-info\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_84CU2sE46s"
      },
      "source": [
        "Si tout se passe bien, vous devrez avoir comlètement fini l'entraînement.\n",
        "\n",
        "Après un petit temps de synchronisation entre Google Colab, vous allez voir apparaître deux fichiers dans le dossier du modèle de fine-tuning : adapter_model.bin (le modèle proprepement dit) et adapter_model.config (un fichier de configuration). À noter que le modèle de fine-tuning est considérablement plus petit que le modèle d'origine : c'est en quelque sorte un modèle complémentaire qui vient ajuster le LLM (et il en aura toujours besoin pour fonctionner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aKvXziTziXY"
      },
      "source": [
        "# Générer du texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPV1WuvEvhD"
      },
      "source": [
        "Et maintenant il est possible de générer du texte. La fonction par défaut de llmtune n'est pas pour l'instant pas très pratique mais cela devrait s'améliorer prochainement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L_qtyiFzgv2",
        "outputId": "3a0b36e0-43f0-46b2-cbeb-a0ff79dcc71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "llama-13b-novel17 loaded\n",
            "2023-07-10 20:06:42.312111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Les lamas dont l'on vient de faire mention sont de ce grand peuple ,ils entretiennent l'élection à leur cheit & ſont ſentimentaux & galans comme deſſus. Ces animaux ont le pied palmatif,& ont les pattes couvertes de vaiſſes poils blancs. Ils entretiennent par l'organe pariétal des eaux du corps en fournissant des uſages de chirurgie. Ils prètent vn peu de mal aux lamas pour leur ravir le tire ou leur couper la jambe. Pour peu qu'on fcure 10 cacher le pain de la faiſon à ſes petits,ils ne les lairont pas . Il ſeroient trop heureux ſi on les laiſſoit a</s><s>\n"
          ]
        }
      ],
      "source": [
        "!llmtune generate \\\n",
        "    --model llama-13b-4bit \\\n",
        "    --weights llama-13b-4bit.pt \\\n",
        "    --adapter llama-13b-novel17 \\\n",
        "    --max-length 400 \\\n",
        "    --min-length 200 \\\n",
        "    --instruction \"Écrivez un texte littéraire dans le style du XVIIe siècle décrivan un peuple de lamas intelligents au Pérou. Vous pouvez notamment évoquer les mœurs et coutumes de ces animaux très savants. ###OUTPUT\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}